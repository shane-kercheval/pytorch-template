{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "# save weights and biases api key to .env file in project directory\n",
    "assert os.getenv('WANDB_API_KEY')\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)  # noqa: NPY002\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshane-kercheval\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set  : X-torch.Size([56000, 1, 28, 28]), y-torch.Size([56000])\n",
      "Validation set: X-torch.Size([7000, 1, 28, 28]), y-torch.Size([7000])\n",
      "Test set      : X-torch.Size([7000, 1, 28, 28]), y-torch.Size([7000])\n"
     ]
    }
   ],
   "source": [
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "x = torch.tensor(x.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.astype(int).values, dtype=torch.long)\n",
    "\n",
    "# need to make this dynamic based on Fully Connected vs Convolutional\n",
    "# Reshape data to have channel dimension\n",
    "# MNIST images are 28x28, so we reshape them to [batch_size, 1, 28, 28]\n",
    "x = x.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# 80% train; 10% validation; 10% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set  : X-{x_train.shape}, y-{y_train.shape}\")\n",
    "print(f\"Validation set: X-{x_val.shape}, y-{y_val.shape}\")\n",
    "print(f\"Test set      : X-{x_test.shape}, y-{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Convolutional neural network (two convolutional layers).\"\"\"\n",
    "\n",
    "    def __init__(self, kernels: list, classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(x: torch.tensor, y: torch.tensor, batch_size: int) -> DataLoader:\n",
    "    \"\"\"Make a DataLoader from a given dataset.\"\"\"\n",
    "    return DataLoader(\n",
    "        dataset=TensorDataset(x, y),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def make(config: dict) -> tuple:\n",
    "    \"\"\"Make the model, data, and optimization objects.\"\"\"\n",
    "    # Make the data\n",
    "    train_loader = make_loader(x_train, y_train, batch_size=config.batch_size)\n",
    "    validation_loader = make_loader(x_val, y_val, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(x_test, y_test, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = ConvNet(config.kernels, config.classes).to(DEVICE)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_log(training_loss: float, validation_loss: float, example_ct: int, epoch: int) -> None:\n",
    "    \"\"\"Logs loss to the console and wandb.\"\"\"\n",
    "    # Where the magic happens\n",
    "    wandb.log(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'training_loss': training_loss,\n",
    "            'validation_loss': validation_loss\n",
    "        },\n",
    "        step=example_ct,\n",
    "    )\n",
    "    print(\n",
    "        f\"Training/Validation Loss after {str(example_ct).zfill(5)} examples: \",\n",
    "        f\"{training_loss:.3f} | {validation_loss:.3f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_average_loss(\n",
    "        data_loader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_func: callable) -> float:\n",
    "    \"\"\"Calculates the average loss over a dataset.\"\"\"\n",
    "    running_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)  # noqa: PLW2901\n",
    "            loss = loss_func(model(x), y)\n",
    "            # weighted average of the loss adjusted for the batch size\n",
    "            running_loss += loss.item() * x.shape[0]\n",
    "            total_samples += x.shape[0]\n",
    "    return running_loss / total_samples\n",
    "\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        validation_loader: DataLoader,\n",
    "        criterion: callable,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        config: dict) -> None:\n",
    "    \"\"\"Trains the model for the number of epochs specified in the config.\"\"\"\n",
    "    model.train()\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    example_ct = 0  # number of examples seen\n",
    "\n",
    "    log_interval = 30 # i.e. every 30 batches\n",
    "    total_batches = len(train_loader)\n",
    "    log_interval = max(1, math.floor(total_batches / log_interval))\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        running_training_loss = 0\n",
    "        total_train_samples = 0\n",
    "        for batch_index, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)  # noqa: PLW2901\n",
    "            # ➡ Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            # ⬅ Backward pass & optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct += len(x_batch)\n",
    "            # weighted average of the training loss\n",
    "            running_training_loss += loss.item() * x_batch.shape[0]\n",
    "            total_train_samples += x_batch.shape[0]\n",
    "            # Report metrics every 25th batch\n",
    "            if batch_index % log_interval == 0:\n",
    "                avg_training_loss = running_training_loss / total_train_samples\n",
    "                running_training_loss = 0\n",
    "                total_train_samples = 0\n",
    "                model.eval()\n",
    "                average_validation_loss = calculate_average_loss(\n",
    "                    data_loader=validation_loader, model=model, loss_func=criterion,\n",
    "                )\n",
    "                train_log(avg_training_loss, average_validation_loss, example_ct, epoch)\n",
    "                model.train()\n",
    "\n",
    "\n",
    "def test(model: nn.Module, test_loader: DataLoader, criterion: callable) -> None:\n",
    "    \"\"\"Tests the model on the test set. Logs the accuracy to the console and to wandb.\"\"\"\n",
    "    model.eval()\n",
    "    avg_test_loss = calculate_average_loss(data_loader=test_loader, model=model, loss_func=criterion)  # noqa\n",
    "    print(f\"Average Loss on test set: {avg_test_loss:.3f}\")\n",
    "    wandb.log({'test_loss': avg_test_loss})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    x, _ = next(iter(test_loader))\n",
    "    torch.onnx.export(model, x.to(DEVICE) , 'model.onnx')\n",
    "    wandb.save('model.onnx')\n",
    "\n",
    "\n",
    "def model_pipeline(config: dict) -> nn.Module:\n",
    "    \"\"\"Builds the model and runs it.\"\"\"\n",
    "    # tell wandb to get started\n",
    "    project = config.pop('project'); assert project\n",
    "    tags = config.pop('tags', None)\n",
    "    notes = config.pop('notes', None)\n",
    "    with wandb.init(project=project, config=config, tags=tags, notes=notes):\n",
    "      config = wandb.config\n",
    "      # make the model, data, and optimization problem\n",
    "      model, train_loader, validation_loader, test_loader, criterion, optimizer = make(config)\n",
    "      print(model)\n",
    "      # and use them to train the model\n",
    "      train(model, train_loader, validation_loader, criterion, optimizer, config)\n",
    "      # and test its final performance\n",
    "      test(model, test_loader, criterion)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231228_191623-bvh0cx6o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo/runs/bvh0cx6o' target=\"_blank\">trim-star-12</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo/runs/bvh0cx6o' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo/runs/bvh0cx6o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Loss after 00064 examples:  14.934 | 91.637\n",
      "Training/Validation Loss after 01920 examples:  8.008 | 1.051\n",
      "Training/Validation Loss after 03776 examples:  0.748 | 0.530\n",
      "Training/Validation Loss after 05632 examples:  0.467 | 0.340\n",
      "Training/Validation Loss after 07488 examples:  0.352 | 0.309\n",
      "Training/Validation Loss after 09344 examples:  0.258 | 0.248\n",
      "Training/Validation Loss after 11200 examples:  0.289 | 0.238\n",
      "Training/Validation Loss after 13056 examples:  0.231 | 0.212\n",
      "Training/Validation Loss after 14912 examples:  0.211 | 0.214\n",
      "Training/Validation Loss after 16768 examples:  0.190 | 0.225\n",
      "Training/Validation Loss after 18624 examples:  0.201 | 0.189\n",
      "Training/Validation Loss after 20480 examples:  0.186 | 0.194\n",
      "Training/Validation Loss after 22336 examples:  0.245 | 0.193\n",
      "Training/Validation Loss after 24192 examples:  0.223 | 0.197\n",
      "Training/Validation Loss after 26048 examples:  0.156 | 0.156\n",
      "Training/Validation Loss after 27904 examples:  0.174 | 0.235\n",
      "Training/Validation Loss after 29760 examples:  0.191 | 0.172\n",
      "Training/Validation Loss after 31616 examples:  0.147 | 0.165\n",
      "Training/Validation Loss after 33472 examples:  0.182 | 0.172\n",
      "Training/Validation Loss after 35328 examples:  0.215 | 0.194\n",
      "Training/Validation Loss after 37184 examples:  0.156 | 0.191\n",
      "Training/Validation Loss after 39040 examples:  0.194 | 0.160\n",
      "Training/Validation Loss after 40896 examples:  0.192 | 0.196\n",
      "Training/Validation Loss after 42752 examples:  0.152 | 0.180\n",
      "Training/Validation Loss after 44608 examples:  0.165 | 0.263\n",
      "Training/Validation Loss after 46464 examples:  0.179 | 0.185\n",
      "Training/Validation Loss after 48320 examples:  0.171 | 0.188\n",
      "Training/Validation Loss after 50176 examples:  0.203 | 0.157\n",
      "Training/Validation Loss after 52032 examples:  0.155 | 0.169\n",
      "Training/Validation Loss after 53888 examples:  0.173 | 0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:10<00:41, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Loss after 55744 examples:  0.203 | 0.169\n",
      "Training/Validation Loss after 56064 examples:  0.066 | 0.191\n",
      "Training/Validation Loss after 57920 examples:  0.134 | 0.160\n",
      "Training/Validation Loss after 59776 examples:  0.171 | 0.158\n",
      "Training/Validation Loss after 61632 examples:  0.161 | 0.152\n",
      "Training/Validation Loss after 63488 examples:  0.143 | 0.142\n",
      "Training/Validation Loss after 65344 examples:  0.153 | 0.173\n",
      "Training/Validation Loss after 67200 examples:  0.158 | 0.155\n",
      "Training/Validation Loss after 69056 examples:  0.154 | 0.157\n",
      "Training/Validation Loss after 70912 examples:  0.157 | 0.156\n",
      "Training/Validation Loss after 72768 examples:  0.159 | 0.165\n",
      "Training/Validation Loss after 74624 examples:  0.160 | 0.141\n",
      "Training/Validation Loss after 76480 examples:  0.159 | 0.177\n",
      "Training/Validation Loss after 78336 examples:  0.104 | 0.143\n",
      "Training/Validation Loss after 80192 examples:  0.121 | 0.177\n",
      "Training/Validation Loss after 82048 examples:  0.203 | 0.146\n",
      "Training/Validation Loss after 83904 examples:  0.115 | 0.128\n",
      "Training/Validation Loss after 85760 examples:  0.155 | 0.151\n",
      "Training/Validation Loss after 87616 examples:  0.177 | 0.191\n",
      "Training/Validation Loss after 89472 examples:  0.175 | 0.155\n",
      "Training/Validation Loss after 91328 examples:  0.138 | 0.169\n",
      "Training/Validation Loss after 93184 examples:  0.145 | 0.151\n",
      "Training/Validation Loss after 95040 examples:  0.145 | 0.161\n",
      "Training/Validation Loss after 96896 examples:  0.162 | 0.168\n",
      "Training/Validation Loss after 98752 examples:  0.119 | 0.146\n",
      "Training/Validation Loss after 100608 examples:  0.106 | 0.223\n",
      "Training/Validation Loss after 102464 examples:  0.173 | 0.138\n",
      "Training/Validation Loss after 104320 examples:  0.153 | 0.136\n",
      "Training/Validation Loss after 106176 examples:  0.130 | 0.126\n",
      "Training/Validation Loss after 108032 examples:  0.147 | 0.134\n",
      "Training/Validation Loss after 109888 examples:  0.110 | 0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:20<00:30, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Loss after 111744 examples:  0.134 | 0.143\n",
      "Training/Validation Loss after 112064 examples:  0.002 | 0.279\n",
      "Training/Validation Loss after 113920 examples:  0.131 | 0.169\n",
      "Training/Validation Loss after 115776 examples:  0.138 | 0.160\n",
      "Training/Validation Loss after 117632 examples:  0.145 | 0.164\n",
      "Training/Validation Loss after 119488 examples:  0.144 | 0.151\n",
      "Training/Validation Loss after 121344 examples:  0.161 | 0.151\n",
      "Training/Validation Loss after 123200 examples:  0.123 | 0.165\n",
      "Training/Validation Loss after 125056 examples:  0.185 | 0.151\n",
      "Training/Validation Loss after 126912 examples:  0.165 | 0.189\n",
      "Training/Validation Loss after 128768 examples:  0.107 | 0.180\n",
      "Training/Validation Loss after 130624 examples:  0.140 | 0.203\n",
      "Training/Validation Loss after 132480 examples:  0.139 | 0.143\n",
      "Training/Validation Loss after 134336 examples:  0.103 | 0.149\n",
      "Training/Validation Loss after 136192 examples:  0.122 | 0.143\n",
      "Training/Validation Loss after 138048 examples:  0.110 | 0.136\n",
      "Training/Validation Loss after 139904 examples:  0.124 | 0.149\n",
      "Training/Validation Loss after 141760 examples:  0.130 | 0.120\n",
      "Training/Validation Loss after 143616 examples:  0.146 | 0.162\n",
      "Training/Validation Loss after 145472 examples:  0.140 | 0.128\n",
      "Training/Validation Loss after 147328 examples:  0.122 | 0.139\n",
      "Training/Validation Loss after 149184 examples:  0.120 | 0.141\n",
      "Training/Validation Loss after 151040 examples:  0.163 | 0.145\n",
      "Training/Validation Loss after 152896 examples:  0.133 | 0.157\n",
      "Training/Validation Loss after 154752 examples:  0.153 | 0.161\n",
      "Training/Validation Loss after 156608 examples:  0.146 | 0.158\n",
      "Training/Validation Loss after 158464 examples:  0.131 | 0.151\n",
      "Training/Validation Loss after 160320 examples:  0.137 | 0.150\n",
      "Training/Validation Loss after 162176 examples:  0.154 | 0.129\n",
      "Training/Validation Loss after 164032 examples:  0.145 | 0.153\n",
      "Training/Validation Loss after 165888 examples:  0.134 | 0.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:31<00:21, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Loss after 167744 examples:  0.141 | 0.134\n",
      "Training/Validation Loss after 168064 examples:  0.078 | 0.124\n",
      "Training/Validation Loss after 169920 examples:  0.109 | 0.120\n",
      "Training/Validation Loss after 171776 examples:  0.097 | 0.125\n",
      "Training/Validation Loss after 173632 examples:  0.101 | 0.130\n",
      "Training/Validation Loss after 175488 examples:  0.119 | 0.134\n",
      "Training/Validation Loss after 177344 examples:  0.109 | 0.135\n",
      "Training/Validation Loss after 179200 examples:  0.124 | 0.141\n",
      "Training/Validation Loss after 181056 examples:  0.111 | 0.125\n",
      "Training/Validation Loss after 182912 examples:  0.109 | 0.120\n",
      "Training/Validation Loss after 184768 examples:  0.119 | 0.157\n",
      "Training/Validation Loss after 186624 examples:  0.070 | 0.114\n",
      "Training/Validation Loss after 188480 examples:  0.143 | 0.160\n",
      "Training/Validation Loss after 190336 examples:  0.176 | 0.150\n",
      "Training/Validation Loss after 192192 examples:  0.144 | 0.133\n",
      "Training/Validation Loss after 194048 examples:  0.130 | 0.116\n",
      "Training/Validation Loss after 195904 examples:  0.138 | 0.143\n",
      "Training/Validation Loss after 197760 examples:  0.114 | 0.124\n",
      "Training/Validation Loss after 199616 examples:  0.111 | 0.142\n",
      "Training/Validation Loss after 201472 examples:  0.102 | 0.131\n",
      "Training/Validation Loss after 203328 examples:  0.147 | 0.160\n",
      "Training/Validation Loss after 205184 examples:  0.123 | 0.148\n",
      "Training/Validation Loss after 207040 examples:  0.123 | 0.162\n",
      "Training/Validation Loss after 208896 examples:  0.157 | 0.123\n",
      "Training/Validation Loss after 210752 examples:  0.124 | 0.142\n",
      "Training/Validation Loss after 212608 examples:  0.152 | 0.149\n",
      "Training/Validation Loss after 214464 examples:  0.150 | 0.138\n",
      "Training/Validation Loss after 216320 examples:  0.134 | 0.125\n",
      "Training/Validation Loss after 218176 examples:  0.132 | 0.126\n",
      "Training/Validation Loss after 220032 examples:  0.141 | 0.120\n",
      "Training/Validation Loss after 221888 examples:  0.132 | 0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:41<00:10, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Loss after 223744 examples:  0.110 | 0.135\n",
      "Training/Validation Loss after 224064 examples:  0.197 | 0.118\n",
      "Training/Validation Loss after 225920 examples:  0.110 | 0.141\n",
      "Training/Validation Loss after 227776 examples:  0.113 | 0.179\n",
      "Training/Validation Loss after 229632 examples:  0.097 | 0.143\n",
      "Training/Validation Loss after 231488 examples:  0.130 | 0.156\n",
      "Training/Validation Loss after 233344 examples:  0.134 | 0.150\n",
      "Training/Validation Loss after 235200 examples:  0.109 | 0.145\n",
      "Training/Validation Loss after 237056 examples:  0.122 | 0.123\n",
      "Training/Validation Loss after 238912 examples:  0.130 | 0.137\n",
      "Training/Validation Loss after 240768 examples:  0.124 | 0.123\n",
      "Training/Validation Loss after 242624 examples:  0.106 | 0.147\n",
      "Training/Validation Loss after 244480 examples:  0.123 | 0.120\n",
      "Training/Validation Loss after 246336 examples:  0.089 | 0.116\n",
      "Training/Validation Loss after 248192 examples:  0.119 | 0.139\n",
      "Training/Validation Loss after 250048 examples:  0.085 | 0.230\n",
      "Training/Validation Loss after 251904 examples:  0.162 | 0.121\n",
      "Training/Validation Loss after 253760 examples:  0.130 | 0.113\n",
      "Training/Validation Loss after 255616 examples:  0.147 | 0.167\n",
      "Training/Validation Loss after 257472 examples:  0.113 | 0.148\n",
      "Training/Validation Loss after 259328 examples:  0.142 | 0.151\n",
      "Training/Validation Loss after 261184 examples:  0.137 | 0.144\n",
      "Training/Validation Loss after 263040 examples:  0.120 | 0.159\n",
      "Training/Validation Loss after 264896 examples:  0.106 | 0.139\n",
      "Training/Validation Loss after 266752 examples:  0.094 | 0.135\n",
      "Training/Validation Loss after 268608 examples:  0.173 | 0.119\n",
      "Training/Validation Loss after 270464 examples:  0.136 | 0.138\n",
      "Training/Validation Loss after 272320 examples:  0.172 | 0.140\n",
      "Training/Validation Loss after 274176 examples:  0.109 | 0.129\n",
      "Training/Validation Loss after 276032 examples:  0.126 | 0.099\n",
      "Training/Validation Loss after 277888 examples:  0.111 | 0.165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:51<00:00, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Loss after 279744 examples:  0.117 | 0.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss on test set: 13.518667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>test_loss</td><td>0.13519</td></tr><tr><td>training_loss</td><td>0.11668</td></tr><tr><td>validation_loss</td><td>0.11477</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-star-12</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo/runs/bvh0cx6o' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo/runs/bvh0cx6o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231228_191623-bvh0cx6o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'project': 'pytorch-demo',\n",
    "    'tags': ['pytorch', 'demo'],\n",
    "    'notes': 'First run with a simple CNN',\n",
    "    'epochs': 5,\n",
    "    'classes': 10,\n",
    "    'kernels': [16, 32],\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.005,\n",
    "    'dataset': 'MNIST',\n",
    "    'architecture': 'CNN',\n",
    "}\n",
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
