{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from source.domain.pytorch_helpers import EarlyStopping\n",
    "\n",
    "# save weights and biases api key to .env file in project directory\n",
    "assert os.getenv('WANDB_API_KEY')\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)  # noqa: NPY002\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.config.fileConfig(\n",
    "    os.path.join(os.getcwd(), '/code/source/config/logging.conf'),\n",
    "    # defaults={'logfilename': os.path.join(os.getcwd(), 'tests/test_files/log.log')},\n",
    "    disable_existing_loggers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 17:18:48 - ERROR    | Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshane-kercheval\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 17:18:54 - INFO     | Training set  : X-torch.Size([56000, 1, 28, 28]), y-torch.Size([56000])\n",
      "2023-12-30 17:18:54 - INFO     | Validation set: X-torch.Size([7000, 1, 28, 28]), y-torch.Size([7000])\n",
      "2023-12-30 17:18:54 - INFO     | Test set      : X-torch.Size([7000, 1, 28, 28]), y-torch.Size([7000])\n"
     ]
    }
   ],
   "source": [
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "x = torch.tensor(x.values, dtype=torch.float32)\n",
    "y = torch.tensor(y.astype(int).values, dtype=torch.long)\n",
    "\n",
    "# need to make this dynamic based on Fully Connected vs Convolutional\n",
    "# Reshape data to have channel dimension\n",
    "# MNIST images are 28x28, so we reshape them to [batch_size, 1, 28, 28]\n",
    "x = x.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# 80% train; 10% validation; 10% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "logging.info(f\"Training set  : X-{x_train.shape}, y-{y_train.shape}\")\n",
    "logging.info(f\"Validation set: X-{x_val.shape}, y-{y_val.shape}\")\n",
    "logging.info(f\"Test set      : X-{x_test.shape}, y-{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Convolutional neural network (two convolutional layers).\"\"\"\n",
    "\n",
    "    def __init__(self, kernels: list, classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(x: torch.tensor, y: torch.tensor, batch_size: int) -> DataLoader:\n",
    "    \"\"\"Make a DataLoader from a given dataset.\"\"\"\n",
    "    return DataLoader(\n",
    "        dataset=TensorDataset(x, y),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def make(config: dict) -> tuple:\n",
    "    \"\"\"Make the model, data, and optimization objects.\"\"\"\n",
    "    # Make the data\n",
    "    train_loader = make_loader(x_train, y_train, batch_size=config.batch_size)\n",
    "    validation_loader = make_loader(x_val, y_val, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(x_test, y_test, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = ConvNet(config.kernels, classes=10).to(DEVICE)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer_creator = lambda lr: torch.optim.Adam(model.parameters(), lr=lr)  # noqa: E731\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer_creator = lambda lr: torch.optim.SGD(model.parameters(), lr=lr)  # noqa: E731\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {config.optimizer}\")\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        optimizer_creator,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_log(\n",
    "        training_loss: float,\n",
    "        validation_loss: float,\n",
    "        example_ct: int,\n",
    "        epoch: int,\n",
    "        learning_rate: float) -> None:\n",
    "    \"\"\"Logs loss to the console and wandb.\"\"\"\n",
    "    # Where the magic happens\n",
    "    wandb.log(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'step_learning_rate': learning_rate,\n",
    "            'step_training_loss': training_loss,\n",
    "            'step_validation_loss': validation_loss,\n",
    "        },\n",
    "        step=example_ct,\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"Epoch: {epoch} | Learning Rate: {learning_rate:.3f}: \"\n",
    "        f\"Training/Validation Loss after {str(example_ct).zfill(5)} examples: \"\n",
    "        f\"{training_loss:.3f} | {validation_loss:.3f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_average_loss(\n",
    "        data_loader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_func: callable) -> float:\n",
    "    \"\"\"Calculates the average loss over a dataset.\"\"\"\n",
    "    running_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)  # noqa: PLW2901\n",
    "            loss = loss_func(model(x), y)\n",
    "            # weighted average of the loss adjusted for the batch size\n",
    "            running_loss += loss.item() * x.shape[0]\n",
    "            total_samples += x.shape[0]\n",
    "    return running_loss / total_samples\n",
    "\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        validation_loader: DataLoader,\n",
    "        criterion: callable,\n",
    "        optimizer_creator: callable,\n",
    "        config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model for the number of epochs specified in the config. Uses early stopping to\n",
    "    prevent overfitting. Takes multiple learning rates and if early stopping is triggered, the\n",
    "    learning rate is reduced and training is continued until no learning rates remain.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log='all', log_freq=20)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    example_ct = 0  # number of examples seen\n",
    "    log_interval = 30 # i.e. every 30 batches\n",
    "    total_batches = len(train_loader)\n",
    "    log_interval = max(1, math.floor(total_batches / log_interval))\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        model=model,\n",
    "        patience=3,\n",
    "        delta=0.05,  # new loss is required to be >%5 better than previous best\n",
    "        delta_type='relative',\n",
    "        verbose=True,\n",
    "    )\n",
    "    early_stopped_count = 0\n",
    "    learning_rate = config.learning_rate\n",
    "    optimizer = optimizer_creator(lr=learning_rate)\n",
    "    wandb.log({'learning_rate': learning_rate})\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        logging.info(f\"Epoch: {epoch} | Learning Rate: {learning_rate:.3f}\")\n",
    "        running_training_loss = 0\n",
    "        total_train_samples = 0\n",
    "        for batch_index, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)  # noqa: PLW2901\n",
    "            # ➡ Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            # ⬅ Backward pass & optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct += len(x_batch)\n",
    "            # weighted average of the training loss\n",
    "            running_training_loss += loss.item() * x_batch.shape[0]\n",
    "            total_train_samples += x_batch.shape[0]\n",
    "            # Report metrics every 25th batch\n",
    "            if batch_index % log_interval == 0:\n",
    "                avg_training_loss = running_training_loss / total_train_samples\n",
    "                running_training_loss = 0\n",
    "                total_train_samples = 0\n",
    "                model.eval()\n",
    "                average_validation_loss = calculate_average_loss(\n",
    "                    data_loader=validation_loader, model=model, loss_func=criterion,\n",
    "                )\n",
    "                train_log(\n",
    "                    avg_training_loss,\n",
    "                    average_validation_loss,\n",
    "                    example_ct,\n",
    "                    epoch,\n",
    "                    learning_rate,\n",
    "                )\n",
    "                model.train()\n",
    "\n",
    "        model.eval()\n",
    "        average_validation_loss = calculate_average_loss(\n",
    "            data_loader=validation_loader, model=model, loss_func=criterion,\n",
    "        )\n",
    "        model.train()\n",
    "        if early_stopping(average_validation_loss):\n",
    "            logging.info(\"Early stopping. Loading previous best state.\")\n",
    "            # we have stopped training (for this learning rate), load the previous best state\n",
    "            model.load_state_dict(early_stopping.best_state)\n",
    "            # if we have more learning rates, reset the optimizer and early stopping and\n",
    "            # continue training\n",
    "            if early_stopped_count < config.early_stopped_count:\n",
    "                logging.info(f\"Reducing learning rate: {learning_rate} -> {learning_rate / 2}\")\n",
    "                learning_rate /= 2\n",
    "                optimizer = optimizer_creator(lr=learning_rate)\n",
    "                early_stopping.reset()\n",
    "                early_stopped_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    wandb.log({\n",
    "        'best_validation_loss': early_stopping.lowest_loss,\n",
    "        'best_epoch': early_stopping.best_index,\n",
    "    })\n",
    "    logging.info(f\"Best validation loss: {early_stopping.lowest_loss:.3f}\")\n",
    "    logging.info(f\"Best early stopping index/epoch: {early_stopping.best_index}\")\n",
    "\n",
    "\n",
    "def plot_misclassified_sample(\n",
    "        num_images: int,\n",
    "        images: torch.tensor,\n",
    "        predictions: np.array,\n",
    "        labels: np.array) -> None:\n",
    "    \"\"\"Plot a sample of the misclassified images.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=num_images // 5, ncols=5, sharex=True, sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    mismatched_indexes = np.where(predictions != labels)[0]\n",
    "    rows = np.random.choice(mismatched_indexes, size=num_images, replace=False)  # noqa: NPY002\n",
    "    for i, row in enumerate(rows):\n",
    "        # img = X_test[row].cpu().numpy().reshape(28, 28)\n",
    "        img = images[row].cpu().numpy().reshape(28, 28)\n",
    "        ax[i].imshow(img, cmap='Greys')\n",
    "        title_color = 'red' if predictions[row] != y_test[row] else 'black'\n",
    "        ax[i].set_title(f'P:{predictions[row]} - A:{y_test[row]}', color=title_color)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    wandb.log({'sample-misclassified': wandb.Image(fig)})\n",
    "\n",
    "\n",
    "def plot_heatmap(predictions: np.array, labels: np.array) -> None:\n",
    "    \"\"\"Plot a heatmap of the misclassified samples.\"\"\"\n",
    "    # create a heatmap of misclassified samples\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    # remove the diagonal values (correct predictions) for better visualization\n",
    "    np.fill_diagonal(cm, 0)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Count of Misclassified Samples by Class')\n",
    "    wandb.log({'count-misclassified': wandb.Image(fig)})\n",
    "\n",
    "\n",
    "def plot_scores(precision: list, recall: list, f1: list) -> None:\n",
    "    \"\"\"Plot the precision, recall, and f1 scores for each class.\"\"\"\n",
    "    # create a bar plot\n",
    "    x = range(10)\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots()\n",
    "    _ = ax.bar(x, precision, width, label='Precision')\n",
    "    _= ax.bar([i + width for i in x], recall, width, label='Recall')\n",
    "    _ = ax.bar([i + 2 * width for i in x], f1, width, label='F1')\n",
    "    # add labels, title, and legend\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Accuracy Metrics by Class')\n",
    "    ax.set_xticks([i + width for i in x])\n",
    "    ax.set_xticklabels(range(10))\n",
    "    ax.legend()\n",
    "    # find the minimum and maximum score values (from precision, recall, and f1 lists) and set the\n",
    "    # y limits slightly wider to make the plot easier to read\n",
    "    ymin = min(*precision, *recall, *f1)\n",
    "    ymax = max(*precision, *recall, *f1)\n",
    "    ax.set_ylim([ymin - 0.03, min(ymax + 0.03, 1)])\n",
    "    wandb.log({'scores': wandb.Image(fig)})\n",
    "\n",
    "\n",
    "def test(model: nn.Module, test_loader: DataLoader, criterion: callable) -> None:\n",
    "    \"\"\"Tests the model on the test set. Logs the accuracy to the console and to wandb.\"\"\"\n",
    "    model.eval()\n",
    "    avg_test_loss = calculate_average_loss(data_loader=test_loader, model=model, loss_func=criterion)  # noqa\n",
    "    logging.info(f\"Average Loss on test set: {avg_test_loss:.3f}\")\n",
    "    wandb.log({'test_loss': avg_test_loss})\n",
    "\n",
    "    # Log confusion matrix\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(DEVICE), y.cpu().numpy()  # noqa: PLW2901\n",
    "            outputs = model(x)\n",
    "            predictions = torch.argmax(outputs.data, dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(y)\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    plot_misclassified_sample(num_images=30, images=x_test, predictions=all_predictions, labels=all_labels)  # noqa\n",
    "    plot_heatmap(predictions=all_predictions, labels=all_labels)\n",
    "\n",
    "    # for each class, calculate the accuracy metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true=all_labels, y_pred=all_predictions)  # noqa\n",
    "    score_table = wandb.Table(columns=[\"class\", \"precision\", \"recall\", \"f1\"])\n",
    "    for i in range(10):\n",
    "        score_table.add_data(str(i), precision[i], recall[i], f1[i])\n",
    "    wandb.log({\"score_table\": score_table})\n",
    "    plot_scores(precision, recall, f1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true=all_labels,\n",
    "        y_pred=all_predictions,\n",
    "        average='weighted',\n",
    "    )\n",
    "    logging.info(f\"Weighted Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "    wandb.log({'weighted_precision': precision, 'weighted_recall': recall, 'weighted_f1': f1})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    x, _ = next(iter(test_loader))\n",
    "    torch.onnx.export(model, x.to(DEVICE) , 'model.onnx')\n",
    "    wandb.save('model.onnx')\n",
    "\n",
    "\n",
    "def model_pipeline(config: dict | None = None) -> nn.Module:\n",
    "    \"\"\"Builds the model and runs it.\"\"\"\n",
    "    # if no config is provided, a sweep is running, and we will get the config from wandb\n",
    "    project = config.pop('project') if config else None\n",
    "    tags = config.pop('tags', None) if config else None\n",
    "    notes = config.pop('notes', None) if config else None\n",
    "    with wandb.init(project=project, config=config, tags=tags, notes=notes):\n",
    "        config = wandb.config\n",
    "        pprint.pprint(config)\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, validation_loader, test_loader, criterion, optimizer_creator \\\n",
    "            = make(config)\n",
    "        print(model)\n",
    "        # and use them to train the model\n",
    "        train(\n",
    "            model, train_loader, validation_loader, criterion, optimizer_creator, config,\n",
    "        )\n",
    "        # and test its final performance\n",
    "        test(model, test_loader, criterion)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'project': 'pytorch-demo',\n",
    "#     'tags': ['pytorch', 'demo'],\n",
    "#     'notes': 'First run with a simple CNN',\n",
    "#     'epochs': 20,\n",
    "#     'classes': 10,\n",
    "#     'kernels': [16, 32],\n",
    "#     'batch_size': 64,\n",
    "#     'optimizer': 'Adam',\n",
    "#     # 'learning_rates': 0.005,\n",
    "#     'learning_rates': [0.005, 0.001, 0.0005],\n",
    "#     'dataset': 'MNIST',\n",
    "#     'architecture': 'CNN',\n",
    "# }\n",
    "# # Build, train and analyze the model with the pipeline\n",
    "# model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'test_loss'},\n",
      " 'parameters': {'architecture': {'value': 'CNN'},\n",
      "                'batch_size': {'values': [64, 128, 256]},\n",
      "                'early_stopped_count': {'value': 5},\n",
      "                'epochs': {'value': 20},\n",
      "                'kernels': {'values': [[8, 16], [16, 32], [32, 64], [16, 64]]},\n",
      "                'learning_rate': {'values': [0.01, 0.005, 0.001]},\n",
      "                'notes': {'value': 'Notes'},\n",
      "                'optimizer': {'values': ['adam', 'sgd']},\n",
      "                'project': {'value': 'pytorch-demo-v2'},\n",
      "                'tags': {'value': ['pytorch', 'demo']}}}\n",
      "Number of grid combinations: 72\n"
     ]
    }
   ],
   "source": [
    "project_name = 'pytorch-demo-v2'\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    # 'method': 'random',\n",
    "    # 'method': 'bayes',\n",
    "    'metric': {\n",
    "        # set this to `validation_loss` if using `bayes` method above\n",
    "        # setting this to test_loss so that w&b will use that to generate the parallel coordinates\n",
    "        'name': 'test_loss',\n",
    "        'goal': 'minimize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        # configuration parameters (fixed; via `value`)\n",
    "        'project': {'value': project_name},\n",
    "        'tags': {'value': ['pytorch', 'demo']},\n",
    "        'notes': {'value': 'Notes'},\n",
    "        'epochs': {'value': 20},\n",
    "        'architecture': {'value': 'CNN'},\n",
    "        'early_stopped_count': {'value': 5},  # we decrease the learning rate 5 times\n",
    "        # hyperparameters (tuned; via e.g `values` or `distribution`)\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd'],\n",
    "            },\n",
    "        'kernels': {\n",
    "            'values': [\n",
    "                [8, 16],\n",
    "                [16, 32],\n",
    "                [32, 64],\n",
    "                [16, 64],\n",
    "            ],\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [64, 128, 256],\n",
    "        },\n",
    "        # 'batch_size': {\n",
    "        #     # integers between 32 and 256 with evenly-distributed logarithms\n",
    "        #     'distribution': 'q_log_uniform_values',\n",
    "        #     'q': 8,\n",
    "        #     'min': 32,\n",
    "        #     'max': 256,\n",
    "        # },\n",
    "        'learning_rate': {\n",
    "            'values': [0.01, 0.005, 0.001],\n",
    "        },\n",
    "        # 'dropout': {\n",
    "        #     'values': [0.3, 0.4, 0.5],\n",
    "        # },\n",
    "    },\n",
    "}\n",
    "pprint.pprint(sweep_config)\n",
    "print(f\"Number of grid combinations: {np.cumprod([len(v['values']) for v in sweep_config['parameters'].values() if 'values' in v])[-1]}\")  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7uanjfsw\n",
      "Sweep URL: https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw\n",
      "7uanjfsw\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "print(sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 17:18:55 - INFO     | Starting sweep agent: entity=None, project=None, count=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7m4j8lgn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [8, 16]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "2023-12-30 17:18:55 - ERROR    | Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_171856-7m4j8lgn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/7m4j8lgn' target=\"_blank\">distinctive-sweep-1</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/7m4j8lgn' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/7m4j8lgn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [8, 16], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:18:57 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 17:18:58 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 10.833 | 42.021\n",
      "2023-12-30 17:18:58 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 3.656 | 0.784\n",
      "2023-12-30 17:18:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 0.593 | 0.438\n",
      "2023-12-30 17:18:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 0.442 | 0.359\n",
      "2023-12-30 17:18:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 0.369 | 0.375\n",
      "2023-12-30 17:19:00 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 0.339 | 0.335\n",
      "2023-12-30 17:19:00 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 0.312 | 0.322\n",
      "2023-12-30 17:19:00 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 0.322 | 0.298\n",
      "2023-12-30 17:19:01 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 0.318 | 0.318\n",
      "2023-12-30 17:19:01 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 0.358 | 0.362\n",
      "2023-12-30 17:19:01 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 0.328 | 0.272\n",
      "2023-12-30 17:19:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 0.314 | 0.271\n",
      "2023-12-30 17:19:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 0.260 | 0.266\n",
      "2023-12-30 17:19:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.274 | 0.263\n",
      "2023-12-30 17:19:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.277 | 0.361\n",
      "2023-12-30 17:19:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.258 | 0.221\n",
      "2023-12-30 17:19:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.271 | 0.251\n",
      "2023-12-30 17:19:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.245 | 0.268\n",
      "2023-12-30 17:19:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.316 | 0.283\n",
      "2023-12-30 17:19:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.230 | 0.298\n",
      "2023-12-30 17:19:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.243 | 0.248\n",
      "2023-12-30 17:19:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.248 | 0.241\n",
      "2023-12-30 17:19:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.272 | 0.265\n",
      "2023-12-30 17:19:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.310 | 0.293\n",
      "2023-12-30 17:19:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.257 | 0.222\n",
      "2023-12-30 17:19:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.224 | 0.254\n",
      "2023-12-30 17:19:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.253 | 0.255\n",
      "2023-12-30 17:19:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.213 | 0.268\n",
      "2023-12-30 17:19:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.282 | 0.227\n",
      "2023-12-30 17:19:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.219 | 0.225\n",
      "2023-12-30 17:19:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.226 | 0.196\n",
      "2023-12-30 17:19:08 - INFO     | Early stopping: loss decreased (inf -> 0.235; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:11<03:29, 11.01s/it]2023-12-30 17:19:08 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 17:19:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.207 | 0.241\n",
      "2023-12-30 17:19:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.202 | 0.186\n",
      "2023-12-30 17:19:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.212 | 0.214\n",
      "2023-12-30 17:19:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.214 | 0.238\n",
      "2023-12-30 17:19:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.244 | 0.240\n",
      "2023-12-30 17:19:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.200 | 0.221\n",
      "2023-12-30 17:19:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.222 | 0.218\n",
      "2023-12-30 17:19:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.220 | 0.231\n",
      "2023-12-30 17:19:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.217 | 0.233\n",
      "2023-12-30 17:19:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.218 | 0.223\n",
      "2023-12-30 17:19:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.229 | 0.199\n",
      "2023-12-30 17:19:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.180 | 0.259\n",
      "2023-12-30 17:19:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.266 | 0.235\n",
      "2023-12-30 17:19:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.209 | 0.229\n",
      "2023-12-30 17:19:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.246 | 0.247\n",
      "2023-12-30 17:19:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.261 | 0.291\n",
      "2023-12-30 17:19:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.262 | 0.241\n",
      "2023-12-30 17:19:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.274 | 0.228\n",
      "2023-12-30 17:19:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.221 | 0.213\n",
      "2023-12-30 17:19:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.235 | 0.198\n",
      "2023-12-30 17:19:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.228 | 0.207\n",
      "2023-12-30 17:19:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.223 | 0.208\n",
      "2023-12-30 17:19:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.185 | 0.233\n",
      "2023-12-30 17:19:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.246 | 0.189\n",
      "2023-12-30 17:19:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.201 | 0.236\n",
      "2023-12-30 17:19:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.243 | 0.177\n",
      "2023-12-30 17:19:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.235 | 0.176\n",
      "2023-12-30 17:19:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.202 | 0.218\n",
      "2023-12-30 17:19:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.178 | 0.178\n",
      "2023-12-30 17:19:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.177 | 0.218\n",
      "2023-12-30 17:19:18 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.233 | 0.202\n",
      "2023-12-30 17:19:18 - INFO     | Early stopping: loss decreased (0.235 -> 0.213; -9.2%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:04, 10.22s/it]2023-12-30 17:19:18 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 17:19:18 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.252 | 0.212\n",
      "2023-12-30 17:19:19 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.210 | 0.226\n",
      "2023-12-30 17:19:19 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.237 | 0.205\n",
      "2023-12-30 17:19:19 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.208 | 0.224\n",
      "2023-12-30 17:19:20 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.155 | 0.236\n",
      "2023-12-30 17:19:20 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.252 | 0.234\n",
      "2023-12-30 17:19:20 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.250 | 0.279\n",
      "2023-12-30 17:19:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.202 | 0.225\n",
      "2023-12-30 17:19:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.234 | 0.236\n",
      "2023-12-30 17:19:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.226 | 0.201\n",
      "2023-12-30 17:19:22 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.196 | 0.199\n",
      "2023-12-30 17:19:22 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.219 | 0.181\n",
      "2023-12-30 17:19:22 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.211 | 0.247\n",
      "2023-12-30 17:19:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.202 | 0.240\n",
      "2023-12-30 17:19:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.232 | 0.265\n",
      "2023-12-30 17:19:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.214 | 0.284\n",
      "2023-12-30 17:19:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.261 | 0.249\n",
      "2023-12-30 17:19:24 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.203 | 0.228\n",
      "2023-12-30 17:19:24 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.241 | 0.210\n",
      "2023-12-30 17:19:24 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.236 | 0.215\n",
      "2023-12-30 17:19:25 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.240 | 0.207\n",
      "2023-12-30 17:19:25 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.192 | 0.232\n",
      "2023-12-30 17:19:25 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.209 | 0.238\n",
      "2023-12-30 17:19:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.248 | 0.231\n",
      "2023-12-30 17:19:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.202 | 0.182\n",
      "2023-12-30 17:19:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.208 | 0.212\n",
      "2023-12-30 17:19:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.195 | 0.195\n",
      "2023-12-30 17:19:27 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.168 | 0.250\n",
      "2023-12-30 17:19:27 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.197 | 0.245\n",
      "2023-12-30 17:19:27 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.229 | 0.210\n",
      "2023-12-30 17:19:28 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.191 | 0.241\n",
      "2023-12-30 17:19:28 - INFO     | Early stopping: loss decreased (0.213 -> 0.193; -9.6%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:51, 10.07s/it]2023-12-30 17:19:28 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 17:19:28 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.121 | 0.193\n",
      "2023-12-30 17:19:28 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.213 | 0.278\n",
      "2023-12-30 17:19:29 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.164 | 0.197\n",
      "2023-12-30 17:19:29 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.162 | 0.177\n",
      "2023-12-30 17:19:29 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.188 | 0.208\n",
      "2023-12-30 17:19:30 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.201 | 0.206\n",
      "2023-12-30 17:19:30 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.213 | 0.227\n",
      "2023-12-30 17:19:30 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.192 | 0.245\n",
      "2023-12-30 17:19:31 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.289 | 0.256\n",
      "2023-12-30 17:19:31 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.187 | 0.243\n",
      "2023-12-30 17:19:31 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.229 | 0.195\n",
      "2023-12-30 17:19:32 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.217 | 0.221\n",
      "2023-12-30 17:19:32 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.211 | 0.192\n",
      "2023-12-30 17:19:32 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.248 | 0.238\n",
      "2023-12-30 17:19:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.193 | 0.238\n",
      "2023-12-30 17:19:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.202 | 0.193\n",
      "2023-12-30 17:19:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.197 | 0.215\n",
      "2023-12-30 17:19:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.203 | 0.184\n",
      "2023-12-30 17:19:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.218 | 0.218\n",
      "2023-12-30 17:19:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.224 | 0.245\n",
      "2023-12-30 17:19:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.225 | 0.183\n",
      "2023-12-30 17:19:35 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.224 | 0.220\n",
      "2023-12-30 17:19:35 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.196 | 0.235\n",
      "2023-12-30 17:19:35 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.218 | 0.198\n",
      "2023-12-30 17:19:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.241 | 0.259\n",
      "2023-12-30 17:19:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.220 | 0.216\n",
      "2023-12-30 17:19:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.206 | 0.190\n",
      "2023-12-30 17:19:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.184 | 0.239\n",
      "2023-12-30 17:19:37 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.194 | 0.246\n",
      "2023-12-30 17:19:37 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.242 | 0.200\n",
      "2023-12-30 17:19:37 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.166 | 0.226\n",
      "2023-12-30 17:19:38 - INFO     | Early stopping: no decrease (0.193 vs 0.215); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:38,  9.92s/it]2023-12-30 17:19:38 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 17:19:38 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.187 | 0.215\n",
      "2023-12-30 17:19:38 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.188 | 0.205\n",
      "2023-12-30 17:19:38 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.175 | 0.190\n",
      "2023-12-30 17:19:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.205 | 0.222\n",
      "2023-12-30 17:19:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.240 | 0.215\n",
      "2023-12-30 17:19:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.185 | 0.216\n",
      "2023-12-30 17:19:40 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.226 | 0.218\n",
      "2023-12-30 17:19:40 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.236 | 0.289\n",
      "2023-12-30 17:19:40 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.247 | 0.213\n",
      "2023-12-30 17:19:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.207 | 0.211\n",
      "2023-12-30 17:19:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.173 | 0.282\n",
      "2023-12-30 17:19:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.198 | 0.198\n",
      "2023-12-30 17:19:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.164 | 0.236\n",
      "2023-12-30 17:19:42 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.189 | 0.177\n",
      "2023-12-30 17:19:42 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.245 | 0.226\n",
      "2023-12-30 17:19:42 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.239 | 0.273\n",
      "2023-12-30 17:19:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.215 | 0.229\n",
      "2023-12-30 17:19:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.258 | 0.246\n",
      "2023-12-30 17:19:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.205 | 0.219\n",
      "2023-12-30 17:19:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.280 | 0.229\n",
      "2023-12-30 17:19:44 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.268 | 0.228\n",
      "2023-12-30 17:19:44 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.206 | 0.213\n",
      "2023-12-30 17:19:44 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.188 | 0.203\n",
      "2023-12-30 17:19:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.179 | 0.217\n",
      "2023-12-30 17:19:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.197 | 0.209\n",
      "2023-12-30 17:19:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.195 | 0.204\n",
      "2023-12-30 17:19:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.227 | 0.250\n",
      "2023-12-30 17:19:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.209 | 0.193\n",
      "2023-12-30 17:19:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.223 | 0.184\n",
      "2023-12-30 17:19:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.167 | 0.232\n",
      "2023-12-30 17:19:47 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.212 | 0.174\n",
      "2023-12-30 17:19:47 - INFO     | Early stopping: no decrease (0.193 vs 0.187); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:49<02:26,  9.75s/it]2023-12-30 17:19:47 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 17:19:47 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.396 | 0.182\n",
      "2023-12-30 17:19:48 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.184 | 0.192\n",
      "2023-12-30 17:19:48 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.223 | 0.219\n",
      "2023-12-30 17:19:48 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.211 | 0.210\n",
      "2023-12-30 17:19:49 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.217 | 0.209\n",
      "2023-12-30 17:19:49 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.195 | 0.221\n",
      "2023-12-30 17:19:49 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.209 | 0.191\n",
      "2023-12-30 17:19:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.180 | 0.216\n",
      "2023-12-30 17:19:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.198 | 0.259\n",
      "2023-12-30 17:19:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.189 | 0.222\n",
      "2023-12-30 17:19:51 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.223 | 0.210\n",
      "2023-12-30 17:19:51 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.167 | 0.209\n",
      "2023-12-30 17:19:51 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.183 | 0.219\n",
      "2023-12-30 17:19:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.216 | 0.182\n",
      "2023-12-30 17:19:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.185 | 0.200\n",
      "2023-12-30 17:19:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.231 | 0.216\n",
      "2023-12-30 17:19:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.210 | 0.202\n",
      "2023-12-30 17:19:53 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.192 | 0.214\n",
      "2023-12-30 17:19:53 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.204 | 0.215\n",
      "2023-12-30 17:19:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.213 | 0.204\n",
      "2023-12-30 17:19:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.190 | 0.184\n",
      "2023-12-30 17:19:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.158 | 0.203\n",
      "2023-12-30 17:19:55 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.197 | 0.250\n",
      "2023-12-30 17:19:55 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.237 | 0.179\n",
      "2023-12-30 17:19:55 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.182 | 0.212\n",
      "2023-12-30 17:19:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.197 | 0.207\n",
      "2023-12-30 17:19:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.190 | 0.181\n",
      "2023-12-30 17:19:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.194 | 0.207\n",
      "2023-12-30 17:19:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.208 | 0.182\n",
      "2023-12-30 17:19:57 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.218 | 0.177\n",
      "2023-12-30 17:19:57 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.170 | 0.215\n",
      "2023-12-30 17:19:57 - INFO     | Early stopping: no decrease (0.193 vs 0.224); counter: 3 out of 3\n",
      "2023-12-30 17:19:57 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:19:57 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 30%|███       | 6/20 [01:00<02:19,  9.94s/it]2023-12-30 17:19:57 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 17:19:58 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.687 | 0.235\n",
      "2023-12-30 17:19:58 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.185 | 0.163\n",
      "2023-12-30 17:19:58 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.134 | 0.154\n",
      "2023-12-30 17:19:58 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.149 | 0.154\n",
      "2023-12-30 17:19:59 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.143 | 0.178\n",
      "2023-12-30 17:19:59 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.135 | 0.156\n",
      "2023-12-30 17:19:59 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.134 | 0.152\n",
      "2023-12-30 17:20:00 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.131 | 0.158\n",
      "2023-12-30 17:20:00 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.166 | 0.165\n",
      "2023-12-30 17:20:00 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.155 | 0.157\n",
      "2023-12-30 17:20:00 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.163 | 0.155\n",
      "2023-12-30 17:20:01 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.134 | 0.178\n",
      "2023-12-30 17:20:01 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.131 | 0.157\n",
      "2023-12-30 17:20:01 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.108 | 0.157\n",
      "2023-12-30 17:20:02 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.171 | 0.160\n",
      "2023-12-30 17:20:02 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.159 | 0.165\n",
      "2023-12-30 17:20:02 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.142 | 0.170\n",
      "2023-12-30 17:20:03 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.159 | 0.168\n",
      "2023-12-30 17:20:03 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.148 | 0.162\n",
      "2023-12-30 17:20:04 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.146 | 0.150\n",
      "2023-12-30 17:20:04 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.127 | 0.139\n",
      "2023-12-30 17:20:04 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.114 | 0.145\n",
      "2023-12-30 17:20:05 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.150 | 0.151\n",
      "2023-12-30 17:20:05 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.127 | 0.152\n",
      "2023-12-30 17:20:05 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.128 | 0.169\n",
      "2023-12-30 17:20:05 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.144 | 0.166\n",
      "2023-12-30 17:20:06 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.152 | 0.172\n",
      "2023-12-30 17:20:06 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.226 | 0.182\n",
      "2023-12-30 17:20:06 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.198 | 0.157\n",
      "2023-12-30 17:20:07 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.145 | 0.162\n",
      "2023-12-30 17:20:07 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.135 | 0.175\n",
      "2023-12-30 17:20:07 - INFO     | Early stopping: loss decreased (0.193 -> 0.166; -14.1%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:09<02:08,  9.91s/it]2023-12-30 17:20:07 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 17:20:07 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.079 | 0.162\n",
      "2023-12-30 17:20:08 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.153 | 0.163\n",
      "2023-12-30 17:20:08 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.133 | 0.171\n",
      "2023-12-30 17:20:08 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.135 | 0.165\n",
      "2023-12-30 17:20:09 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.171 | 0.177\n",
      "2023-12-30 17:20:09 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.147 | 0.154\n",
      "2023-12-30 17:20:09 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.135 | 0.148\n",
      "2023-12-30 17:20:09 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.126 | 0.166\n",
      "2023-12-30 17:20:10 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.176 | 0.149\n",
      "2023-12-30 17:20:10 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.142 | 0.179\n",
      "2023-12-30 17:20:10 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.155 | 0.166\n",
      "2023-12-30 17:20:11 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.151 | 0.163\n",
      "2023-12-30 17:20:11 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.137 | 0.152\n",
      "2023-12-30 17:20:11 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.140 | 0.164\n",
      "2023-12-30 17:20:12 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.165 | 0.200\n",
      "2023-12-30 17:20:12 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.142 | 0.221\n",
      "2023-12-30 17:20:12 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.166 | 0.169\n",
      "2023-12-30 17:20:12 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.135 | 0.135\n",
      "2023-12-30 17:20:13 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.117 | 0.159\n",
      "2023-12-30 17:20:13 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.132 | 0.138\n",
      "2023-12-30 17:20:13 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.127 | 0.134\n",
      "2023-12-30 17:20:14 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.126 | 0.141\n",
      "2023-12-30 17:20:14 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.120 | 0.166\n",
      "2023-12-30 17:20:14 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.135 | 0.157\n",
      "2023-12-30 17:20:15 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.163 | 0.139\n",
      "2023-12-30 17:20:15 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.107 | 0.165\n",
      "2023-12-30 17:20:15 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.125 | 0.149\n",
      "2023-12-30 17:20:15 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.108 | 0.156\n",
      "2023-12-30 17:20:16 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.152 | 0.166\n",
      "2023-12-30 17:20:16 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.149 | 0.171\n",
      "2023-12-30 17:20:16 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.146 | 0.144\n",
      "2023-12-30 17:20:17 - INFO     | Early stopping: no decrease (0.166 vs 0.163); counter: 1 out of 3\n",
      " 40%|████      | 8/20 [01:19<01:57,  9.81s/it]2023-12-30 17:20:17 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 17:20:17 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.319 | 0.168\n",
      "2023-12-30 17:20:17 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.138 | 0.150\n",
      "2023-12-30 17:20:18 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.107 | 0.131\n",
      "2023-12-30 17:20:18 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.104 | 0.138\n",
      "2023-12-30 17:20:18 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.133 | 0.154\n",
      "2023-12-30 17:20:19 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.121 | 0.145\n",
      "2023-12-30 17:20:19 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.129 | 0.159\n",
      "2023-12-30 17:20:19 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.139 | 0.154\n",
      "2023-12-30 17:20:20 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.146 | 0.175\n",
      "2023-12-30 17:20:20 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.127 | 0.158\n",
      "2023-12-30 17:20:20 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.130 | 0.173\n",
      "2023-12-30 17:20:21 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.114 | 0.172\n",
      "2023-12-30 17:20:21 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.136 | 0.149\n",
      "2023-12-30 17:20:21 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.195 | 0.170\n",
      "2023-12-30 17:20:21 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.157 | 0.186\n",
      "2023-12-30 17:20:22 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.125 | 0.155\n",
      "2023-12-30 17:20:22 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.166 | 0.182\n",
      "2023-12-30 17:20:22 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.117 | 0.139\n",
      "2023-12-30 17:20:23 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.150 | 0.137\n",
      "2023-12-30 17:20:23 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.093 | 0.157\n",
      "2023-12-30 17:20:23 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.133 | 0.159\n",
      "2023-12-30 17:20:23 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.124 | 0.156\n",
      "2023-12-30 17:20:24 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.148 | 0.156\n",
      "2023-12-30 17:20:24 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.148 | 0.142\n",
      "2023-12-30 17:20:24 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.152 | 0.150\n",
      "2023-12-30 17:20:25 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.124 | 0.148\n",
      "2023-12-30 17:20:25 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.128 | 0.153\n",
      "2023-12-30 17:20:25 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.131 | 0.140\n",
      "2023-12-30 17:20:25 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.150 | 0.148\n",
      "2023-12-30 17:20:26 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.125 | 0.134\n",
      "2023-12-30 17:20:26 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.131 | 0.176\n",
      "2023-12-30 17:20:26 - INFO     | Early stopping: loss decreased (0.166 -> 0.153; -7.5%). Caching model state.\n",
      " 45%|████▌     | 9/20 [01:28<01:46,  9.72s/it]2023-12-30 17:20:26 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 17:20:27 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.069 | 0.155\n",
      "2023-12-30 17:20:27 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.126 | 0.159\n",
      "2023-12-30 17:20:27 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.124 | 0.145\n",
      "2023-12-30 17:20:27 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.106 | 0.150\n",
      "2023-12-30 17:20:28 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.143 | 0.143\n",
      "2023-12-30 17:20:28 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.124 | 0.168\n",
      "2023-12-30 17:20:28 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.148 | 0.158\n",
      "2023-12-30 17:20:29 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.118 | 0.143\n",
      "2023-12-30 17:20:29 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.103 | 0.162\n",
      "2023-12-30 17:20:29 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.126 | 0.140\n",
      "2023-12-30 17:20:30 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.158 | 0.131\n",
      "2023-12-30 17:20:30 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.097 | 0.135\n",
      "2023-12-30 17:20:30 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.118 | 0.150\n",
      "2023-12-30 17:20:31 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.127 | 0.185\n",
      "2023-12-30 17:20:31 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.165 | 0.166\n",
      "2023-12-30 17:20:31 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.157 | 0.144\n",
      "2023-12-30 17:20:31 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.137 | 0.139\n",
      "2023-12-30 17:20:32 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.139 | 0.159\n",
      "2023-12-30 17:20:32 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.131 | 0.162\n",
      "2023-12-30 17:20:32 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.154 | 0.145\n",
      "2023-12-30 17:20:33 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.156 | 0.151\n",
      "2023-12-30 17:20:33 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.123 | 0.174\n",
      "2023-12-30 17:20:33 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.145 | 0.139\n",
      "2023-12-30 17:20:34 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.136 | 0.141\n",
      "2023-12-30 17:20:34 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.103 | 0.161\n",
      "2023-12-30 17:20:34 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.140 | 0.143\n",
      "2023-12-30 17:20:34 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.115 | 0.140\n",
      "2023-12-30 17:20:35 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.115 | 0.211\n",
      "2023-12-30 17:20:35 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.145 | 0.155\n",
      "2023-12-30 17:20:35 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.153 | 0.160\n",
      "2023-12-30 17:20:36 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.115 | 0.167\n",
      "2023-12-30 17:20:36 - INFO     | Early stopping: no decrease (0.153 vs 0.163); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:38<01:37,  9.75s/it]2023-12-30 17:20:36 - INFO     | Epoch: 10 | Learning Rate: 0.005\n",
      "2023-12-30 17:20:36 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 560064 examples: 0.129 | 0.161\n",
      "2023-12-30 17:20:37 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 561920 examples: 0.131 | 0.141\n",
      "2023-12-30 17:20:37 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 563776 examples: 0.115 | 0.132\n",
      "2023-12-30 17:20:37 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 565632 examples: 0.099 | 0.132\n",
      "2023-12-30 17:20:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 567488 examples: 0.080 | 0.148\n",
      "2023-12-30 17:20:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 569344 examples: 0.118 | 0.135\n",
      "2023-12-30 17:20:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 571200 examples: 0.118 | 0.140\n",
      "2023-12-30 17:20:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 573056 examples: 0.132 | 0.145\n",
      "2023-12-30 17:20:39 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 574912 examples: 0.115 | 0.145\n",
      "2023-12-30 17:20:39 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 576768 examples: 0.121 | 0.154\n",
      "2023-12-30 17:20:39 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 578624 examples: 0.133 | 0.160\n",
      "2023-12-30 17:20:40 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 580480 examples: 0.132 | 0.187\n",
      "2023-12-30 17:20:40 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 582336 examples: 0.188 | 0.161\n",
      "2023-12-30 17:20:40 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 584192 examples: 0.143 | 0.161\n",
      "2023-12-30 17:20:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 586048 examples: 0.138 | 0.146\n",
      "2023-12-30 17:20:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 587904 examples: 0.133 | 0.163\n",
      "2023-12-30 17:20:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 589760 examples: 0.153 | 0.186\n",
      "2023-12-30 17:20:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 591616 examples: 0.088 | 0.174\n",
      "2023-12-30 17:20:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 593472 examples: 0.122 | 0.163\n",
      "2023-12-30 17:20:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 595328 examples: 0.140 | 0.128\n",
      "2023-12-30 17:20:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 597184 examples: 0.090 | 0.150\n",
      "2023-12-30 17:20:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 599040 examples: 0.139 | 0.175\n",
      "2023-12-30 17:20:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 600896 examples: 0.107 | 0.150\n",
      "2023-12-30 17:20:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 602752 examples: 0.136 | 0.148\n",
      "2023-12-30 17:20:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 604608 examples: 0.114 | 0.139\n",
      "2023-12-30 17:20:44 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 606464 examples: 0.165 | 0.179\n",
      "2023-12-30 17:20:44 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 608320 examples: 0.158 | 0.142\n",
      "2023-12-30 17:20:44 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 610176 examples: 0.135 | 0.212\n",
      "2023-12-30 17:20:45 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 612032 examples: 0.132 | 0.144\n",
      "2023-12-30 17:20:45 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 613888 examples: 0.100 | 0.140\n",
      "2023-12-30 17:20:45 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 615744 examples: 0.120 | 0.142\n",
      "2023-12-30 17:20:45 - INFO     | Early stopping: loss decreased (0.153 -> 0.134; -12.4%). Caching model state.\n",
      " 55%|█████▌    | 11/20 [01:48<01:26,  9.62s/it]2023-12-30 17:20:45 - INFO     | Epoch: 11 | Learning Rate: 0.005\n",
      "2023-12-30 17:20:46 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 616064 examples: 0.150 | 0.133\n",
      "2023-12-30 17:20:46 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 617920 examples: 0.148 | 0.160\n",
      "2023-12-30 17:20:46 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 619776 examples: 0.136 | 0.153\n",
      "2023-12-30 17:20:47 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 621632 examples: 0.118 | 0.141\n",
      "2023-12-30 17:20:47 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 623488 examples: 0.116 | 0.141\n",
      "2023-12-30 17:20:47 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 625344 examples: 0.101 | 0.132\n",
      "2023-12-30 17:20:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 627200 examples: 0.097 | 0.229\n",
      "2023-12-30 17:20:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 629056 examples: 0.161 | 0.176\n",
      "2023-12-30 17:20:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 630912 examples: 0.133 | 0.151\n",
      "2023-12-30 17:20:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 632768 examples: 0.128 | 0.168\n",
      "2023-12-30 17:20:49 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 634624 examples: 0.104 | 0.150\n",
      "2023-12-30 17:20:49 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 636480 examples: 0.127 | 0.160\n",
      "2023-12-30 17:20:49 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 638336 examples: 0.130 | 0.141\n",
      "2023-12-30 17:20:50 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 640192 examples: 0.124 | 0.156\n",
      "2023-12-30 17:20:50 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 642048 examples: 0.136 | 0.165\n",
      "2023-12-30 17:20:50 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 643904 examples: 0.155 | 0.154\n",
      "2023-12-30 17:20:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 645760 examples: 0.139 | 0.159\n",
      "2023-12-30 17:20:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 647616 examples: 0.124 | 0.140\n",
      "2023-12-30 17:20:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 649472 examples: 0.111 | 0.157\n",
      "2023-12-30 17:20:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 651328 examples: 0.113 | 0.144\n",
      "2023-12-30 17:20:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 653184 examples: 0.109 | 0.157\n",
      "2023-12-30 17:20:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 655040 examples: 0.197 | 0.156\n",
      "2023-12-30 17:20:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 656896 examples: 0.225 | 0.184\n",
      "2023-12-30 17:20:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 658752 examples: 0.140 | 0.177\n",
      "2023-12-30 17:20:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 660608 examples: 0.133 | 0.172\n",
      "2023-12-30 17:20:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 662464 examples: 0.111 | 0.170\n",
      "2023-12-30 17:20:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 664320 examples: 0.157 | 0.145\n",
      "2023-12-30 17:20:54 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 666176 examples: 0.146 | 0.147\n",
      "2023-12-30 17:20:54 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 668032 examples: 0.117 | 0.143\n",
      "2023-12-30 17:20:54 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 669888 examples: 0.129 | 0.137\n",
      "2023-12-30 17:20:55 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 671744 examples: 0.081 | 0.137\n",
      "2023-12-30 17:20:55 - INFO     | Early stopping: no decrease (0.134 vs 0.136); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [01:57<01:16,  9.57s/it]2023-12-30 17:20:55 - INFO     | Epoch: 12 | Learning Rate: 0.005\n",
      "2023-12-30 17:20:55 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 672064 examples: 0.056 | 0.137\n",
      "2023-12-30 17:20:56 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 673920 examples: 0.100 | 0.160\n",
      "2023-12-30 17:20:56 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 675776 examples: 0.095 | 0.158\n",
      "2023-12-30 17:20:56 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 677632 examples: 0.105 | 0.162\n",
      "2023-12-30 17:20:57 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 679488 examples: 0.092 | 0.148\n",
      "2023-12-30 17:20:57 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 681344 examples: 0.123 | 0.150\n",
      "2023-12-30 17:20:57 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 683200 examples: 0.148 | 0.161\n",
      "2023-12-30 17:20:57 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 685056 examples: 0.156 | 0.149\n",
      "2023-12-30 17:20:58 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 686912 examples: 0.108 | 0.153\n",
      "2023-12-30 17:20:58 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 688768 examples: 0.126 | 0.153\n",
      "2023-12-30 17:20:58 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 690624 examples: 0.102 | 0.143\n",
      "2023-12-30 17:20:59 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 692480 examples: 0.126 | 0.158\n",
      "2023-12-30 17:20:59 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 694336 examples: 0.125 | 0.162\n",
      "2023-12-30 17:21:00 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 696192 examples: 0.113 | 0.147\n",
      "2023-12-30 17:21:00 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 698048 examples: 0.118 | 0.139\n",
      "2023-12-30 17:21:00 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 699904 examples: 0.134 | 0.127\n",
      "2023-12-30 17:21:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 701760 examples: 0.103 | 0.153\n",
      "2023-12-30 17:21:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 703616 examples: 0.118 | 0.138\n",
      "2023-12-30 17:21:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 705472 examples: 0.099 | 0.132\n",
      "2023-12-30 17:21:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 707328 examples: 0.140 | 0.165\n",
      "2023-12-30 17:21:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 709184 examples: 0.110 | 0.129\n",
      "2023-12-30 17:21:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 711040 examples: 0.128 | 0.147\n",
      "2023-12-30 17:21:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 712896 examples: 0.162 | 0.157\n",
      "2023-12-30 17:21:03 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 714752 examples: 0.106 | 0.146\n",
      "2023-12-30 17:21:03 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 716608 examples: 0.155 | 0.135\n",
      "2023-12-30 17:21:03 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 718464 examples: 0.109 | 0.149\n",
      "2023-12-30 17:21:04 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 720320 examples: 0.121 | 0.159\n",
      "2023-12-30 17:21:04 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 722176 examples: 0.131 | 0.179\n",
      "2023-12-30 17:21:04 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 724032 examples: 0.114 | 0.186\n",
      "2023-12-30 17:21:05 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 725888 examples: 0.157 | 0.166\n",
      "2023-12-30 17:21:05 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 727744 examples: 0.125 | 0.142\n",
      "2023-12-30 17:21:05 - INFO     | Early stopping: no decrease (0.134 vs 0.149); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:07<01:08,  9.74s/it]2023-12-30 17:21:05 - INFO     | Epoch: 13 | Learning Rate: 0.005\n",
      "2023-12-30 17:21:05 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 728064 examples: 0.012 | 0.149\n",
      "2023-12-30 17:21:06 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 729920 examples: 0.092 | 0.139\n",
      "2023-12-30 17:21:06 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 731776 examples: 0.130 | 0.169\n",
      "2023-12-30 17:21:06 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 733632 examples: 0.102 | 0.160\n",
      "2023-12-30 17:21:07 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 735488 examples: 0.130 | 0.214\n",
      "2023-12-30 17:21:07 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 737344 examples: 0.154 | 0.154\n",
      "2023-12-30 17:21:07 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 739200 examples: 0.116 | 0.143\n",
      "2023-12-30 17:21:07 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 741056 examples: 0.112 | 0.175\n",
      "2023-12-30 17:21:08 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 742912 examples: 0.160 | 0.165\n",
      "2023-12-30 17:21:08 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 744768 examples: 0.113 | 0.163\n",
      "2023-12-30 17:21:08 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 746624 examples: 0.099 | 0.141\n",
      "2023-12-30 17:21:09 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 748480 examples: 0.120 | 0.148\n",
      "2023-12-30 17:21:09 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 750336 examples: 0.125 | 0.175\n",
      "2023-12-30 17:21:09 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 752192 examples: 0.155 | 0.154\n",
      "2023-12-30 17:21:10 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 754048 examples: 0.111 | 0.150\n",
      "2023-12-30 17:21:10 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 755904 examples: 0.111 | 0.169\n",
      "2023-12-30 17:21:10 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 757760 examples: 0.150 | 0.160\n",
      "2023-12-30 17:21:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 759616 examples: 0.135 | 0.158\n",
      "2023-12-30 17:21:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 761472 examples: 0.125 | 0.172\n",
      "2023-12-30 17:21:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 763328 examples: 0.121 | 0.203\n",
      "2023-12-30 17:21:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 765184 examples: 0.136 | 0.172\n",
      "2023-12-30 17:21:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 767040 examples: 0.118 | 0.171\n",
      "2023-12-30 17:21:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 768896 examples: 0.145 | 0.193\n",
      "2023-12-30 17:21:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 770752 examples: 0.127 | 0.163\n",
      "2023-12-30 17:21:13 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 772608 examples: 0.161 | 0.145\n",
      "2023-12-30 17:21:13 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 774464 examples: 0.155 | 0.166\n",
      "2023-12-30 17:21:13 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 776320 examples: 0.116 | 0.157\n",
      "2023-12-30 17:21:14 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 778176 examples: 0.124 | 0.145\n",
      "2023-12-30 17:21:14 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 780032 examples: 0.152 | 0.169\n",
      "2023-12-30 17:21:14 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 781888 examples: 0.131 | 0.140\n",
      "2023-12-30 17:21:15 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 783744 examples: 0.116 | 0.141\n",
      "2023-12-30 17:21:15 - INFO     | Early stopping: no decrease (0.134 vs 0.144); counter: 3 out of 3\n",
      "2023-12-30 17:21:15 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:21:15 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 70%|███████   | 14/20 [02:17<00:58,  9.77s/it]2023-12-30 17:21:15 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 17:21:15 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.244 | 0.133\n",
      "2023-12-30 17:21:15 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.122 | 0.151\n",
      "2023-12-30 17:21:16 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.095 | 0.122\n",
      "2023-12-30 17:21:16 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.100 | 0.118\n",
      "2023-12-30 17:21:16 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.078 | 0.125\n",
      "2023-12-30 17:21:17 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.099 | 0.142\n",
      "2023-12-30 17:21:17 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.099 | 0.136\n",
      "2023-12-30 17:21:18 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.081 | 0.122\n",
      "2023-12-30 17:21:18 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.088 | 0.116\n",
      "2023-12-30 17:21:18 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.103 | 0.115\n",
      "2023-12-30 17:21:19 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.085 | 0.122\n",
      "2023-12-30 17:21:19 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.078 | 0.129\n",
      "2023-12-30 17:21:19 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.082 | 0.131\n",
      "2023-12-30 17:21:20 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.096 | 0.140\n",
      "2023-12-30 17:21:20 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.084 | 0.128\n",
      "2023-12-30 17:21:20 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.100 | 0.124\n",
      "2023-12-30 17:21:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.108 | 0.117\n",
      "2023-12-30 17:21:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.106 | 0.132\n",
      "2023-12-30 17:21:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.109 | 0.129\n",
      "2023-12-30 17:21:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.074 | 0.115\n",
      "2023-12-30 17:21:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.103 | 0.121\n",
      "2023-12-30 17:21:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.073 | 0.116\n",
      "2023-12-30 17:21:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.077 | 0.129\n",
      "2023-12-30 17:21:23 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.089 | 0.121\n",
      "2023-12-30 17:21:23 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.102 | 0.121\n",
      "2023-12-30 17:21:23 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.092 | 0.129\n",
      "2023-12-30 17:21:24 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.094 | 0.119\n",
      "2023-12-30 17:21:24 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.076 | 0.113\n",
      "2023-12-30 17:21:24 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.097 | 0.119\n",
      "2023-12-30 17:21:25 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.072 | 0.122\n",
      "2023-12-30 17:21:25 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.102 | 0.115\n",
      "2023-12-30 17:21:25 - INFO     | Early stopping: loss decreased (0.134 -> 0.118; -12.3%). Caching model state.\n",
      " 75%|███████▌  | 15/20 [02:27<00:49,  9.90s/it]2023-12-30 17:21:25 - INFO     | Epoch: 15 | Learning Rate: 0.003\n",
      "2023-12-30 17:21:25 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 840064 examples: 0.120 | 0.119\n",
      "2023-12-30 17:21:26 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 841920 examples: 0.073 | 0.124\n",
      "2023-12-30 17:21:26 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 843776 examples: 0.084 | 0.141\n",
      "2023-12-30 17:21:26 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 845632 examples: 0.099 | 0.135\n",
      "2023-12-30 17:21:27 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 847488 examples: 0.076 | 0.132\n",
      "2023-12-30 17:21:27 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 849344 examples: 0.116 | 0.126\n",
      "2023-12-30 17:21:27 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 851200 examples: 0.083 | 0.121\n",
      "2023-12-30 17:21:27 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 853056 examples: 0.091 | 0.115\n",
      "2023-12-30 17:21:28 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 854912 examples: 0.099 | 0.117\n",
      "2023-12-30 17:21:28 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 856768 examples: 0.075 | 0.120\n",
      "2023-12-30 17:21:28 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 858624 examples: 0.080 | 0.131\n",
      "2023-12-30 17:21:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 860480 examples: 0.096 | 0.117\n",
      "2023-12-30 17:21:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 862336 examples: 0.075 | 0.123\n",
      "2023-12-30 17:21:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 864192 examples: 0.094 | 0.122\n",
      "2023-12-30 17:21:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 866048 examples: 0.082 | 0.115\n",
      "2023-12-30 17:21:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 867904 examples: 0.067 | 0.113\n",
      "2023-12-30 17:21:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 869760 examples: 0.075 | 0.132\n",
      "2023-12-30 17:21:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 871616 examples: 0.071 | 0.132\n",
      "2023-12-30 17:21:31 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 873472 examples: 0.124 | 0.116\n",
      "2023-12-30 17:21:31 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 875328 examples: 0.080 | 0.115\n",
      "2023-12-30 17:21:31 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 877184 examples: 0.102 | 0.125\n",
      "2023-12-30 17:21:32 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 879040 examples: 0.096 | 0.120\n",
      "2023-12-30 17:21:32 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 880896 examples: 0.068 | 0.131\n",
      "2023-12-30 17:21:32 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 882752 examples: 0.074 | 0.128\n",
      "2023-12-30 17:21:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 884608 examples: 0.080 | 0.125\n",
      "2023-12-30 17:21:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 886464 examples: 0.107 | 0.117\n",
      "2023-12-30 17:21:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 888320 examples: 0.080 | 0.115\n",
      "2023-12-30 17:21:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 890176 examples: 0.085 | 0.122\n",
      "2023-12-30 17:21:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 892032 examples: 0.091 | 0.113\n",
      "2023-12-30 17:21:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 893888 examples: 0.070 | 0.123\n",
      "2023-12-30 17:21:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 895744 examples: 0.076 | 0.118\n",
      "2023-12-30 17:21:35 - INFO     | Early stopping: no decrease (0.118 vs 0.118); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:37<00:39,  9.79s/it]2023-12-30 17:21:35 - INFO     | Epoch: 16 | Learning Rate: 0.003\n",
      "2023-12-30 17:21:35 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 896064 examples: 0.165 | 0.118\n",
      "2023-12-30 17:21:35 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 897920 examples: 0.074 | 0.111\n",
      "2023-12-30 17:21:35 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 899776 examples: 0.068 | 0.124\n",
      "2023-12-30 17:21:36 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 901632 examples: 0.086 | 0.116\n",
      "2023-12-30 17:21:36 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 903488 examples: 0.072 | 0.117\n",
      "2023-12-30 17:21:36 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 905344 examples: 0.073 | 0.115\n",
      "2023-12-30 17:21:37 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 907200 examples: 0.078 | 0.120\n",
      "2023-12-30 17:21:37 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 909056 examples: 0.082 | 0.115\n",
      "2023-12-30 17:21:37 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 910912 examples: 0.086 | 0.126\n",
      "2023-12-30 17:21:38 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 912768 examples: 0.119 | 0.136\n",
      "2023-12-30 17:21:38 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 914624 examples: 0.087 | 0.122\n",
      "2023-12-30 17:21:38 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 916480 examples: 0.081 | 0.135\n",
      "2023-12-30 17:21:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 918336 examples: 0.101 | 0.114\n",
      "2023-12-30 17:21:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 920192 examples: 0.072 | 0.117\n",
      "2023-12-30 17:21:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 922048 examples: 0.049 | 0.116\n",
      "2023-12-30 17:21:40 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 923904 examples: 0.099 | 0.120\n",
      "2023-12-30 17:21:40 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 925760 examples: 0.065 | 0.113\n",
      "2023-12-30 17:21:40 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 927616 examples: 0.068 | 0.134\n",
      "2023-12-30 17:21:41 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 929472 examples: 0.106 | 0.127\n",
      "2023-12-30 17:21:41 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 931328 examples: 0.096 | 0.131\n",
      "2023-12-30 17:21:41 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 933184 examples: 0.087 | 0.124\n",
      "2023-12-30 17:21:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 935040 examples: 0.087 | 0.121\n",
      "2023-12-30 17:21:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 936896 examples: 0.101 | 0.117\n",
      "2023-12-30 17:21:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 938752 examples: 0.091 | 0.127\n",
      "2023-12-30 17:21:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 940608 examples: 0.066 | 0.112\n",
      "2023-12-30 17:21:43 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 942464 examples: 0.068 | 0.121\n",
      "2023-12-30 17:21:43 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 944320 examples: 0.069 | 0.115\n",
      "2023-12-30 17:21:43 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 946176 examples: 0.077 | 0.125\n",
      "2023-12-30 17:21:44 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 948032 examples: 0.086 | 0.109\n",
      "2023-12-30 17:21:44 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 949888 examples: 0.095 | 0.120\n",
      "2023-12-30 17:21:44 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 951744 examples: 0.101 | 0.123\n",
      "2023-12-30 17:21:45 - INFO     | Early stopping: no decrease (0.118 vs 0.122); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:47<00:29,  9.84s/it]2023-12-30 17:21:45 - INFO     | Epoch: 17 | Learning Rate: 0.003\n",
      "2023-12-30 17:21:45 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 952064 examples: 0.037 | 0.123\n",
      "2023-12-30 17:21:45 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 953920 examples: 0.074 | 0.120\n",
      "2023-12-30 17:21:45 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 955776 examples: 0.088 | 0.121\n",
      "2023-12-30 17:21:46 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 957632 examples: 0.098 | 0.130\n",
      "2023-12-30 17:21:46 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 959488 examples: 0.073 | 0.121\n",
      "2023-12-30 17:21:46 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 961344 examples: 0.070 | 0.138\n",
      "2023-12-30 17:21:47 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 963200 examples: 0.075 | 0.127\n",
      "2023-12-30 17:21:47 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 965056 examples: 0.067 | 0.121\n",
      "2023-12-30 17:21:47 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 966912 examples: 0.087 | 0.138\n",
      "2023-12-30 17:21:47 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 968768 examples: 0.096 | 0.127\n",
      "2023-12-30 17:21:48 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 970624 examples: 0.079 | 0.129\n",
      "2023-12-30 17:21:48 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 972480 examples: 0.075 | 0.144\n",
      "2023-12-30 17:21:48 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 974336 examples: 0.101 | 0.128\n",
      "2023-12-30 17:21:49 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 976192 examples: 0.096 | 0.119\n",
      "2023-12-30 17:21:49 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 978048 examples: 0.091 | 0.115\n",
      "2023-12-30 17:21:49 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 979904 examples: 0.068 | 0.121\n",
      "2023-12-30 17:21:50 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 981760 examples: 0.068 | 0.130\n",
      "2023-12-30 17:21:50 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 983616 examples: 0.103 | 0.117\n",
      "2023-12-30 17:21:50 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 985472 examples: 0.070 | 0.118\n",
      "2023-12-30 17:21:51 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 987328 examples: 0.069 | 0.139\n",
      "2023-12-30 17:21:51 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 989184 examples: 0.079 | 0.140\n",
      "2023-12-30 17:21:51 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 991040 examples: 0.086 | 0.144\n",
      "2023-12-30 17:21:51 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 992896 examples: 0.096 | 0.123\n",
      "2023-12-30 17:21:52 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 994752 examples: 0.078 | 0.119\n",
      "2023-12-30 17:21:52 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 996608 examples: 0.090 | 0.114\n",
      "2023-12-30 17:21:52 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 998464 examples: 0.070 | 0.124\n",
      "2023-12-30 17:21:53 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1000320 examples: 0.080 | 0.125\n",
      "2023-12-30 17:21:53 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1002176 examples: 0.084 | 0.116\n",
      "2023-12-30 17:21:53 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1004032 examples: 0.097 | 0.113\n",
      "2023-12-30 17:21:53 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1005888 examples: 0.088 | 0.107\n",
      "2023-12-30 17:21:54 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1007744 examples: 0.064 | 0.109\n",
      "2023-12-30 17:21:54 - INFO     | Early stopping: loss decreased (0.118 -> 0.109; -7.4%). Caching model state.\n",
      " 90%|█████████ | 18/20 [02:56<00:19,  9.71s/it]2023-12-30 17:21:54 - INFO     | Epoch: 18 | Learning Rate: 0.003\n",
      "2023-12-30 17:21:54 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1008064 examples: 0.007 | 0.109\n",
      "2023-12-30 17:21:55 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1009920 examples: 0.078 | 0.125\n",
      "2023-12-30 17:21:55 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1011776 examples: 0.073 | 0.130\n",
      "2023-12-30 17:21:55 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1013632 examples: 0.071 | 0.122\n",
      "2023-12-30 17:21:56 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1015488 examples: 0.056 | 0.131\n",
      "2023-12-30 17:21:56 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1017344 examples: 0.082 | 0.125\n",
      "2023-12-30 17:21:56 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1019200 examples: 0.073 | 0.124\n",
      "2023-12-30 17:21:57 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1021056 examples: 0.075 | 0.131\n",
      "2023-12-30 17:21:57 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1022912 examples: 0.069 | 0.128\n",
      "2023-12-30 17:21:57 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1024768 examples: 0.086 | 0.118\n",
      "2023-12-30 17:21:57 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1026624 examples: 0.081 | 0.120\n",
      "2023-12-30 17:21:58 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1028480 examples: 0.110 | 0.126\n",
      "2023-12-30 17:21:58 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1030336 examples: 0.076 | 0.131\n",
      "2023-12-30 17:21:58 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1032192 examples: 0.094 | 0.124\n",
      "2023-12-30 17:21:59 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1034048 examples: 0.082 | 0.123\n",
      "2023-12-30 17:21:59 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1035904 examples: 0.084 | 0.117\n",
      "2023-12-30 17:21:59 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1037760 examples: 0.075 | 0.130\n",
      "2023-12-30 17:22:00 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1039616 examples: 0.093 | 0.120\n",
      "2023-12-30 17:22:00 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1041472 examples: 0.063 | 0.125\n",
      "2023-12-30 17:22:00 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1043328 examples: 0.059 | 0.126\n",
      "2023-12-30 17:22:00 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1045184 examples: 0.085 | 0.124\n",
      "2023-12-30 17:22:01 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1047040 examples: 0.079 | 0.131\n",
      "2023-12-30 17:22:01 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1048896 examples: 0.092 | 0.128\n",
      "2023-12-30 17:22:01 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1050752 examples: 0.087 | 0.128\n",
      "2023-12-30 17:22:02 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1052608 examples: 0.087 | 0.129\n",
      "2023-12-30 17:22:02 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1054464 examples: 0.095 | 0.135\n",
      "2023-12-30 17:22:02 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1056320 examples: 0.081 | 0.119\n",
      "2023-12-30 17:22:03 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1058176 examples: 0.090 | 0.127\n",
      "2023-12-30 17:22:03 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1060032 examples: 0.085 | 0.124\n",
      "2023-12-30 17:22:03 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1061888 examples: 0.084 | 0.115\n",
      "2023-12-30 17:22:04 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1063744 examples: 0.085 | 0.126\n",
      "2023-12-30 17:22:04 - INFO     | Early stopping: no decrease (0.109 vs 0.135); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [03:06<00:09,  9.77s/it]2023-12-30 17:22:04 - INFO     | Epoch: 19 | Learning Rate: 0.003\n",
      "2023-12-30 17:22:04 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1064064 examples: 0.034 | 0.136\n",
      "2023-12-30 17:22:04 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1065920 examples: 0.067 | 0.120\n",
      "2023-12-30 17:22:05 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1067776 examples: 0.070 | 0.122\n",
      "2023-12-30 17:22:05 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1069632 examples: 0.067 | 0.141\n",
      "2023-12-30 17:22:06 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1071488 examples: 0.071 | 0.119\n",
      "2023-12-30 17:22:06 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1073344 examples: 0.061 | 0.115\n",
      "2023-12-30 17:22:06 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1075200 examples: 0.072 | 0.128\n",
      "2023-12-30 17:22:07 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1077056 examples: 0.081 | 0.141\n",
      "2023-12-30 17:22:07 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1078912 examples: 0.062 | 0.128\n",
      "2023-12-30 17:22:07 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1080768 examples: 0.073 | 0.139\n",
      "2023-12-30 17:22:08 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1082624 examples: 0.071 | 0.136\n",
      "2023-12-30 17:22:08 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1084480 examples: 0.065 | 0.143\n",
      "2023-12-30 17:22:08 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1086336 examples: 0.069 | 0.157\n",
      "2023-12-30 17:22:08 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1088192 examples: 0.090 | 0.124\n",
      "2023-12-30 17:22:09 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1090048 examples: 0.069 | 0.129\n",
      "2023-12-30 17:22:09 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1091904 examples: 0.090 | 0.141\n",
      "2023-12-30 17:22:09 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1093760 examples: 0.079 | 0.124\n",
      "2023-12-30 17:22:10 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1095616 examples: 0.084 | 0.123\n",
      "2023-12-30 17:22:10 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1097472 examples: 0.065 | 0.120\n",
      "2023-12-30 17:22:10 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1099328 examples: 0.070 | 0.125\n",
      "2023-12-30 17:22:10 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1101184 examples: 0.097 | 0.117\n",
      "2023-12-30 17:22:11 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1103040 examples: 0.081 | 0.121\n",
      "2023-12-30 17:22:11 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1104896 examples: 0.095 | 0.128\n",
      "2023-12-30 17:22:11 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1106752 examples: 0.103 | 0.117\n",
      "2023-12-30 17:22:12 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1108608 examples: 0.083 | 0.124\n",
      "2023-12-30 17:22:12 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1110464 examples: 0.077 | 0.127\n",
      "2023-12-30 17:22:12 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1112320 examples: 0.067 | 0.142\n",
      "2023-12-30 17:22:13 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1114176 examples: 0.083 | 0.126\n",
      "2023-12-30 17:22:13 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1116032 examples: 0.093 | 0.149\n",
      "2023-12-30 17:22:13 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1117888 examples: 0.101 | 0.130\n",
      "2023-12-30 17:22:14 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1119744 examples: 0.088 | 0.128\n",
      "2023-12-30 17:22:14 - INFO     | Early stopping: no decrease (0.109 vs 0.127); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:16<00:00,  9.83s/it]\n",
      "2023-12-30 17:22:14 - INFO     | Best validation loss: 0.109\n",
      "2023-12-30 17:22:14 - INFO     | Best early stopping index/epoch: 17\n",
      "2023-12-30 17:22:14 - INFO     | Average Loss on test set: 0.130\n",
      "2023-12-30 17:22:16 - INFO     | Weighted Precision: 0.963, Recall: 0.963, F1: 0.963\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▃▃▃▃▃▃▃▃▁▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▅▆▅▅▄▅▅▇▅▄▃▂▃▂▃▃▂▂▃▂▃▂▂▃▃▃▂▁▁▁▁▂▂▂▂▂▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▆▆▅▅▄▅▅▅▅▄▃▂▂▃▃▂▃▃▂▃▂▃▃▃▅▃▂▁▁▂▁▁▂▁▁▂▂▂</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_validation_loss</td><td>0.10894</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.0025</td></tr><tr><td>step_learning_rate</td><td>0.0025</td></tr><tr><td>step_training_loss</td><td>0.08775</td></tr><tr><td>step_validation_loss</td><td>0.12787</td></tr><tr><td>test_loss</td><td>0.12965</td></tr><tr><td>weighted_f1</td><td>0.96316</td></tr><tr><td>weighted_precision</td><td>0.9633</td></tr><tr><td>weighted_recall</td><td>0.96314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-1</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/7m4j8lgn' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/7m4j8lgn</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_171856-7m4j8lgn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d0ngkzgm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [8, 16]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_172226-d0ngkzgm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d0ngkzgm' target=\"_blank\">dashing-sweep-2</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d0ngkzgm' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d0ngkzgm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [8, 16], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:22:26 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 17:22:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 20.796 | 160.582\n",
      "2023-12-30 17:22:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 7.847 | 2.078\n",
      "2023-12-30 17:22:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 1.049 | 1.219\n",
      "2023-12-30 17:22:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 0.785 | 0.615\n",
      "2023-12-30 17:22:28 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 0.597 | 0.594\n",
      "2023-12-30 17:22:28 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 0.551 | 0.484\n",
      "2023-12-30 17:22:28 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 0.472 | 0.452\n",
      "2023-12-30 17:22:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 0.425 | 0.432\n",
      "2023-12-30 17:22:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 0.398 | 0.513\n",
      "2023-12-30 17:22:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 0.406 | 0.363\n",
      "2023-12-30 17:22:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 0.341 | 0.300\n",
      "2023-12-30 17:22:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 0.327 | 0.322\n",
      "2023-12-30 17:22:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 0.315 | 0.295\n",
      "2023-12-30 17:22:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.271 | 0.318\n",
      "2023-12-30 17:22:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.306 | 0.306\n",
      "2023-12-30 17:22:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.237 | 0.247\n",
      "2023-12-30 17:22:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.253 | 0.252\n",
      "2023-12-30 17:22:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.252 | 0.216\n",
      "2023-12-30 17:22:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.246 | 0.226\n",
      "2023-12-30 17:22:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.219 | 0.197\n",
      "2023-12-30 17:22:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.193 | 0.190\n",
      "2023-12-30 17:22:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.204 | 0.196\n",
      "2023-12-30 17:22:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.229 | 0.212\n",
      "2023-12-30 17:22:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.224 | 0.177\n",
      "2023-12-30 17:22:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.193 | 0.169\n",
      "2023-12-30 17:22:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.185 | 0.190\n",
      "2023-12-30 17:22:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.215 | 0.174\n",
      "2023-12-30 17:22:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.155 | 0.184\n",
      "2023-12-30 17:22:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.155 | 0.173\n",
      "2023-12-30 17:22:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.179 | 0.176\n",
      "2023-12-30 17:22:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.189 | 0.158\n",
      "2023-12-30 17:22:35 - INFO     | Early stopping: loss decreased (inf -> 0.154; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:51,  9.03s/it]2023-12-30 17:22:35 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 17:22:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.106 | 0.150\n",
      "2023-12-30 17:22:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.160 | 0.154\n",
      "2023-12-30 17:22:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.165 | 0.162\n",
      "2023-12-30 17:22:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.136 | 0.166\n",
      "2023-12-30 17:22:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.126 | 0.193\n",
      "2023-12-30 17:22:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.154 | 0.140\n",
      "2023-12-30 17:22:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.163 | 0.131\n",
      "2023-12-30 17:22:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.118 | 0.136\n",
      "2023-12-30 17:22:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.159 | 0.141\n",
      "2023-12-30 17:22:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.149 | 0.134\n",
      "2023-12-30 17:22:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.140 | 0.134\n",
      "2023-12-30 17:22:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.160 | 0.134\n",
      "2023-12-30 17:22:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.136 | 0.166\n",
      "2023-12-30 17:22:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.135 | 0.126\n",
      "2023-12-30 17:22:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.134 | 0.163\n",
      "2023-12-30 17:22:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.132 | 0.146\n",
      "2023-12-30 17:22:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.130 | 0.121\n",
      "2023-12-30 17:22:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.136 | 0.133\n",
      "2023-12-30 17:22:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.117 | 0.153\n",
      "2023-12-30 17:22:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.119 | 0.132\n",
      "2023-12-30 17:22:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.156 | 0.138\n",
      "2023-12-30 17:22:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.166 | 0.131\n",
      "2023-12-30 17:22:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.133 | 0.134\n",
      "2023-12-30 17:22:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.129 | 0.148\n",
      "2023-12-30 17:22:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.113 | 0.141\n",
      "2023-12-30 17:22:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.137 | 0.133\n",
      "2023-12-30 17:22:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.121 | 0.139\n",
      "2023-12-30 17:22:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.133 | 0.125\n",
      "2023-12-30 17:22:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.117 | 0.145\n",
      "2023-12-30 17:22:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.123 | 0.119\n",
      "2023-12-30 17:22:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.122 | 0.132\n",
      "2023-12-30 17:22:44 - INFO     | Early stopping: loss decreased (0.154 -> 0.113; -26.6%). Caching model state.\n",
      " 10%|█         | 2/20 [00:17<02:38,  8.82s/it]2023-12-30 17:22:44 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 17:22:44 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.181 | 0.123\n",
      "2023-12-30 17:22:44 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.105 | 0.141\n",
      "2023-12-30 17:22:45 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.126 | 0.122\n",
      "2023-12-30 17:22:45 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.109 | 0.125\n",
      "2023-12-30 17:22:45 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.120 | 0.118\n",
      "2023-12-30 17:22:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.100 | 0.115\n",
      "2023-12-30 17:22:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.098 | 0.129\n",
      "2023-12-30 17:22:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.126 | 0.119\n",
      "2023-12-30 17:22:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.094 | 0.108\n",
      "2023-12-30 17:22:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.129 | 0.109\n",
      "2023-12-30 17:22:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.135 | 0.112\n",
      "2023-12-30 17:22:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.088 | 0.111\n",
      "2023-12-30 17:22:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.106 | 0.111\n",
      "2023-12-30 17:22:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.085 | 0.108\n",
      "2023-12-30 17:22:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.114 | 0.121\n",
      "2023-12-30 17:22:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.091 | 0.115\n",
      "2023-12-30 17:22:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.086 | 0.122\n",
      "2023-12-30 17:22:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.080 | 0.105\n",
      "2023-12-30 17:22:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.091 | 0.124\n",
      "2023-12-30 17:22:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.135 | 0.115\n",
      "2023-12-30 17:22:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.122 | 0.104\n",
      "2023-12-30 17:22:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.116 | 0.110\n",
      "2023-12-30 17:22:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.080 | 0.112\n",
      "2023-12-30 17:22:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.123 | 0.112\n",
      "2023-12-30 17:22:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.095 | 0.105\n",
      "2023-12-30 17:22:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.111 | 0.104\n",
      "2023-12-30 17:22:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.120 | 0.108\n",
      "2023-12-30 17:22:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.093 | 0.105\n",
      "2023-12-30 17:22:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.097 | 0.104\n",
      "2023-12-30 17:22:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.083 | 0.111\n",
      "2023-12-30 17:22:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.101 | 0.120\n",
      "2023-12-30 17:22:52 - INFO     | Early stopping: no decrease (0.113 vs 0.115); counter: 1 out of 3\n",
      " 15%|█▌        | 3/20 [00:26<02:27,  8.69s/it]2023-12-30 17:22:52 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 17:22:53 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.218 | 0.138\n",
      "2023-12-30 17:22:53 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.086 | 0.109\n",
      "2023-12-30 17:22:53 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.087 | 0.112\n",
      "2023-12-30 17:22:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.094 | 0.120\n",
      "2023-12-30 17:22:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.078 | 0.107\n",
      "2023-12-30 17:22:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.114 | 0.116\n",
      "2023-12-30 17:22:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.094 | 0.117\n",
      "2023-12-30 17:22:55 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.087 | 0.107\n",
      "2023-12-30 17:22:55 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.097 | 0.123\n",
      "2023-12-30 17:22:55 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.093 | 0.101\n",
      "2023-12-30 17:22:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.103 | 0.096\n",
      "2023-12-30 17:22:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.084 | 0.095\n",
      "2023-12-30 17:22:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.070 | 0.100\n",
      "2023-12-30 17:22:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.079 | 0.134\n",
      "2023-12-30 17:22:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.107 | 0.098\n",
      "2023-12-30 17:22:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.105 | 0.106\n",
      "2023-12-30 17:22:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.091 | 0.109\n",
      "2023-12-30 17:22:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.085 | 0.101\n",
      "2023-12-30 17:22:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.063 | 0.124\n",
      "2023-12-30 17:22:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.085 | 0.122\n",
      "2023-12-30 17:22:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.080 | 0.109\n",
      "2023-12-30 17:22:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.094 | 0.092\n",
      "2023-12-30 17:22:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.107 | 0.099\n",
      "2023-12-30 17:22:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.091 | 0.100\n",
      "2023-12-30 17:22:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.109 | 0.095\n",
      "2023-12-30 17:23:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.087 | 0.100\n",
      "2023-12-30 17:23:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.064 | 0.091\n",
      "2023-12-30 17:23:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.076 | 0.086\n",
      "2023-12-30 17:23:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.094 | 0.103\n",
      "2023-12-30 17:23:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.106 | 0.097\n",
      "2023-12-30 17:23:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.111 | 0.094\n",
      "2023-12-30 17:23:01 - INFO     | Early stopping: loss decreased (0.113 -> 0.104; -8.2%). Caching model state.\n",
      " 20%|██        | 4/20 [00:34<02:19,  8.71s/it]2023-12-30 17:23:01 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:01 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.098 | 0.115\n",
      "2023-12-30 17:23:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.072 | 0.089\n",
      "2023-12-30 17:23:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.059 | 0.100\n",
      "2023-12-30 17:23:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.089 | 0.114\n",
      "2023-12-30 17:23:03 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.083 | 0.096\n",
      "2023-12-30 17:23:03 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.062 | 0.142\n",
      "2023-12-30 17:23:03 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.090 | 0.091\n",
      "2023-12-30 17:23:03 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.091 | 0.099\n",
      "2023-12-30 17:23:04 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.068 | 0.092\n",
      "2023-12-30 17:23:04 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.069 | 0.098\n",
      "2023-12-30 17:23:04 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.084 | 0.095\n",
      "2023-12-30 17:23:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.071 | 0.121\n",
      "2023-12-30 17:23:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.088 | 0.104\n",
      "2023-12-30 17:23:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.075 | 0.088\n",
      "2023-12-30 17:23:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.067 | 0.081\n",
      "2023-12-30 17:23:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.075 | 0.105\n",
      "2023-12-30 17:23:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.096 | 0.101\n",
      "2023-12-30 17:23:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.078 | 0.088\n",
      "2023-12-30 17:23:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.059 | 0.095\n",
      "2023-12-30 17:23:07 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.093 | 0.101\n",
      "2023-12-30 17:23:07 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.085 | 0.099\n",
      "2023-12-30 17:23:07 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.082 | 0.098\n",
      "2023-12-30 17:23:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.064 | 0.080\n",
      "2023-12-30 17:23:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.078 | 0.089\n",
      "2023-12-30 17:23:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.078 | 0.091\n",
      "2023-12-30 17:23:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.056 | 0.184\n",
      "2023-12-30 17:23:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.071 | 0.103\n",
      "2023-12-30 17:23:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.099 | 0.098\n",
      "2023-12-30 17:23:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.105 | 0.132\n",
      "2023-12-30 17:23:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.104 | 0.089\n",
      "2023-12-30 17:23:10 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.085 | 0.109\n",
      "2023-12-30 17:23:10 - INFO     | Early stopping: loss decreased (0.104 -> 0.099; -5.0%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:43<02:11,  8.74s/it]2023-12-30 17:23:10 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:10 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.054 | 0.092\n",
      "2023-12-30 17:23:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.065 | 0.091\n",
      "2023-12-30 17:23:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.085 | 0.091\n",
      "2023-12-30 17:23:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.078 | 0.095\n",
      "2023-12-30 17:23:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.062 | 0.092\n",
      "2023-12-30 17:23:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.059 | 0.094\n",
      "2023-12-30 17:23:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.066 | 0.089\n",
      "2023-12-30 17:23:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.059 | 0.090\n",
      "2023-12-30 17:23:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.058 | 0.089\n",
      "2023-12-30 17:23:13 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.074 | 0.113\n",
      "2023-12-30 17:23:13 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.092 | 0.094\n",
      "2023-12-30 17:23:13 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.069 | 0.088\n",
      "2023-12-30 17:23:14 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.072 | 0.094\n",
      "2023-12-30 17:23:14 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.070 | 0.092\n",
      "2023-12-30 17:23:14 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.083 | 0.095\n",
      "2023-12-30 17:23:14 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.098 | 0.098\n",
      "2023-12-30 17:23:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.078 | 0.081\n",
      "2023-12-30 17:23:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.081 | 0.096\n",
      "2023-12-30 17:23:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.064 | 0.086\n",
      "2023-12-30 17:23:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.075 | 0.081\n",
      "2023-12-30 17:23:16 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.076 | 0.082\n",
      "2023-12-30 17:23:16 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.083 | 0.091\n",
      "2023-12-30 17:23:16 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.072 | 0.084\n",
      "2023-12-30 17:23:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.059 | 0.077\n",
      "2023-12-30 17:23:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.090 | 0.091\n",
      "2023-12-30 17:23:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.075 | 0.087\n",
      "2023-12-30 17:23:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.060 | 0.085\n",
      "2023-12-30 17:23:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.067 | 0.088\n",
      "2023-12-30 17:23:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.086 | 0.086\n",
      "2023-12-30 17:23:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.093 | 0.108\n",
      "2023-12-30 17:23:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.065 | 0.101\n",
      "2023-12-30 17:23:19 - INFO     | Early stopping: loss decreased (0.099 -> 0.087; -11.7%). Caching model state.\n",
      " 30%|███       | 6/20 [00:52<02:02,  8.73s/it]2023-12-30 17:23:19 - INFO     | Epoch: 6 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:19 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 336064 examples: 0.167 | 0.090\n",
      "2023-12-30 17:23:19 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 337920 examples: 0.063 | 0.080\n",
      "2023-12-30 17:23:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 339776 examples: 0.057 | 0.085\n",
      "2023-12-30 17:23:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 341632 examples: 0.047 | 0.087\n",
      "2023-12-30 17:23:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 343488 examples: 0.069 | 0.089\n",
      "2023-12-30 17:23:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 345344 examples: 0.073 | 0.090\n",
      "2023-12-30 17:23:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 347200 examples: 0.048 | 0.088\n",
      "2023-12-30 17:23:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 349056 examples: 0.064 | 0.097\n",
      "2023-12-30 17:23:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 350912 examples: 0.085 | 0.110\n",
      "2023-12-30 17:23:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 352768 examples: 0.078 | 0.089\n",
      "2023-12-30 17:23:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 354624 examples: 0.066 | 0.079\n",
      "2023-12-30 17:23:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 356480 examples: 0.064 | 0.081\n",
      "2023-12-30 17:23:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 358336 examples: 0.081 | 0.088\n",
      "2023-12-30 17:23:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 360192 examples: 0.061 | 0.133\n",
      "2023-12-30 17:23:23 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 362048 examples: 0.084 | 0.092\n",
      "2023-12-30 17:23:23 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 363904 examples: 0.060 | 0.087\n",
      "2023-12-30 17:23:23 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 365760 examples: 0.063 | 0.109\n",
      "2023-12-30 17:23:23 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 367616 examples: 0.069 | 0.090\n",
      "2023-12-30 17:23:24 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 369472 examples: 0.064 | 0.090\n",
      "2023-12-30 17:23:24 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 371328 examples: 0.071 | 0.088\n",
      "2023-12-30 17:23:24 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 373184 examples: 0.071 | 0.091\n",
      "2023-12-30 17:23:25 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 375040 examples: 0.080 | 0.075\n",
      "2023-12-30 17:23:25 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 376896 examples: 0.077 | 0.079\n",
      "2023-12-30 17:23:25 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 378752 examples: 0.058 | 0.095\n",
      "2023-12-30 17:23:25 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 380608 examples: 0.075 | 0.078\n",
      "2023-12-30 17:23:26 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 382464 examples: 0.059 | 0.080\n",
      "2023-12-30 17:23:26 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 384320 examples: 0.056 | 0.084\n",
      "2023-12-30 17:23:26 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 386176 examples: 0.061 | 0.116\n",
      "2023-12-30 17:23:26 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 388032 examples: 0.067 | 0.089\n",
      "2023-12-30 17:23:27 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 389888 examples: 0.061 | 0.083\n",
      "2023-12-30 17:23:27 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 391744 examples: 0.067 | 0.100\n",
      "2023-12-30 17:23:27 - INFO     | Early stopping: no decrease (0.087 vs 0.087); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:00<01:52,  8.65s/it]2023-12-30 17:23:27 - INFO     | Epoch: 7 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:27 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 392064 examples: 0.106 | 0.092\n",
      "2023-12-30 17:23:28 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 393920 examples: 0.046 | 0.097\n",
      "2023-12-30 17:23:28 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 395776 examples: 0.047 | 0.089\n",
      "2023-12-30 17:23:28 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 397632 examples: 0.081 | 0.093\n",
      "2023-12-30 17:23:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 399488 examples: 0.073 | 0.081\n",
      "2023-12-30 17:23:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 401344 examples: 0.074 | 0.086\n",
      "2023-12-30 17:23:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 403200 examples: 0.058 | 0.077\n",
      "2023-12-30 17:23:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 405056 examples: 0.049 | 0.090\n",
      "2023-12-30 17:23:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 406912 examples: 0.064 | 0.082\n",
      "2023-12-30 17:23:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 408768 examples: 0.061 | 0.088\n",
      "2023-12-30 17:23:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 410624 examples: 0.070 | 0.078\n",
      "2023-12-30 17:23:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 412480 examples: 0.060 | 0.088\n",
      "2023-12-30 17:23:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 414336 examples: 0.073 | 0.089\n",
      "2023-12-30 17:23:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 416192 examples: 0.076 | 0.090\n",
      "2023-12-30 17:23:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 418048 examples: 0.066 | 0.103\n",
      "2023-12-30 17:23:32 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 419904 examples: 0.082 | 0.091\n",
      "2023-12-30 17:23:32 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 421760 examples: 0.052 | 0.102\n",
      "2023-12-30 17:23:32 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 423616 examples: 0.062 | 0.083\n",
      "2023-12-30 17:23:33 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 425472 examples: 0.054 | 0.083\n",
      "2023-12-30 17:23:33 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 427328 examples: 0.074 | 0.088\n",
      "2023-12-30 17:23:33 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 429184 examples: 0.061 | 0.085\n",
      "2023-12-30 17:23:33 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 431040 examples: 0.068 | 0.093\n",
      "2023-12-30 17:23:34 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 432896 examples: 0.065 | 0.085\n",
      "2023-12-30 17:23:34 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 434752 examples: 0.073 | 0.090\n",
      "2023-12-30 17:23:34 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 436608 examples: 0.061 | 0.103\n",
      "2023-12-30 17:23:34 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 438464 examples: 0.053 | 0.091\n",
      "2023-12-30 17:23:35 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 440320 examples: 0.084 | 0.098\n",
      "2023-12-30 17:23:35 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 442176 examples: 0.081 | 0.080\n",
      "2023-12-30 17:23:35 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 444032 examples: 0.058 | 0.083\n",
      "2023-12-30 17:23:35 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 445888 examples: 0.072 | 0.082\n",
      "2023-12-30 17:23:36 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 447744 examples: 0.058 | 0.081\n",
      "2023-12-30 17:23:36 - INFO     | Early stopping: loss decreased (0.087 -> 0.079; -9.7%). Caching model state.\n",
      " 40%|████      | 8/20 [01:09<01:44,  8.68s/it]2023-12-30 17:23:36 - INFO     | Epoch: 8 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:36 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 448064 examples: 0.013 | 0.079\n",
      "2023-12-30 17:23:36 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 449920 examples: 0.057 | 0.077\n",
      "2023-12-30 17:23:37 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 451776 examples: 0.054 | 0.084\n",
      "2023-12-30 17:23:37 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 453632 examples: 0.035 | 0.080\n",
      "2023-12-30 17:23:37 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 455488 examples: 0.079 | 0.077\n",
      "2023-12-30 17:23:38 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 457344 examples: 0.045 | 0.081\n",
      "2023-12-30 17:23:38 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 459200 examples: 0.069 | 0.090\n",
      "2023-12-30 17:23:38 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 461056 examples: 0.070 | 0.089\n",
      "2023-12-30 17:23:38 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 462912 examples: 0.075 | 0.090\n",
      "2023-12-30 17:23:39 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 464768 examples: 0.055 | 0.088\n",
      "2023-12-30 17:23:39 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 466624 examples: 0.061 | 0.093\n",
      "2023-12-30 17:23:39 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 468480 examples: 0.054 | 0.089\n",
      "2023-12-30 17:23:39 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 470336 examples: 0.051 | 0.080\n",
      "2023-12-30 17:23:40 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 472192 examples: 0.072 | 0.083\n",
      "2023-12-30 17:23:40 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 474048 examples: 0.056 | 0.080\n",
      "2023-12-30 17:23:40 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 475904 examples: 0.062 | 0.084\n",
      "2023-12-30 17:23:41 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 477760 examples: 0.071 | 0.082\n",
      "2023-12-30 17:23:41 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 479616 examples: 0.062 | 0.084\n",
      "2023-12-30 17:23:41 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 481472 examples: 0.071 | 0.093\n",
      "2023-12-30 17:23:41 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 483328 examples: 0.074 | 0.083\n",
      "2023-12-30 17:23:42 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 485184 examples: 0.058 | 0.090\n",
      "2023-12-30 17:23:42 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 487040 examples: 0.061 | 0.081\n",
      "2023-12-30 17:23:42 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 488896 examples: 0.037 | 0.084\n",
      "2023-12-30 17:23:42 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 490752 examples: 0.082 | 0.081\n",
      "2023-12-30 17:23:43 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 492608 examples: 0.058 | 0.080\n",
      "2023-12-30 17:23:43 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 494464 examples: 0.047 | 0.106\n",
      "2023-12-30 17:23:43 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 496320 examples: 0.071 | 0.089\n",
      "2023-12-30 17:23:43 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 498176 examples: 0.087 | 0.080\n",
      "2023-12-30 17:23:44 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 500032 examples: 0.064 | 0.077\n",
      "2023-12-30 17:23:44 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 501888 examples: 0.072 | 0.108\n",
      "2023-12-30 17:23:44 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 503744 examples: 0.055 | 0.082\n",
      "2023-12-30 17:23:45 - INFO     | Early stopping: no decrease (0.079 vs 0.085); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:18<01:35,  8.67s/it]2023-12-30 17:23:45 - INFO     | Epoch: 9 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:45 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 504064 examples: 0.035 | 0.088\n",
      "2023-12-30 17:23:45 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 505920 examples: 0.037 | 0.093\n",
      "2023-12-30 17:23:46 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 507776 examples: 0.064 | 0.072\n",
      "2023-12-30 17:23:46 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 509632 examples: 0.032 | 0.079\n",
      "2023-12-30 17:23:46 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 511488 examples: 0.070 | 0.084\n",
      "2023-12-30 17:23:46 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 513344 examples: 0.070 | 0.079\n",
      "2023-12-30 17:23:47 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 515200 examples: 0.053 | 0.077\n",
      "2023-12-30 17:23:47 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 517056 examples: 0.058 | 0.085\n",
      "2023-12-30 17:23:47 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 518912 examples: 0.065 | 0.097\n",
      "2023-12-30 17:23:48 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 520768 examples: 0.069 | 0.109\n",
      "2023-12-30 17:23:48 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 522624 examples: 0.065 | 0.076\n",
      "2023-12-30 17:23:48 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 524480 examples: 0.049 | 0.082\n",
      "2023-12-30 17:23:48 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 526336 examples: 0.064 | 0.076\n",
      "2023-12-30 17:23:49 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 528192 examples: 0.062 | 0.095\n",
      "2023-12-30 17:23:49 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 530048 examples: 0.058 | 0.092\n",
      "2023-12-30 17:23:49 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 531904 examples: 0.057 | 0.081\n",
      "2023-12-30 17:23:49 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 533760 examples: 0.073 | 0.087\n",
      "2023-12-30 17:23:50 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 535616 examples: 0.071 | 0.079\n",
      "2023-12-30 17:23:50 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 537472 examples: 0.046 | 0.081\n",
      "2023-12-30 17:23:50 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 539328 examples: 0.067 | 0.092\n",
      "2023-12-30 17:23:51 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 541184 examples: 0.061 | 0.088\n",
      "2023-12-30 17:23:51 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 543040 examples: 0.033 | 0.089\n",
      "2023-12-30 17:23:51 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 544896 examples: 0.062 | 0.087\n",
      "2023-12-30 17:23:51 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 546752 examples: 0.058 | 0.096\n",
      "2023-12-30 17:23:52 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 548608 examples: 0.080 | 0.086\n",
      "2023-12-30 17:23:52 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 550464 examples: 0.054 | 0.093\n",
      "2023-12-30 17:23:52 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 552320 examples: 0.068 | 0.088\n",
      "2023-12-30 17:23:52 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 554176 examples: 0.062 | 0.077\n",
      "2023-12-30 17:23:53 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 556032 examples: 0.067 | 0.092\n",
      "2023-12-30 17:23:53 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 557888 examples: 0.070 | 0.073\n",
      "2023-12-30 17:23:53 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 559744 examples: 0.032 | 0.082\n",
      "2023-12-30 17:23:54 - INFO     | Early stopping: no decrease (0.079 vs 0.082); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:27<01:27,  8.74s/it]2023-12-30 17:23:54 - INFO     | Epoch: 10 | Learning Rate: 0.010\n",
      "2023-12-30 17:23:54 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 560064 examples: 0.045 | 0.080\n",
      "2023-12-30 17:23:54 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 561920 examples: 0.040 | 0.092\n",
      "2023-12-30 17:23:54 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 563776 examples: 0.073 | 0.099\n",
      "2023-12-30 17:23:55 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 565632 examples: 0.041 | 0.076\n",
      "2023-12-30 17:23:55 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 567488 examples: 0.061 | 0.093\n",
      "2023-12-30 17:23:55 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 569344 examples: 0.067 | 0.077\n",
      "2023-12-30 17:23:55 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 571200 examples: 0.047 | 0.082\n",
      "2023-12-30 17:23:56 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 573056 examples: 0.068 | 0.078\n",
      "2023-12-30 17:23:56 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 574912 examples: 0.062 | 0.081\n",
      "2023-12-30 17:23:56 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 576768 examples: 0.063 | 0.081\n",
      "2023-12-30 17:23:56 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 578624 examples: 0.050 | 0.106\n",
      "2023-12-30 17:23:57 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 580480 examples: 0.054 | 0.109\n",
      "2023-12-30 17:23:57 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 582336 examples: 0.076 | 0.080\n",
      "2023-12-30 17:23:57 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 584192 examples: 0.060 | 0.091\n",
      "2023-12-30 17:23:58 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 586048 examples: 0.061 | 0.091\n",
      "2023-12-30 17:23:58 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 587904 examples: 0.057 | 0.074\n",
      "2023-12-30 17:23:58 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 589760 examples: 0.055 | 0.080\n",
      "2023-12-30 17:23:58 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 591616 examples: 0.074 | 0.080\n",
      "2023-12-30 17:23:59 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 593472 examples: 0.060 | 0.095\n",
      "2023-12-30 17:23:59 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 595328 examples: 0.060 | 0.078\n",
      "2023-12-30 17:23:59 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 597184 examples: 0.043 | 0.092\n",
      "2023-12-30 17:24:00 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 599040 examples: 0.035 | 0.087\n",
      "2023-12-30 17:24:00 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 600896 examples: 0.042 | 0.085\n",
      "2023-12-30 17:24:00 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 602752 examples: 0.074 | 0.080\n",
      "2023-12-30 17:24:00 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 604608 examples: 0.053 | 0.089\n",
      "2023-12-30 17:24:01 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 606464 examples: 0.061 | 0.082\n",
      "2023-12-30 17:24:01 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 608320 examples: 0.057 | 0.086\n",
      "2023-12-30 17:24:01 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 610176 examples: 0.070 | 0.101\n",
      "2023-12-30 17:24:01 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 612032 examples: 0.082 | 0.081\n",
      "2023-12-30 17:24:02 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 613888 examples: 0.065 | 0.081\n",
      "2023-12-30 17:24:02 - INFO     | Epoch: 10 | Learning Rate: 0.010: Training/Validation Loss after 615744 examples: 0.032 | 0.083\n",
      "2023-12-30 17:24:02 - INFO     | Early stopping: no decrease (0.079 vs 0.076); counter: 3 out of 3\n",
      "2023-12-30 17:24:02 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:24:02 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 55%|█████▌    | 11/20 [01:36<01:18,  8.78s/it]2023-12-30 17:24:02 - INFO     | Epoch: 11 | Learning Rate: 0.005\n",
      "2023-12-30 17:24:03 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 616064 examples: 0.075 | 0.075\n",
      "2023-12-30 17:24:03 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 617920 examples: 0.042 | 0.067\n",
      "2023-12-30 17:24:03 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 619776 examples: 0.047 | 0.066\n",
      "2023-12-30 17:24:03 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 621632 examples: 0.053 | 0.067\n",
      "2023-12-30 17:24:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 623488 examples: 0.031 | 0.068\n",
      "2023-12-30 17:24:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 625344 examples: 0.036 | 0.069\n",
      "2023-12-30 17:24:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 627200 examples: 0.038 | 0.074\n",
      "2023-12-30 17:24:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 629056 examples: 0.037 | 0.070\n",
      "2023-12-30 17:24:05 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 630912 examples: 0.033 | 0.070\n",
      "2023-12-30 17:24:05 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 632768 examples: 0.032 | 0.069\n",
      "2023-12-30 17:24:05 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 634624 examples: 0.036 | 0.075\n",
      "2023-12-30 17:24:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 636480 examples: 0.048 | 0.068\n",
      "2023-12-30 17:24:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 638336 examples: 0.041 | 0.063\n",
      "2023-12-30 17:24:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 640192 examples: 0.050 | 0.069\n",
      "2023-12-30 17:24:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 642048 examples: 0.057 | 0.066\n",
      "2023-12-30 17:24:07 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 643904 examples: 0.037 | 0.071\n",
      "2023-12-30 17:24:07 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 645760 examples: 0.049 | 0.072\n",
      "2023-12-30 17:24:07 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 647616 examples: 0.039 | 0.067\n",
      "2023-12-30 17:24:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 649472 examples: 0.037 | 0.073\n",
      "2023-12-30 17:24:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 651328 examples: 0.027 | 0.073\n",
      "2023-12-30 17:24:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 653184 examples: 0.038 | 0.074\n",
      "2023-12-30 17:24:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 655040 examples: 0.044 | 0.076\n",
      "2023-12-30 17:24:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 656896 examples: 0.031 | 0.079\n",
      "2023-12-30 17:24:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 658752 examples: 0.066 | 0.078\n",
      "2023-12-30 17:24:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 660608 examples: 0.037 | 0.069\n",
      "2023-12-30 17:24:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 662464 examples: 0.039 | 0.068\n",
      "2023-12-30 17:24:10 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 664320 examples: 0.033 | 0.069\n",
      "2023-12-30 17:24:10 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 666176 examples: 0.032 | 0.066\n",
      "2023-12-30 17:24:10 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 668032 examples: 0.036 | 0.069\n",
      "2023-12-30 17:24:11 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 669888 examples: 0.045 | 0.070\n",
      "2023-12-30 17:24:11 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 671744 examples: 0.043 | 0.067\n",
      "2023-12-30 17:24:11 - INFO     | Early stopping: loss decreased (0.079 -> 0.068; -13.2%). Caching model state.\n",
      " 60%|██████    | 12/20 [01:44<01:10,  8.75s/it]2023-12-30 17:24:11 - INFO     | Epoch: 12 | Learning Rate: 0.005\n",
      "2023-12-30 17:24:11 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 672064 examples: 0.053 | 0.071\n",
      "2023-12-30 17:24:12 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 673920 examples: 0.030 | 0.071\n",
      "2023-12-30 17:24:12 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 675776 examples: 0.048 | 0.071\n",
      "2023-12-30 17:24:12 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 677632 examples: 0.034 | 0.072\n",
      "2023-12-30 17:24:12 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 679488 examples: 0.026 | 0.067\n",
      "2023-12-30 17:24:13 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 681344 examples: 0.043 | 0.073\n",
      "2023-12-30 17:24:13 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 683200 examples: 0.033 | 0.068\n",
      "2023-12-30 17:24:13 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 685056 examples: 0.054 | 0.072\n",
      "2023-12-30 17:24:14 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 686912 examples: 0.039 | 0.069\n",
      "2023-12-30 17:24:14 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 688768 examples: 0.036 | 0.074\n",
      "2023-12-30 17:24:14 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 690624 examples: 0.047 | 0.070\n",
      "2023-12-30 17:24:14 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 692480 examples: 0.026 | 0.068\n",
      "2023-12-30 17:24:15 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 694336 examples: 0.026 | 0.074\n",
      "2023-12-30 17:24:15 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 696192 examples: 0.036 | 0.072\n",
      "2023-12-30 17:24:15 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 698048 examples: 0.055 | 0.069\n",
      "2023-12-30 17:24:15 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 699904 examples: 0.027 | 0.066\n",
      "2023-12-30 17:24:16 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 701760 examples: 0.029 | 0.073\n",
      "2023-12-30 17:24:16 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 703616 examples: 0.041 | 0.075\n",
      "2023-12-30 17:24:16 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 705472 examples: 0.037 | 0.078\n",
      "2023-12-30 17:24:16 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 707328 examples: 0.039 | 0.075\n",
      "2023-12-30 17:24:17 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 709184 examples: 0.038 | 0.076\n",
      "2023-12-30 17:24:17 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 711040 examples: 0.033 | 0.073\n",
      "2023-12-30 17:24:17 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 712896 examples: 0.032 | 0.076\n",
      "2023-12-30 17:24:18 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 714752 examples: 0.026 | 0.078\n",
      "2023-12-30 17:24:18 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 716608 examples: 0.042 | 0.071\n",
      "2023-12-30 17:24:18 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 718464 examples: 0.036 | 0.069\n",
      "2023-12-30 17:24:18 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 720320 examples: 0.038 | 0.065\n",
      "2023-12-30 17:24:19 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 722176 examples: 0.065 | 0.067\n",
      "2023-12-30 17:24:19 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 724032 examples: 0.032 | 0.068\n",
      "2023-12-30 17:24:19 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 725888 examples: 0.029 | 0.070\n",
      "2023-12-30 17:24:19 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 727744 examples: 0.031 | 0.071\n",
      "2023-12-30 17:24:20 - INFO     | Early stopping: no decrease (0.068 vs 0.066); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [01:53<01:00,  8.70s/it]2023-12-30 17:24:20 - INFO     | Epoch: 13 | Learning Rate: 0.005\n",
      "2023-12-30 17:24:20 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 728064 examples: 0.007 | 0.066\n",
      "2023-12-30 17:24:20 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 729920 examples: 0.034 | 0.075\n",
      "2023-12-30 17:24:20 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 731776 examples: 0.027 | 0.067\n",
      "2023-12-30 17:24:21 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 733632 examples: 0.035 | 0.071\n",
      "2023-12-30 17:24:21 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 735488 examples: 0.025 | 0.075\n",
      "2023-12-30 17:24:21 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 737344 examples: 0.044 | 0.068\n",
      "2023-12-30 17:24:22 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 739200 examples: 0.022 | 0.071\n",
      "2023-12-30 17:24:22 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 741056 examples: 0.041 | 0.073\n",
      "2023-12-30 17:24:22 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 742912 examples: 0.041 | 0.069\n",
      "2023-12-30 17:24:22 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 744768 examples: 0.038 | 0.070\n",
      "2023-12-30 17:24:23 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 746624 examples: 0.045 | 0.065\n",
      "2023-12-30 17:24:23 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 748480 examples: 0.031 | 0.065\n",
      "2023-12-30 17:24:23 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 750336 examples: 0.042 | 0.068\n",
      "2023-12-30 17:24:23 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 752192 examples: 0.048 | 0.078\n",
      "2023-12-30 17:24:24 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 754048 examples: 0.029 | 0.068\n",
      "2023-12-30 17:24:24 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 755904 examples: 0.036 | 0.071\n",
      "2023-12-30 17:24:24 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 757760 examples: 0.036 | 0.072\n",
      "2023-12-30 17:24:24 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 759616 examples: 0.038 | 0.068\n",
      "2023-12-30 17:24:25 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 761472 examples: 0.034 | 0.071\n",
      "2023-12-30 17:24:25 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 763328 examples: 0.054 | 0.070\n",
      "2023-12-30 17:24:25 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 765184 examples: 0.038 | 0.072\n",
      "2023-12-30 17:24:26 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 767040 examples: 0.032 | 0.072\n",
      "2023-12-30 17:24:26 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 768896 examples: 0.038 | 0.067\n",
      "2023-12-30 17:24:26 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 770752 examples: 0.033 | 0.063\n",
      "2023-12-30 17:24:26 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 772608 examples: 0.021 | 0.066\n",
      "2023-12-30 17:24:27 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 774464 examples: 0.033 | 0.076\n",
      "2023-12-30 17:24:27 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 776320 examples: 0.047 | 0.066\n",
      "2023-12-30 17:24:27 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 778176 examples: 0.037 | 0.069\n",
      "2023-12-30 17:24:27 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 780032 examples: 0.043 | 0.063\n",
      "2023-12-30 17:24:28 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 781888 examples: 0.035 | 0.067\n",
      "2023-12-30 17:24:28 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 783744 examples: 0.025 | 0.065\n",
      "2023-12-30 17:24:28 - INFO     | Early stopping: loss decreased (0.068 -> 0.063; -7.1%). Caching model state.\n",
      " 70%|███████   | 14/20 [02:02<00:52,  8.68s/it]2023-12-30 17:24:28 - INFO     | Epoch: 14 | Learning Rate: 0.005\n",
      "2023-12-30 17:24:29 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 784064 examples: 0.019 | 0.063\n",
      "2023-12-30 17:24:29 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 785920 examples: 0.028 | 0.068\n",
      "2023-12-30 17:24:29 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 787776 examples: 0.035 | 0.070\n",
      "2023-12-30 17:24:29 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 789632 examples: 0.027 | 0.067\n",
      "2023-12-30 17:24:30 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 791488 examples: 0.026 | 0.070\n",
      "2023-12-30 17:24:30 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 793344 examples: 0.028 | 0.074\n",
      "2023-12-30 17:24:30 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 795200 examples: 0.034 | 0.069\n",
      "2023-12-30 17:24:30 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 797056 examples: 0.037 | 0.063\n",
      "2023-12-30 17:24:31 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 798912 examples: 0.031 | 0.064\n",
      "2023-12-30 17:24:31 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 800768 examples: 0.021 | 0.069\n",
      "2023-12-30 17:24:31 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 802624 examples: 0.033 | 0.068\n",
      "2023-12-30 17:24:31 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 804480 examples: 0.054 | 0.069\n",
      "2023-12-30 17:24:32 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 806336 examples: 0.022 | 0.067\n",
      "2023-12-30 17:24:32 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 808192 examples: 0.041 | 0.068\n",
      "2023-12-30 17:24:32 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 810048 examples: 0.030 | 0.069\n",
      "2023-12-30 17:24:33 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 811904 examples: 0.034 | 0.067\n",
      "2023-12-30 17:24:33 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 813760 examples: 0.031 | 0.076\n",
      "2023-12-30 17:24:33 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 815616 examples: 0.029 | 0.073\n",
      "2023-12-30 17:24:34 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 817472 examples: 0.024 | 0.078\n",
      "2023-12-30 17:24:34 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 819328 examples: 0.037 | 0.073\n",
      "2023-12-30 17:24:34 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 821184 examples: 0.036 | 0.069\n",
      "2023-12-30 17:24:34 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 823040 examples: 0.047 | 0.073\n",
      "2023-12-30 17:24:35 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 824896 examples: 0.041 | 0.076\n",
      "2023-12-30 17:24:35 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 826752 examples: 0.033 | 0.075\n",
      "2023-12-30 17:24:35 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 828608 examples: 0.033 | 0.071\n",
      "2023-12-30 17:24:36 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 830464 examples: 0.041 | 0.070\n",
      "2023-12-30 17:24:36 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 832320 examples: 0.045 | 0.071\n",
      "2023-12-30 17:24:36 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 834176 examples: 0.044 | 0.105\n",
      "2023-12-30 17:24:36 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 836032 examples: 0.046 | 0.076\n",
      "2023-12-30 17:24:37 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 837888 examples: 0.032 | 0.073\n",
      "2023-12-30 17:24:37 - INFO     | Epoch: 14 | Learning Rate: 0.005: Training/Validation Loss after 839744 examples: 0.041 | 0.070\n",
      "2023-12-30 17:24:37 - INFO     | Early stopping: no decrease (0.063 vs 0.067); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:10<00:43,  8.74s/it]2023-12-30 17:24:37 - INFO     | Epoch: 15 | Learning Rate: 0.005\n",
      "2023-12-30 17:24:37 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 840064 examples: 0.010 | 0.066\n",
      "2023-12-30 17:24:38 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 841920 examples: 0.024 | 0.068\n",
      "2023-12-30 17:24:38 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 843776 examples: 0.020 | 0.066\n",
      "2023-12-30 17:24:38 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 845632 examples: 0.030 | 0.069\n",
      "2023-12-30 17:24:39 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 847488 examples: 0.021 | 0.070\n",
      "2023-12-30 17:24:39 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 849344 examples: 0.037 | 0.071\n",
      "2023-12-30 17:24:39 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 851200 examples: 0.032 | 0.071\n",
      "2023-12-30 17:24:39 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 853056 examples: 0.022 | 0.067\n",
      "2023-12-30 17:24:40 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 854912 examples: 0.030 | 0.074\n",
      "2023-12-30 17:24:40 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 856768 examples: 0.036 | 0.078\n",
      "2023-12-30 17:24:40 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 858624 examples: 0.039 | 0.072\n",
      "2023-12-30 17:24:40 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 860480 examples: 0.030 | 0.073\n",
      "2023-12-30 17:24:41 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 862336 examples: 0.045 | 0.070\n",
      "2023-12-30 17:24:41 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 864192 examples: 0.032 | 0.074\n",
      "2023-12-30 17:24:41 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 866048 examples: 0.035 | 0.072\n",
      "2023-12-30 17:24:42 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 867904 examples: 0.027 | 0.073\n",
      "2023-12-30 17:24:42 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 869760 examples: 0.046 | 0.071\n",
      "2023-12-30 17:24:42 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 871616 examples: 0.043 | 0.071\n",
      "2023-12-30 17:24:42 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 873472 examples: 0.031 | 0.074\n",
      "2023-12-30 17:24:43 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 875328 examples: 0.037 | 0.069\n",
      "2023-12-30 17:24:43 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 877184 examples: 0.034 | 0.070\n",
      "2023-12-30 17:24:43 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 879040 examples: 0.037 | 0.078\n",
      "2023-12-30 17:24:43 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 880896 examples: 0.055 | 0.069\n",
      "2023-12-30 17:24:44 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 882752 examples: 0.034 | 0.067\n",
      "2023-12-30 17:24:44 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 884608 examples: 0.040 | 0.069\n",
      "2023-12-30 17:24:44 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 886464 examples: 0.038 | 0.068\n",
      "2023-12-30 17:24:45 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 888320 examples: 0.024 | 0.070\n",
      "2023-12-30 17:24:45 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 890176 examples: 0.029 | 0.074\n",
      "2023-12-30 17:24:45 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 892032 examples: 0.033 | 0.075\n",
      "2023-12-30 17:24:45 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 893888 examples: 0.026 | 0.072\n",
      "2023-12-30 17:24:46 - INFO     | Epoch: 15 | Learning Rate: 0.005: Training/Validation Loss after 895744 examples: 0.038 | 0.075\n",
      "2023-12-30 17:24:46 - INFO     | Early stopping: no decrease (0.063 vs 0.070); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:19<00:35,  8.78s/it]2023-12-30 17:24:46 - INFO     | Epoch: 16 | Learning Rate: 0.005\n",
      "2023-12-30 17:24:46 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 896064 examples: 0.113 | 0.070\n",
      "2023-12-30 17:24:47 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 897920 examples: 0.035 | 0.075\n",
      "2023-12-30 17:24:47 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 899776 examples: 0.027 | 0.067\n",
      "2023-12-30 17:24:47 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 901632 examples: 0.025 | 0.072\n",
      "2023-12-30 17:24:48 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 903488 examples: 0.037 | 0.069\n",
      "2023-12-30 17:24:48 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 905344 examples: 0.024 | 0.078\n",
      "2023-12-30 17:24:48 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 907200 examples: 0.025 | 0.071\n",
      "2023-12-30 17:24:48 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 909056 examples: 0.032 | 0.068\n",
      "2023-12-30 17:24:49 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 910912 examples: 0.030 | 0.070\n",
      "2023-12-30 17:24:49 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 912768 examples: 0.046 | 0.074\n",
      "2023-12-30 17:24:49 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 914624 examples: 0.028 | 0.071\n",
      "2023-12-30 17:24:50 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 916480 examples: 0.038 | 0.072\n",
      "2023-12-30 17:24:50 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 918336 examples: 0.031 | 0.069\n",
      "2023-12-30 17:24:50 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 920192 examples: 0.026 | 0.068\n",
      "2023-12-30 17:24:50 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 922048 examples: 0.044 | 0.073\n",
      "2023-12-30 17:24:51 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 923904 examples: 0.031 | 0.070\n",
      "2023-12-30 17:24:51 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 925760 examples: 0.031 | 0.068\n",
      "2023-12-30 17:24:51 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 927616 examples: 0.022 | 0.071\n",
      "2023-12-30 17:24:51 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 929472 examples: 0.028 | 0.076\n",
      "2023-12-30 17:24:52 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 931328 examples: 0.040 | 0.075\n",
      "2023-12-30 17:24:52 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 933184 examples: 0.033 | 0.079\n",
      "2023-12-30 17:24:52 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 935040 examples: 0.029 | 0.071\n",
      "2023-12-30 17:24:52 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 936896 examples: 0.034 | 0.069\n",
      "2023-12-30 17:24:53 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 938752 examples: 0.023 | 0.071\n",
      "2023-12-30 17:24:53 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 940608 examples: 0.026 | 0.068\n",
      "2023-12-30 17:24:53 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 942464 examples: 0.029 | 0.069\n",
      "2023-12-30 17:24:54 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 944320 examples: 0.034 | 0.068\n",
      "2023-12-30 17:24:54 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 946176 examples: 0.042 | 0.078\n",
      "2023-12-30 17:24:54 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 948032 examples: 0.035 | 0.077\n",
      "2023-12-30 17:24:54 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 949888 examples: 0.033 | 0.076\n",
      "2023-12-30 17:24:55 - INFO     | Epoch: 16 | Learning Rate: 0.005: Training/Validation Loss after 951744 examples: 0.041 | 0.073\n",
      "2023-12-30 17:24:55 - INFO     | Early stopping: no decrease (0.063 vs 0.073); counter: 3 out of 3\n",
      "2023-12-30 17:24:55 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:24:55 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 85%|████████▌ | 17/20 [02:28<00:26,  8.82s/it]2023-12-30 17:24:55 - INFO     | Epoch: 17 | Learning Rate: 0.003\n",
      "2023-12-30 17:24:55 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 952064 examples: 0.027 | 0.073\n",
      "2023-12-30 17:24:55 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 953920 examples: 0.025 | 0.069\n",
      "2023-12-30 17:24:56 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 955776 examples: 0.018 | 0.069\n",
      "2023-12-30 17:24:56 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 957632 examples: 0.017 | 0.068\n",
      "2023-12-30 17:24:56 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 959488 examples: 0.020 | 0.070\n",
      "2023-12-30 17:24:57 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 961344 examples: 0.028 | 0.069\n",
      "2023-12-30 17:24:57 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 963200 examples: 0.022 | 0.073\n",
      "2023-12-30 17:24:57 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 965056 examples: 0.022 | 0.069\n",
      "2023-12-30 17:24:57 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 966912 examples: 0.023 | 0.071\n",
      "2023-12-30 17:24:58 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 968768 examples: 0.023 | 0.068\n",
      "2023-12-30 17:24:58 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 970624 examples: 0.020 | 0.069\n",
      "2023-12-30 17:24:58 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 972480 examples: 0.024 | 0.067\n",
      "2023-12-30 17:24:58 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 974336 examples: 0.020 | 0.068\n",
      "2023-12-30 17:24:59 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 976192 examples: 0.028 | 0.070\n",
      "2023-12-30 17:24:59 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 978048 examples: 0.030 | 0.070\n",
      "2023-12-30 17:24:59 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 979904 examples: 0.023 | 0.069\n",
      "2023-12-30 17:24:59 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 981760 examples: 0.020 | 0.070\n",
      "2023-12-30 17:25:00 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 983616 examples: 0.044 | 0.068\n",
      "2023-12-30 17:25:00 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 985472 examples: 0.034 | 0.068\n",
      "2023-12-30 17:25:00 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 987328 examples: 0.024 | 0.067\n",
      "2023-12-30 17:25:00 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 989184 examples: 0.026 | 0.070\n",
      "2023-12-30 17:25:01 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 991040 examples: 0.023 | 0.070\n",
      "2023-12-30 17:25:01 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 992896 examples: 0.027 | 0.068\n",
      "2023-12-30 17:25:01 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 994752 examples: 0.036 | 0.067\n",
      "2023-12-30 17:25:02 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 996608 examples: 0.020 | 0.065\n",
      "2023-12-30 17:25:02 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 998464 examples: 0.027 | 0.068\n",
      "2023-12-30 17:25:02 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1000320 examples: 0.037 | 0.071\n",
      "2023-12-30 17:25:02 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1002176 examples: 0.028 | 0.068\n",
      "2023-12-30 17:25:03 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1004032 examples: 0.022 | 0.068\n",
      "2023-12-30 17:25:03 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1005888 examples: 0.036 | 0.068\n",
      "2023-12-30 17:25:03 - INFO     | Epoch: 17 | Learning Rate: 0.003: Training/Validation Loss after 1007744 examples: 0.036 | 0.070\n",
      "2023-12-30 17:25:03 - INFO     | Early stopping: no decrease (0.063 vs 0.069); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [02:37<00:17,  8.70s/it]2023-12-30 17:25:03 - INFO     | Epoch: 18 | Learning Rate: 0.003\n",
      "2023-12-30 17:25:04 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1008064 examples: 0.014 | 0.069\n",
      "2023-12-30 17:25:04 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1009920 examples: 0.023 | 0.073\n",
      "2023-12-30 17:25:04 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1011776 examples: 0.016 | 0.070\n",
      "2023-12-30 17:25:04 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1013632 examples: 0.018 | 0.069\n",
      "2023-12-30 17:25:05 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1015488 examples: 0.025 | 0.073\n",
      "2023-12-30 17:25:05 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1017344 examples: 0.025 | 0.070\n",
      "2023-12-30 17:25:05 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1019200 examples: 0.029 | 0.069\n",
      "2023-12-30 17:25:05 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1021056 examples: 0.023 | 0.069\n",
      "2023-12-30 17:25:06 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1022912 examples: 0.028 | 0.068\n",
      "2023-12-30 17:25:06 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1024768 examples: 0.021 | 0.069\n",
      "2023-12-30 17:25:06 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1026624 examples: 0.028 | 0.071\n",
      "2023-12-30 17:25:07 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1028480 examples: 0.025 | 0.069\n",
      "2023-12-30 17:25:07 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1030336 examples: 0.028 | 0.070\n",
      "2023-12-30 17:25:07 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1032192 examples: 0.023 | 0.068\n",
      "2023-12-30 17:25:07 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1034048 examples: 0.032 | 0.069\n",
      "2023-12-30 17:25:08 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1035904 examples: 0.018 | 0.069\n",
      "2023-12-30 17:25:08 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1037760 examples: 0.029 | 0.069\n",
      "2023-12-30 17:25:08 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1039616 examples: 0.018 | 0.071\n",
      "2023-12-30 17:25:08 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1041472 examples: 0.030 | 0.070\n",
      "2023-12-30 17:25:09 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1043328 examples: 0.012 | 0.070\n",
      "2023-12-30 17:25:09 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1045184 examples: 0.021 | 0.071\n",
      "2023-12-30 17:25:09 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1047040 examples: 0.029 | 0.071\n",
      "2023-12-30 17:25:09 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1048896 examples: 0.025 | 0.074\n",
      "2023-12-30 17:25:10 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1050752 examples: 0.017 | 0.071\n",
      "2023-12-30 17:25:10 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1052608 examples: 0.024 | 0.074\n",
      "2023-12-30 17:25:10 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1054464 examples: 0.031 | 0.069\n",
      "2023-12-30 17:25:11 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1056320 examples: 0.019 | 0.069\n",
      "2023-12-30 17:25:11 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1058176 examples: 0.022 | 0.070\n",
      "2023-12-30 17:25:11 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1060032 examples: 0.024 | 0.069\n",
      "2023-12-30 17:25:11 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1061888 examples: 0.043 | 0.069\n",
      "2023-12-30 17:25:12 - INFO     | Epoch: 18 | Learning Rate: 0.003: Training/Validation Loss after 1063744 examples: 0.032 | 0.071\n",
      "2023-12-30 17:25:12 - INFO     | Early stopping: no decrease (0.063 vs 0.072); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [02:45<00:08,  8.66s/it]2023-12-30 17:25:12 - INFO     | Epoch: 19 | Learning Rate: 0.003\n",
      "2023-12-30 17:25:12 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1064064 examples: 0.037 | 0.071\n",
      "2023-12-30 17:25:12 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1065920 examples: 0.023 | 0.072\n",
      "2023-12-30 17:25:13 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1067776 examples: 0.022 | 0.071\n",
      "2023-12-30 17:25:13 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1069632 examples: 0.021 | 0.070\n",
      "2023-12-30 17:25:13 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1071488 examples: 0.039 | 0.070\n",
      "2023-12-30 17:25:14 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1073344 examples: 0.018 | 0.073\n",
      "2023-12-30 17:25:14 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1075200 examples: 0.018 | 0.077\n",
      "2023-12-30 17:25:14 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1077056 examples: 0.021 | 0.072\n",
      "2023-12-30 17:25:14 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1078912 examples: 0.030 | 0.070\n",
      "2023-12-30 17:25:15 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1080768 examples: 0.021 | 0.069\n",
      "2023-12-30 17:25:15 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1082624 examples: 0.024 | 0.073\n",
      "2023-12-30 17:25:15 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1084480 examples: 0.018 | 0.069\n",
      "2023-12-30 17:25:15 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1086336 examples: 0.034 | 0.069\n",
      "2023-12-30 17:25:16 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1088192 examples: 0.029 | 0.072\n",
      "2023-12-30 17:25:16 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1090048 examples: 0.020 | 0.070\n",
      "2023-12-30 17:25:16 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1091904 examples: 0.023 | 0.073\n",
      "2023-12-30 17:25:17 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1093760 examples: 0.023 | 0.069\n",
      "2023-12-30 17:25:17 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1095616 examples: 0.018 | 0.070\n",
      "2023-12-30 17:25:17 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1097472 examples: 0.030 | 0.069\n",
      "2023-12-30 17:25:17 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1099328 examples: 0.014 | 0.069\n",
      "2023-12-30 17:25:18 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1101184 examples: 0.022 | 0.069\n",
      "2023-12-30 17:25:18 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1103040 examples: 0.022 | 0.067\n",
      "2023-12-30 17:25:18 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1104896 examples: 0.020 | 0.068\n",
      "2023-12-30 17:25:18 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1106752 examples: 0.025 | 0.071\n",
      "2023-12-30 17:25:19 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1108608 examples: 0.022 | 0.074\n",
      "2023-12-30 17:25:19 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1110464 examples: 0.026 | 0.072\n",
      "2023-12-30 17:25:19 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1112320 examples: 0.027 | 0.070\n",
      "2023-12-30 17:25:20 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1114176 examples: 0.021 | 0.069\n",
      "2023-12-30 17:25:20 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1116032 examples: 0.023 | 0.068\n",
      "2023-12-30 17:25:20 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1117888 examples: 0.026 | 0.073\n",
      "2023-12-30 17:25:20 - INFO     | Epoch: 19 | Learning Rate: 0.003: Training/Validation Loss after 1119744 examples: 0.030 | 0.071\n",
      "2023-12-30 17:25:21 - INFO     | Early stopping: no decrease (0.063 vs 0.073); counter: 3 out of 3\n",
      "2023-12-30 17:25:21 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:25:21 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      "100%|██████████| 20/20 [02:54<00:00,  8.72s/it]\n",
      "2023-12-30 17:25:21 - INFO     | Best validation loss: 0.063\n",
      "2023-12-30 17:25:21 - INFO     | Best early stopping index/epoch: 13\n",
      "2023-12-30 17:25:21 - INFO     | Average Loss on test set: 0.059\n",
      "2023-12-30 17:25:23 - INFO     | Weighted Precision: 0.982, Recall: 0.982, F1: 0.982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>███████████▃▃▃▃▃▃▁▁▁</td></tr><tr><td>step_learning_rate</td><td>██████████████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_validation_loss</td><td>0.06343</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.0025</td></tr><tr><td>step_learning_rate</td><td>0.0025</td></tr><tr><td>step_training_loss</td><td>0.03045</td></tr><tr><td>step_validation_loss</td><td>0.071</td></tr><tr><td>test_loss</td><td>0.05907</td></tr><tr><td>weighted_f1</td><td>0.98171</td></tr><tr><td>weighted_precision</td><td>0.98175</td></tr><tr><td>weighted_recall</td><td>0.98171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-2</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d0ngkzgm' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d0ngkzgm</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_172226-d0ngkzgm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k8t7kfsi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [8, 16]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_172534-k8t7kfsi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/k8t7kfsi' target=\"_blank\">autumn-sweep-3</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/k8t7kfsi' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/k8t7kfsi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [8, 16], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:25:34 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 17:25:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 37.373 | 44.593\n",
      "2023-12-30 17:25:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 5.630 | 1.364\n",
      "2023-12-30 17:25:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 0.975 | 0.694\n",
      "2023-12-30 17:25:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 0.587 | 0.547\n",
      "2023-12-30 17:25:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 0.483 | 0.446\n",
      "2023-12-30 17:25:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 0.453 | 0.362\n",
      "2023-12-30 17:25:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 0.313 | 0.285\n",
      "2023-12-30 17:25:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.283 | 0.279\n",
      "2023-12-30 17:25:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.257 | 0.274\n",
      "2023-12-30 17:25:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.281 | 0.272\n",
      "2023-12-30 17:25:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.240 | 0.256\n",
      "2023-12-30 17:25:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.280 | 0.302\n",
      "2023-12-30 17:25:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.241 | 0.235\n",
      "2023-12-30 17:25:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.276 | 0.242\n",
      "2023-12-30 17:25:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.213 | 0.185\n",
      "2023-12-30 17:25:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.178 | 0.205\n",
      "2023-12-30 17:25:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.212 | 0.180\n",
      "2023-12-30 17:25:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.211 | 0.173\n",
      "2023-12-30 17:25:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.152 | 0.162\n",
      "2023-12-30 17:25:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.169 | 0.170\n",
      "2023-12-30 17:25:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.208 | 0.171\n",
      "2023-12-30 17:25:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.187 | 0.179\n",
      "2023-12-30 17:25:41 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.175 | 0.154\n",
      "2023-12-30 17:25:41 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.173 | 0.187\n",
      "2023-12-30 17:25:41 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.183 | 0.161\n",
      "2023-12-30 17:25:42 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.221 | 0.192\n",
      "2023-12-30 17:25:42 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.190 | 0.157\n",
      "2023-12-30 17:25:42 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.143 | 0.167\n",
      "2023-12-30 17:25:42 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.160 | 0.154\n",
      "2023-12-30 17:25:43 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.172 | 0.162\n",
      "2023-12-30 17:25:43 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.148 | 0.133\n",
      "2023-12-30 17:25:43 - INFO     | Early stopping: loss decreased (inf -> 0.141; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:08<02:50,  8.96s/it]2023-12-30 17:25:43 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 17:25:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.096 | 0.137\n",
      "2023-12-30 17:25:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.138 | 0.148\n",
      "2023-12-30 17:25:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.143 | 0.159\n",
      "2023-12-30 17:25:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.103 | 0.124\n",
      "2023-12-30 17:25:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.161 | 0.139\n",
      "2023-12-30 17:25:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.133 | 0.117\n",
      "2023-12-30 17:25:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.148 | 0.157\n",
      "2023-12-30 17:25:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.160 | 0.121\n",
      "2023-12-30 17:25:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.137 | 0.155\n",
      "2023-12-30 17:25:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.145 | 0.124\n",
      "2023-12-30 17:25:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.135 | 0.129\n",
      "2023-12-30 17:25:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.137 | 0.125\n",
      "2023-12-30 17:25:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.127 | 0.137\n",
      "2023-12-30 17:25:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.107 | 0.134\n",
      "2023-12-30 17:25:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.173 | 0.161\n",
      "2023-12-30 17:25:48 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.165 | 0.140\n",
      "2023-12-30 17:25:48 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.137 | 0.165\n",
      "2023-12-30 17:25:48 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.157 | 0.136\n",
      "2023-12-30 17:25:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.139 | 0.124\n",
      "2023-12-30 17:25:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.125 | 0.117\n",
      "2023-12-30 17:25:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.137 | 0.141\n",
      "2023-12-30 17:25:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.147 | 0.136\n",
      "2023-12-30 17:25:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.118 | 0.152\n",
      "2023-12-30 17:25:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.141 | 0.114\n",
      "2023-12-30 17:25:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.114 | 0.124\n",
      "2023-12-30 17:25:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.127 | 0.129\n",
      "2023-12-30 17:25:51 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.107 | 0.120\n",
      "2023-12-30 17:25:51 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.158 | 0.111\n",
      "2023-12-30 17:25:51 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.111 | 0.119\n",
      "2023-12-30 17:25:52 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.143 | 0.133\n",
      "2023-12-30 17:25:52 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.102 | 0.153\n",
      "2023-12-30 17:25:52 - INFO     | Early stopping: no decrease (0.141 vs 0.147); counter: 1 out of 3\n",
      " 10%|█         | 2/20 [00:17<02:40,  8.93s/it]2023-12-30 17:25:52 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 17:25:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.163 | 0.145\n",
      "2023-12-30 17:25:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.152 | 0.167\n",
      "2023-12-30 17:25:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.095 | 0.155\n",
      "2023-12-30 17:25:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.136 | 0.150\n",
      "2023-12-30 17:25:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.128 | 0.140\n",
      "2023-12-30 17:25:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.152 | 0.154\n",
      "2023-12-30 17:25:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.140 | 0.131\n",
      "2023-12-30 17:25:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.106 | 0.120\n",
      "2023-12-30 17:25:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.114 | 0.161\n",
      "2023-12-30 17:25:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.144 | 0.154\n",
      "2023-12-30 17:25:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.108 | 0.118\n",
      "2023-12-30 17:25:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.128 | 0.135\n",
      "2023-12-30 17:25:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.131 | 0.118\n",
      "2023-12-30 17:25:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.112 | 0.138\n",
      "2023-12-30 17:25:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.131 | 0.131\n",
      "2023-12-30 17:25:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.089 | 0.158\n",
      "2023-12-30 17:25:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.132 | 0.152\n",
      "2023-12-30 17:25:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.128 | 0.108\n",
      "2023-12-30 17:25:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.129 | 0.135\n",
      "2023-12-30 17:25:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.128 | 0.148\n",
      "2023-12-30 17:25:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.127 | 0.128\n",
      "2023-12-30 17:25:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.130 | 0.109\n",
      "2023-12-30 17:25:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.100 | 0.125\n",
      "2023-12-30 17:25:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.111 | 0.144\n",
      "2023-12-30 17:25:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.126 | 0.167\n",
      "2023-12-30 17:25:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.165 | 0.160\n",
      "2023-12-30 17:26:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.112 | 0.118\n",
      "2023-12-30 17:26:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.138 | 0.123\n",
      "2023-12-30 17:26:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.135 | 0.146\n",
      "2023-12-30 17:26:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.144 | 0.124\n",
      "2023-12-30 17:26:01 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.146 | 0.128\n",
      "2023-12-30 17:26:01 - INFO     | Early stopping: loss decreased (0.141 -> 0.126; -10.7%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:26<02:31,  8.93s/it]2023-12-30 17:26:01 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 17:26:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.111 | 0.123\n",
      "2023-12-30 17:26:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.125 | 0.153\n",
      "2023-12-30 17:26:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.139 | 0.108\n",
      "2023-12-30 17:26:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.115 | 0.112\n",
      "2023-12-30 17:26:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.109 | 0.123\n",
      "2023-12-30 17:26:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.104 | 0.125\n",
      "2023-12-30 17:26:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.107 | 0.195\n",
      "2023-12-30 17:26:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.103 | 0.169\n",
      "2023-12-30 17:26:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.150 | 0.138\n",
      "2023-12-30 17:26:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.116 | 0.134\n",
      "2023-12-30 17:26:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.130 | 0.142\n",
      "2023-12-30 17:26:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.098 | 0.164\n",
      "2023-12-30 17:26:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.144 | 0.151\n",
      "2023-12-30 17:26:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.150 | 0.167\n",
      "2023-12-30 17:26:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.153 | 0.133\n",
      "2023-12-30 17:26:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.144 | 0.208\n",
      "2023-12-30 17:26:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.149 | 0.132\n",
      "2023-12-30 17:26:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.120 | 0.123\n",
      "2023-12-30 17:26:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.114 | 0.111\n",
      "2023-12-30 17:26:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.102 | 0.104\n",
      "2023-12-30 17:26:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.110 | 0.118\n",
      "2023-12-30 17:26:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.118 | 0.151\n",
      "2023-12-30 17:26:08 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.120 | 0.120\n",
      "2023-12-30 17:26:08 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.122 | 0.132\n",
      "2023-12-30 17:26:08 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.109 | 0.145\n",
      "2023-12-30 17:26:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.105 | 0.121\n",
      "2023-12-30 17:26:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.128 | 0.139\n",
      "2023-12-30 17:26:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.100 | 0.138\n",
      "2023-12-30 17:26:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.124 | 0.159\n",
      "2023-12-30 17:26:10 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.181 | 0.162\n",
      "2023-12-30 17:26:10 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.118 | 0.155\n",
      "2023-12-30 17:26:10 - INFO     | Early stopping: no decrease (0.126 vs 0.128); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:36<02:24,  9.06s/it]2023-12-30 17:26:10 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 17:26:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.040 | 0.130\n",
      "2023-12-30 17:26:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.175 | 0.192\n",
      "2023-12-30 17:26:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.121 | 0.152\n",
      "2023-12-30 17:26:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.141 | 0.214\n",
      "2023-12-30 17:26:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.111 | 0.126\n",
      "2023-12-30 17:26:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.132 | 0.148\n",
      "2023-12-30 17:26:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.100 | 0.152\n",
      "2023-12-30 17:26:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.142 | 0.153\n",
      "2023-12-30 17:26:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.155 | 0.151\n",
      "2023-12-30 17:26:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.109 | 0.139\n",
      "2023-12-30 17:26:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.142 | 0.170\n",
      "2023-12-30 17:26:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.191 | 0.130\n",
      "2023-12-30 17:26:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.081 | 0.149\n",
      "2023-12-30 17:26:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.154 | 0.166\n",
      "2023-12-30 17:26:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.157 | 0.143\n",
      "2023-12-30 17:26:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.118 | 0.162\n",
      "2023-12-30 17:26:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.151 | 0.133\n",
      "2023-12-30 17:26:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.142 | 0.160\n",
      "2023-12-30 17:26:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.154 | 0.142\n",
      "2023-12-30 17:26:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.149 | 0.168\n",
      "2023-12-30 17:26:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.162 | 0.127\n",
      "2023-12-30 17:26:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.102 | 0.120\n",
      "2023-12-30 17:26:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.132 | 0.123\n",
      "2023-12-30 17:26:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.095 | 0.129\n",
      "2023-12-30 17:26:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.170 | 0.160\n",
      "2023-12-30 17:26:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.111 | 0.135\n",
      "2023-12-30 17:26:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.136 | 0.132\n",
      "2023-12-30 17:26:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.131 | 0.130\n",
      "2023-12-30 17:26:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.111 | 0.124\n",
      "2023-12-30 17:26:19 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.110 | 0.134\n",
      "2023-12-30 17:26:19 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.137 | 0.141\n",
      "2023-12-30 17:26:19 - INFO     | Early stopping: no decrease (0.126 vs 0.127); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:45<02:15,  9.06s/it]2023-12-30 17:26:19 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 17:26:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.039 | 0.124\n",
      "2023-12-30 17:26:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.102 | 0.139\n",
      "2023-12-30 17:26:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.112 | 0.149\n",
      "2023-12-30 17:26:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.103 | 0.169\n",
      "2023-12-30 17:26:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.141 | 0.150\n",
      "2023-12-30 17:26:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.104 | 0.161\n",
      "2023-12-30 17:26:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.114 | 0.138\n",
      "2023-12-30 17:26:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.127 | 0.135\n",
      "2023-12-30 17:26:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.099 | 0.153\n",
      "2023-12-30 17:26:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.148 | 0.128\n",
      "2023-12-30 17:26:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.124 | 0.156\n",
      "2023-12-30 17:26:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.144 | 0.136\n",
      "2023-12-30 17:26:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.129 | 0.165\n",
      "2023-12-30 17:26:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.133 | 0.152\n",
      "2023-12-30 17:26:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.139 | 0.194\n",
      "2023-12-30 17:26:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.144 | 0.164\n",
      "2023-12-30 17:26:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.127 | 0.136\n",
      "2023-12-30 17:26:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.151 | 0.131\n",
      "2023-12-30 17:26:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.137 | 0.141\n",
      "2023-12-30 17:26:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.141 | 0.126\n",
      "2023-12-30 17:26:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.116 | 0.123\n",
      "2023-12-30 17:26:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.100 | 0.133\n",
      "2023-12-30 17:26:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.101 | 0.148\n",
      "2023-12-30 17:26:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.147 | 0.168\n",
      "2023-12-30 17:26:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.147 | 0.134\n",
      "2023-12-30 17:26:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.133 | 0.121\n",
      "2023-12-30 17:26:27 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.138 | 0.121\n",
      "2023-12-30 17:26:27 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.110 | 0.118\n",
      "2023-12-30 17:26:27 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.121 | 0.151\n",
      "2023-12-30 17:26:28 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.122 | 0.141\n",
      "2023-12-30 17:26:28 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.107 | 0.133\n",
      "2023-12-30 17:26:28 - INFO     | Early stopping: no decrease (0.126 vs 0.131); counter: 3 out of 3\n",
      "2023-12-30 17:26:28 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:26:28 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 30%|███       | 6/20 [00:53<02:05,  8.99s/it]2023-12-30 17:26:28 - INFO     | Epoch: 6 | Learning Rate: 0.003\n",
      "2023-12-30 17:26:28 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 336064 examples: 0.028 | 0.122\n",
      "2023-12-30 17:26:29 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 337920 examples: 0.095 | 0.117\n",
      "2023-12-30 17:26:29 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 339776 examples: 0.101 | 0.121\n",
      "2023-12-30 17:26:29 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 341632 examples: 0.046 | 0.121\n",
      "2023-12-30 17:26:30 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 343488 examples: 0.100 | 0.105\n",
      "2023-12-30 17:26:30 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 345344 examples: 0.084 | 0.116\n",
      "2023-12-30 17:26:30 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 347200 examples: 0.097 | 0.106\n",
      "2023-12-30 17:26:30 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 349056 examples: 0.084 | 0.105\n",
      "2023-12-30 17:26:31 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 350912 examples: 0.084 | 0.105\n",
      "2023-12-30 17:26:31 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 352768 examples: 0.077 | 0.096\n",
      "2023-12-30 17:26:31 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 354624 examples: 0.073 | 0.125\n",
      "2023-12-30 17:26:31 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 356480 examples: 0.088 | 0.123\n",
      "2023-12-30 17:26:32 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 358336 examples: 0.091 | 0.109\n",
      "2023-12-30 17:26:32 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 360192 examples: 0.066 | 0.111\n",
      "2023-12-30 17:26:32 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 362048 examples: 0.085 | 0.123\n",
      "2023-12-30 17:26:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 363904 examples: 0.073 | 0.093\n",
      "2023-12-30 17:26:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 365760 examples: 0.064 | 0.091\n",
      "2023-12-30 17:26:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 367616 examples: 0.068 | 0.092\n",
      "2023-12-30 17:26:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 369472 examples: 0.079 | 0.101\n",
      "2023-12-30 17:26:34 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 371328 examples: 0.072 | 0.098\n",
      "2023-12-30 17:26:34 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 373184 examples: 0.103 | 0.112\n",
      "2023-12-30 17:26:34 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 375040 examples: 0.071 | 0.095\n",
      "2023-12-30 17:26:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 376896 examples: 0.071 | 0.105\n",
      "2023-12-30 17:26:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 378752 examples: 0.061 | 0.099\n",
      "2023-12-30 17:26:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 380608 examples: 0.058 | 0.088\n",
      "2023-12-30 17:26:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 382464 examples: 0.046 | 0.107\n",
      "2023-12-30 17:26:36 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 384320 examples: 0.071 | 0.120\n",
      "2023-12-30 17:26:36 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 386176 examples: 0.074 | 0.103\n",
      "2023-12-30 17:26:36 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 388032 examples: 0.088 | 0.094\n",
      "2023-12-30 17:26:37 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 389888 examples: 0.101 | 0.118\n",
      "2023-12-30 17:26:37 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 391744 examples: 0.076 | 0.095\n",
      "2023-12-30 17:26:37 - INFO     | Early stopping: loss decreased (0.126 -> 0.099; -21.5%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:02<01:56,  8.95s/it]2023-12-30 17:26:37 - INFO     | Epoch: 7 | Learning Rate: 0.003\n",
      "2023-12-30 17:26:37 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 392064 examples: 0.028 | 0.100\n",
      "2023-12-30 17:26:38 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 393920 examples: 0.065 | 0.095\n",
      "2023-12-30 17:26:38 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 395776 examples: 0.035 | 0.085\n",
      "2023-12-30 17:26:38 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 397632 examples: 0.048 | 0.106\n",
      "2023-12-30 17:26:38 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 399488 examples: 0.076 | 0.099\n",
      "2023-12-30 17:26:39 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 401344 examples: 0.069 | 0.097\n",
      "2023-12-30 17:26:39 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 403200 examples: 0.050 | 0.104\n",
      "2023-12-30 17:26:39 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 405056 examples: 0.091 | 0.089\n",
      "2023-12-30 17:26:40 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 406912 examples: 0.054 | 0.099\n",
      "2023-12-30 17:26:40 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 408768 examples: 0.045 | 0.093\n",
      "2023-12-30 17:26:40 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 410624 examples: 0.067 | 0.105\n",
      "2023-12-30 17:26:41 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 412480 examples: 0.072 | 0.110\n",
      "2023-12-30 17:26:41 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 414336 examples: 0.099 | 0.110\n",
      "2023-12-30 17:26:41 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 416192 examples: 0.074 | 0.095\n",
      "2023-12-30 17:26:41 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 418048 examples: 0.052 | 0.100\n",
      "2023-12-30 17:26:42 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 419904 examples: 0.046 | 0.089\n",
      "2023-12-30 17:26:42 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 421760 examples: 0.093 | 0.094\n",
      "2023-12-30 17:26:42 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 423616 examples: 0.081 | 0.106\n",
      "2023-12-30 17:26:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 425472 examples: 0.095 | 0.098\n",
      "2023-12-30 17:26:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 427328 examples: 0.068 | 0.091\n",
      "2023-12-30 17:26:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 429184 examples: 0.064 | 0.101\n",
      "2023-12-30 17:26:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 431040 examples: 0.092 | 0.094\n",
      "2023-12-30 17:26:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 432896 examples: 0.071 | 0.101\n",
      "2023-12-30 17:26:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 434752 examples: 0.068 | 0.103\n",
      "2023-12-30 17:26:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 436608 examples: 0.058 | 0.096\n",
      "2023-12-30 17:26:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 438464 examples: 0.080 | 0.100\n",
      "2023-12-30 17:26:45 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 440320 examples: 0.074 | 0.112\n",
      "2023-12-30 17:26:45 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 442176 examples: 0.073 | 0.102\n",
      "2023-12-30 17:26:45 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 444032 examples: 0.100 | 0.111\n",
      "2023-12-30 17:26:46 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 445888 examples: 0.101 | 0.101\n",
      "2023-12-30 17:26:46 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 447744 examples: 0.071 | 0.095\n",
      "2023-12-30 17:26:46 - INFO     | Early stopping: loss decreased (0.099 -> 0.093; -6.2%). Caching model state.\n",
      " 40%|████      | 8/20 [01:11<01:47,  8.99s/it]2023-12-30 17:26:46 - INFO     | Epoch: 8 | Learning Rate: 0.003\n",
      "2023-12-30 17:26:46 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 448064 examples: 0.016 | 0.093\n",
      "2023-12-30 17:26:47 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 449920 examples: 0.059 | 0.091\n",
      "2023-12-30 17:26:47 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 451776 examples: 0.046 | 0.089\n",
      "2023-12-30 17:26:47 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 453632 examples: 0.037 | 0.090\n",
      "2023-12-30 17:26:48 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 455488 examples: 0.054 | 0.090\n",
      "2023-12-30 17:26:48 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 457344 examples: 0.042 | 0.090\n",
      "2023-12-30 17:26:48 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 459200 examples: 0.055 | 0.097\n",
      "2023-12-30 17:26:48 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 461056 examples: 0.064 | 0.100\n",
      "2023-12-30 17:26:49 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 462912 examples: 0.060 | 0.093\n",
      "2023-12-30 17:26:49 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 464768 examples: 0.058 | 0.106\n",
      "2023-12-30 17:26:49 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 466624 examples: 0.078 | 0.091\n",
      "2023-12-30 17:26:49 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 468480 examples: 0.058 | 0.094\n",
      "2023-12-30 17:26:50 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 470336 examples: 0.076 | 0.096\n",
      "2023-12-30 17:26:50 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 472192 examples: 0.060 | 0.099\n",
      "2023-12-30 17:26:50 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 474048 examples: 0.061 | 0.119\n",
      "2023-12-30 17:26:51 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 475904 examples: 0.058 | 0.111\n",
      "2023-12-30 17:26:51 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 477760 examples: 0.056 | 0.100\n",
      "2023-12-30 17:26:51 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 479616 examples: 0.088 | 0.106\n",
      "2023-12-30 17:26:51 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 481472 examples: 0.054 | 0.097\n",
      "2023-12-30 17:26:52 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 483328 examples: 0.069 | 0.096\n",
      "2023-12-30 17:26:52 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 485184 examples: 0.083 | 0.108\n",
      "2023-12-30 17:26:52 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 487040 examples: 0.061 | 0.093\n",
      "2023-12-30 17:26:52 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 488896 examples: 0.063 | 0.095\n",
      "2023-12-30 17:26:53 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 490752 examples: 0.053 | 0.107\n",
      "2023-12-30 17:26:53 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 492608 examples: 0.063 | 0.141\n",
      "2023-12-30 17:26:53 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 494464 examples: 0.112 | 0.122\n",
      "2023-12-30 17:26:54 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 496320 examples: 0.053 | 0.091\n",
      "2023-12-30 17:26:54 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 498176 examples: 0.077 | 0.118\n",
      "2023-12-30 17:26:54 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 500032 examples: 0.079 | 0.097\n",
      "2023-12-30 17:26:55 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 501888 examples: 0.057 | 0.094\n",
      "2023-12-30 17:26:55 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 503744 examples: 0.090 | 0.105\n",
      "2023-12-30 17:26:55 - INFO     | Early stopping: no decrease (0.093 vs 0.096); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:20<01:38,  8.97s/it]2023-12-30 17:26:55 - INFO     | Epoch: 9 | Learning Rate: 0.003\n",
      "2023-12-30 17:26:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 504064 examples: 0.003 | 0.095\n",
      "2023-12-30 17:26:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 505920 examples: 0.057 | 0.095\n",
      "2023-12-30 17:26:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 507776 examples: 0.052 | 0.100\n",
      "2023-12-30 17:26:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 509632 examples: 0.044 | 0.098\n",
      "2023-12-30 17:26:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 511488 examples: 0.064 | 0.104\n",
      "2023-12-30 17:26:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 513344 examples: 0.048 | 0.093\n",
      "2023-12-30 17:26:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 515200 examples: 0.052 | 0.111\n",
      "2023-12-30 17:26:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 517056 examples: 0.064 | 0.105\n",
      "2023-12-30 17:26:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 518912 examples: 0.052 | 0.107\n",
      "2023-12-30 17:26:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 520768 examples: 0.077 | 0.096\n",
      "2023-12-30 17:26:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 522624 examples: 0.067 | 0.092\n",
      "2023-12-30 17:26:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 524480 examples: 0.048 | 0.091\n",
      "2023-12-30 17:26:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 526336 examples: 0.040 | 0.101\n",
      "2023-12-30 17:26:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 528192 examples: 0.048 | 0.100\n",
      "2023-12-30 17:26:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 530048 examples: 0.060 | 0.092\n",
      "2023-12-30 17:26:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 531904 examples: 0.054 | 0.098\n",
      "2023-12-30 17:27:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 533760 examples: 0.073 | 0.089\n",
      "2023-12-30 17:27:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 535616 examples: 0.067 | 0.099\n",
      "2023-12-30 17:27:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 537472 examples: 0.068 | 0.096\n",
      "2023-12-30 17:27:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 539328 examples: 0.043 | 0.090\n",
      "2023-12-30 17:27:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 541184 examples: 0.041 | 0.100\n",
      "2023-12-30 17:27:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 543040 examples: 0.055 | 0.104\n",
      "2023-12-30 17:27:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 544896 examples: 0.077 | 0.096\n",
      "2023-12-30 17:27:02 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 546752 examples: 0.059 | 0.093\n",
      "2023-12-30 17:27:02 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 548608 examples: 0.046 | 0.108\n",
      "2023-12-30 17:27:02 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 550464 examples: 0.051 | 0.118\n",
      "2023-12-30 17:27:03 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 552320 examples: 0.078 | 0.097\n",
      "2023-12-30 17:27:03 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 554176 examples: 0.053 | 0.107\n",
      "2023-12-30 17:27:03 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 556032 examples: 0.074 | 0.115\n",
      "2023-12-30 17:27:03 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 557888 examples: 0.073 | 0.093\n",
      "2023-12-30 17:27:04 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 559744 examples: 0.083 | 0.100\n",
      "2023-12-30 17:27:04 - INFO     | Early stopping: no decrease (0.093 vs 0.110); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:29<01:29,  8.95s/it]2023-12-30 17:27:04 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 17:27:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.025 | 0.112\n",
      "2023-12-30 17:27:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.055 | 0.102\n",
      "2023-12-30 17:27:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.037 | 0.097\n",
      "2023-12-30 17:27:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.031 | 0.106\n",
      "2023-12-30 17:27:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.029 | 0.102\n",
      "2023-12-30 17:27:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.054 | 0.127\n",
      "2023-12-30 17:27:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.076 | 0.125\n",
      "2023-12-30 17:27:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.088 | 0.103\n",
      "2023-12-30 17:27:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.061 | 0.109\n",
      "2023-12-30 17:27:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.062 | 0.098\n",
      "2023-12-30 17:27:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.052 | 0.100\n",
      "2023-12-30 17:27:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.043 | 0.099\n",
      "2023-12-30 17:27:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.047 | 0.104\n",
      "2023-12-30 17:27:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.061 | 0.115\n",
      "2023-12-30 17:27:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.056 | 0.106\n",
      "2023-12-30 17:27:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.057 | 0.108\n",
      "2023-12-30 17:27:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.064 | 0.101\n",
      "2023-12-30 17:27:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.065 | 0.112\n",
      "2023-12-30 17:27:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.065 | 0.095\n",
      "2023-12-30 17:27:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.055 | 0.104\n",
      "2023-12-30 17:27:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.062 | 0.121\n",
      "2023-12-30 17:27:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.082 | 0.094\n",
      "2023-12-30 17:27:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.073 | 0.108\n",
      "2023-12-30 17:27:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.076 | 0.098\n",
      "2023-12-30 17:27:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.046 | 0.120\n",
      "2023-12-30 17:27:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.062 | 0.131\n",
      "2023-12-30 17:27:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.057 | 0.100\n",
      "2023-12-30 17:27:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.062 | 0.127\n",
      "2023-12-30 17:27:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.076 | 0.149\n",
      "2023-12-30 17:27:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.064 | 0.125\n",
      "2023-12-30 17:27:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.073 | 0.129\n",
      "2023-12-30 17:27:13 - INFO     | Early stopping: no decrease (0.093 vs 0.131); counter: 3 out of 3\n",
      "2023-12-30 17:27:13 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:27:13 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 55%|█████▌    | 11/20 [01:38<01:20,  8.96s/it]2023-12-30 17:27:13 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 17:27:13 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.207 | 0.124\n",
      "2023-12-30 17:27:13 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.034 | 0.116\n",
      "2023-12-30 17:27:14 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.043 | 0.106\n",
      "2023-12-30 17:27:14 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.027 | 0.106\n",
      "2023-12-30 17:27:14 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.047 | 0.113\n",
      "2023-12-30 17:27:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.027 | 0.102\n",
      "2023-12-30 17:27:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.033 | 0.126\n",
      "2023-12-30 17:27:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.043 | 0.100\n",
      "2023-12-30 17:27:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.028 | 0.092\n",
      "2023-12-30 17:27:16 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.030 | 0.093\n",
      "2023-12-30 17:27:16 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.028 | 0.091\n",
      "2023-12-30 17:27:16 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.043 | 0.097\n",
      "2023-12-30 17:27:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.029 | 0.091\n",
      "2023-12-30 17:27:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.052 | 0.099\n",
      "2023-12-30 17:27:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.028 | 0.098\n",
      "2023-12-30 17:27:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.045 | 0.091\n",
      "2023-12-30 17:27:18 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.039 | 0.092\n",
      "2023-12-30 17:27:18 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.024 | 0.086\n",
      "2023-12-30 17:27:18 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.043 | 0.086\n",
      "2023-12-30 17:27:19 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.021 | 0.085\n",
      "2023-12-30 17:27:19 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.042 | 0.091\n",
      "2023-12-30 17:27:19 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.034 | 0.098\n",
      "2023-12-30 17:27:19 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.032 | 0.097\n",
      "2023-12-30 17:27:20 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.056 | 0.090\n",
      "2023-12-30 17:27:20 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.043 | 0.086\n",
      "2023-12-30 17:27:20 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.023 | 0.089\n",
      "2023-12-30 17:27:21 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.022 | 0.090\n",
      "2023-12-30 17:27:21 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.036 | 0.089\n",
      "2023-12-30 17:27:21 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.028 | 0.098\n",
      "2023-12-30 17:27:21 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.067 | 0.092\n",
      "2023-12-30 17:27:22 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.048 | 0.085\n",
      "2023-12-30 17:27:22 - INFO     | Early stopping: loss decreased (0.093 -> 0.086; -7.5%). Caching model state.\n",
      " 60%|██████    | 12/20 [01:47<01:12,  9.02s/it]2023-12-30 17:27:22 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 17:27:22 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.002 | 0.086\n",
      "2023-12-30 17:27:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.020 | 0.098\n",
      "2023-12-30 17:27:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.018 | 0.096\n",
      "2023-12-30 17:27:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.030 | 0.092\n",
      "2023-12-30 17:27:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.013 | 0.092\n",
      "2023-12-30 17:27:24 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.022 | 0.088\n",
      "2023-12-30 17:27:24 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.029 | 0.087\n",
      "2023-12-30 17:27:24 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.034 | 0.092\n",
      "2023-12-30 17:27:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.016 | 0.094\n",
      "2023-12-30 17:27:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.031 | 0.089\n",
      "2023-12-30 17:27:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.029 | 0.090\n",
      "2023-12-30 17:27:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.023 | 0.086\n",
      "2023-12-30 17:27:26 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.026 | 0.091\n",
      "2023-12-30 17:27:26 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.020 | 0.094\n",
      "2023-12-30 17:27:26 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.031 | 0.103\n",
      "2023-12-30 17:27:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.032 | 0.098\n",
      "2023-12-30 17:27:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.037 | 0.098\n",
      "2023-12-30 17:27:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.022 | 0.099\n",
      "2023-12-30 17:27:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.026 | 0.102\n",
      "2023-12-30 17:27:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.024 | 0.097\n",
      "2023-12-30 17:27:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.028 | 0.110\n",
      "2023-12-30 17:27:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.018 | 0.095\n",
      "2023-12-30 17:27:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.029 | 0.096\n",
      "2023-12-30 17:27:29 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.035 | 0.095\n",
      "2023-12-30 17:27:29 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.022 | 0.096\n",
      "2023-12-30 17:27:29 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.028 | 0.094\n",
      "2023-12-30 17:27:30 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.020 | 0.094\n",
      "2023-12-30 17:27:30 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.036 | 0.102\n",
      "2023-12-30 17:27:30 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.038 | 0.089\n",
      "2023-12-30 17:27:30 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.017 | 0.097\n",
      "2023-12-30 17:27:31 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.022 | 0.102\n",
      "2023-12-30 17:27:31 - INFO     | Early stopping: no decrease (0.086 vs 0.101); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [01:56<01:02,  8.96s/it]2023-12-30 17:27:31 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 17:27:31 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.001 | 0.101\n",
      "2023-12-30 17:27:31 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.025 | 0.100\n",
      "2023-12-30 17:27:32 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.025 | 0.093\n",
      "2023-12-30 17:27:32 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.012 | 0.091\n",
      "2023-12-30 17:27:32 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.015 | 0.094\n",
      "2023-12-30 17:27:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.010 | 0.101\n",
      "2023-12-30 17:27:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.018 | 0.102\n",
      "2023-12-30 17:27:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.023 | 0.103\n",
      "2023-12-30 17:27:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.014 | 0.101\n",
      "2023-12-30 17:27:34 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.016 | 0.106\n",
      "2023-12-30 17:27:34 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.024 | 0.113\n",
      "2023-12-30 17:27:34 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.029 | 0.106\n",
      "2023-12-30 17:27:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.031 | 0.105\n",
      "2023-12-30 17:27:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.018 | 0.104\n",
      "2023-12-30 17:27:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.024 | 0.108\n",
      "2023-12-30 17:27:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.018 | 0.107\n",
      "2023-12-30 17:27:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.026 | 0.101\n",
      "2023-12-30 17:27:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.013 | 0.103\n",
      "2023-12-30 17:27:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.016 | 0.102\n",
      "2023-12-30 17:27:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.022 | 0.107\n",
      "2023-12-30 17:27:37 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.024 | 0.105\n",
      "2023-12-30 17:27:37 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.007 | 0.100\n",
      "2023-12-30 17:27:37 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.018 | 0.102\n",
      "2023-12-30 17:27:38 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.029 | 0.113\n",
      "2023-12-30 17:27:38 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.031 | 0.107\n",
      "2023-12-30 17:27:38 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.023 | 0.121\n",
      "2023-12-30 17:27:39 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.041 | 0.113\n",
      "2023-12-30 17:27:39 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.037 | 0.124\n",
      "2023-12-30 17:27:39 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.029 | 0.103\n",
      "2023-12-30 17:27:39 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.032 | 0.120\n",
      "2023-12-30 17:27:40 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.033 | 0.110\n",
      "2023-12-30 17:27:40 - INFO     | Early stopping: no decrease (0.086 vs 0.111); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:05<00:53,  8.99s/it]2023-12-30 17:27:40 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 17:27:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.001 | 0.110\n",
      "2023-12-30 17:27:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.015 | 0.117\n",
      "2023-12-30 17:27:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.026 | 0.108\n",
      "2023-12-30 17:27:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.025 | 0.111\n",
      "2023-12-30 17:27:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.014 | 0.103\n",
      "2023-12-30 17:27:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.013 | 0.108\n",
      "2023-12-30 17:27:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.015 | 0.105\n",
      "2023-12-30 17:27:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.041 | 0.111\n",
      "2023-12-30 17:27:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.009 | 0.097\n",
      "2023-12-30 17:27:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.015 | 0.101\n",
      "2023-12-30 17:27:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.020 | 0.118\n",
      "2023-12-30 17:27:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.025 | 0.115\n",
      "2023-12-30 17:27:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.019 | 0.112\n",
      "2023-12-30 17:27:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.018 | 0.113\n",
      "2023-12-30 17:27:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.017 | 0.120\n",
      "2023-12-30 17:27:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.023 | 0.124\n",
      "2023-12-30 17:27:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.033 | 0.106\n",
      "2023-12-30 17:27:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.025 | 0.121\n",
      "2023-12-30 17:27:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.035 | 0.111\n",
      "2023-12-30 17:27:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.055 | 0.098\n",
      "2023-12-30 17:27:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.038 | 0.120\n",
      "2023-12-30 17:27:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.029 | 0.110\n",
      "2023-12-30 17:27:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.027 | 0.107\n",
      "2023-12-30 17:27:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.025 | 0.134\n",
      "2023-12-30 17:27:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.023 | 0.123\n",
      "2023-12-30 17:27:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.026 | 0.104\n",
      "2023-12-30 17:27:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.021 | 0.113\n",
      "2023-12-30 17:27:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.021 | 0.113\n",
      "2023-12-30 17:27:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.015 | 0.132\n",
      "2023-12-30 17:27:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.040 | 0.121\n",
      "2023-12-30 17:27:49 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.049 | 0.119\n",
      "2023-12-30 17:27:49 - INFO     | Early stopping: no decrease (0.086 vs 0.116); counter: 3 out of 3\n",
      "2023-12-30 17:27:49 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:27:49 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 75%|███████▌  | 15/20 [02:14<00:44,  8.96s/it]2023-12-30 17:27:49 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 17:27:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.011 | 0.115\n",
      "2023-12-30 17:27:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.027 | 0.126\n",
      "2023-12-30 17:27:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.023 | 0.118\n",
      "2023-12-30 17:27:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.009 | 0.115\n",
      "2023-12-30 17:27:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.015 | 0.116\n",
      "2023-12-30 17:27:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.015 | 0.107\n",
      "2023-12-30 17:27:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.007 | 0.109\n",
      "2023-12-30 17:27:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.008 | 0.108\n",
      "2023-12-30 17:27:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.015 | 0.102\n",
      "2023-12-30 17:27:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.009 | 0.105\n",
      "2023-12-30 17:27:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.028 | 0.110\n",
      "2023-12-30 17:27:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.021 | 0.105\n",
      "2023-12-30 17:27:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.013 | 0.108\n",
      "2023-12-30 17:27:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.017 | 0.105\n",
      "2023-12-30 17:27:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.012 | 0.107\n",
      "2023-12-30 17:27:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.008 | 0.106\n",
      "2023-12-30 17:27:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.009 | 0.108\n",
      "2023-12-30 17:27:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.009 | 0.115\n",
      "2023-12-30 17:27:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.015 | 0.113\n",
      "2023-12-30 17:27:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.013 | 0.114\n",
      "2023-12-30 17:27:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.007 | 0.113\n",
      "2023-12-30 17:27:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.015 | 0.108\n",
      "2023-12-30 17:27:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.010 | 0.106\n",
      "2023-12-30 17:27:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.006 | 0.105\n",
      "2023-12-30 17:27:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.004 | 0.107\n",
      "2023-12-30 17:27:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.010 | 0.102\n",
      "2023-12-30 17:27:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.018 | 0.110\n",
      "2023-12-30 17:27:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.017 | 0.113\n",
      "2023-12-30 17:27:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.017 | 0.112\n",
      "2023-12-30 17:27:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.008 | 0.109\n",
      "2023-12-30 17:27:58 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.016 | 0.106\n",
      "2023-12-30 17:27:58 - INFO     | Early stopping: no decrease (0.086 vs 0.104); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:23<00:35,  8.97s/it]2023-12-30 17:27:58 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 17:27:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.004 | 0.104\n",
      "2023-12-30 17:27:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.006 | 0.103\n",
      "2023-12-30 17:27:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.007 | 0.106\n",
      "2023-12-30 17:27:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.007 | 0.110\n",
      "2023-12-30 17:27:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.008 | 0.109\n",
      "2023-12-30 17:27:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.004 | 0.107\n",
      "2023-12-30 17:28:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.005 | 0.107\n",
      "2023-12-30 17:28:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.005 | 0.108\n",
      "2023-12-30 17:28:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.014 | 0.107\n",
      "2023-12-30 17:28:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.008 | 0.108\n",
      "2023-12-30 17:28:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.008 | 0.108\n",
      "2023-12-30 17:28:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.009 | 0.112\n",
      "2023-12-30 17:28:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.004 | 0.114\n",
      "2023-12-30 17:28:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.005 | 0.113\n",
      "2023-12-30 17:28:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.004 | 0.105\n",
      "2023-12-30 17:28:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.008 | 0.106\n",
      "2023-12-30 17:28:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.009 | 0.112\n",
      "2023-12-30 17:28:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.004 | 0.110\n",
      "2023-12-30 17:28:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.005 | 0.109\n",
      "2023-12-30 17:28:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.005 | 0.107\n",
      "2023-12-30 17:28:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.007 | 0.110\n",
      "2023-12-30 17:28:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.005 | 0.115\n",
      "2023-12-30 17:28:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.012 | 0.107\n",
      "2023-12-30 17:28:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.007 | 0.113\n",
      "2023-12-30 17:28:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.006 | 0.111\n",
      "2023-12-30 17:28:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.003 | 0.111\n",
      "2023-12-30 17:28:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.005 | 0.108\n",
      "2023-12-30 17:28:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.022 | 0.116\n",
      "2023-12-30 17:28:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.007 | 0.112\n",
      "2023-12-30 17:28:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.003 | 0.115\n",
      "2023-12-30 17:28:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.008 | 0.114\n",
      "2023-12-30 17:28:07 - INFO     | Early stopping: no decrease (0.086 vs 0.116); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:32<00:27,  9.03s/it]2023-12-30 17:28:07 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 17:28:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.001 | 0.116\n",
      "2023-12-30 17:28:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.005 | 0.115\n",
      "2023-12-30 17:28:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.004 | 0.110\n",
      "2023-12-30 17:28:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.004 | 0.114\n",
      "2023-12-30 17:28:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.014 | 0.120\n",
      "2023-12-30 17:28:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.003 | 0.115\n",
      "2023-12-30 17:28:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.003 | 0.109\n",
      "2023-12-30 17:28:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.003 | 0.115\n",
      "2023-12-30 17:28:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.003 | 0.118\n",
      "2023-12-30 17:28:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.003 | 0.118\n",
      "2023-12-30 17:28:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.002 | 0.116\n",
      "2023-12-30 17:28:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.009 | 0.114\n",
      "2023-12-30 17:28:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.002 | 0.114\n",
      "2023-12-30 17:28:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.011 | 0.113\n",
      "2023-12-30 17:28:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.003 | 0.113\n",
      "2023-12-30 17:28:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.007 | 0.114\n",
      "2023-12-30 17:28:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.016 | 0.129\n",
      "2023-12-30 17:28:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.003 | 0.125\n",
      "2023-12-30 17:28:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.022 | 0.126\n",
      "2023-12-30 17:28:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.005 | 0.127\n",
      "2023-12-30 17:28:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.012 | 0.119\n",
      "2023-12-30 17:28:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.012 | 0.115\n",
      "2023-12-30 17:28:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.013 | 0.123\n",
      "2023-12-30 17:28:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.012 | 0.116\n",
      "2023-12-30 17:28:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.007 | 0.115\n",
      "2023-12-30 17:28:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.003 | 0.114\n",
      "2023-12-30 17:28:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.006 | 0.113\n",
      "2023-12-30 17:28:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.010 | 0.113\n",
      "2023-12-30 17:28:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.008 | 0.123\n",
      "2023-12-30 17:28:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.003 | 0.129\n",
      "2023-12-30 17:28:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.005 | 0.120\n",
      "2023-12-30 17:28:16 - INFO     | Early stopping: no decrease (0.086 vs 0.122); counter: 3 out of 3\n",
      "2023-12-30 17:28:16 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:28:16 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      " 90%|█████████ | 18/20 [02:41<00:18,  9.05s/it]2023-12-30 17:28:16 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 17:28:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.018 | 0.121\n",
      "2023-12-30 17:28:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.003 | 0.119\n",
      "2023-12-30 17:28:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.001 | 0.115\n",
      "2023-12-30 17:28:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.002 | 0.114\n",
      "2023-12-30 17:28:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.004 | 0.118\n",
      "2023-12-30 17:28:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.001 | 0.120\n",
      "2023-12-30 17:28:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.001 | 0.120\n",
      "2023-12-30 17:28:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.003 | 0.113\n",
      "2023-12-30 17:28:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.004 | 0.118\n",
      "2023-12-30 17:28:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.003 | 0.121\n",
      "2023-12-30 17:28:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.003 | 0.122\n",
      "2023-12-30 17:28:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.001 | 0.121\n",
      "2023-12-30 17:28:20 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.003 | 0.124\n",
      "2023-12-30 17:28:20 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.003 | 0.122\n",
      "2023-12-30 17:28:20 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.002 | 0.123\n",
      "2023-12-30 17:28:21 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.003 | 0.130\n",
      "2023-12-30 17:28:21 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.001 | 0.133\n",
      "2023-12-30 17:28:21 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.006 | 0.132\n",
      "2023-12-30 17:28:21 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.006 | 0.124\n",
      "2023-12-30 17:28:22 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.002 | 0.125\n",
      "2023-12-30 17:28:22 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.005 | 0.121\n",
      "2023-12-30 17:28:22 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.004 | 0.122\n",
      "2023-12-30 17:28:23 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.002 | 0.123\n",
      "2023-12-30 17:28:23 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.005 | 0.125\n",
      "2023-12-30 17:28:23 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.012 | 0.124\n",
      "2023-12-30 17:28:23 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.017 | 0.121\n",
      "2023-12-30 17:28:24 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.003 | 0.121\n",
      "2023-12-30 17:28:24 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.003 | 0.124\n",
      "2023-12-30 17:28:24 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.002 | 0.129\n",
      "2023-12-30 17:28:24 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.002 | 0.128\n",
      "2023-12-30 17:28:25 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:25 - INFO     | Early stopping: no decrease (0.086 vs 0.126); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [02:50<00:09,  9.02s/it]2023-12-30 17:28:25 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 17:28:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.001 | 0.124\n",
      "2023-12-30 17:28:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.002 | 0.125\n",
      "2023-12-30 17:28:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.001 | 0.125\n",
      "2023-12-30 17:28:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.001 | 0.124\n",
      "2023-12-30 17:28:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.001 | 0.124\n",
      "2023-12-30 17:28:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.125\n",
      "2023-12-30 17:28:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.001 | 0.125\n",
      "2023-12-30 17:28:29 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.001 | 0.125\n",
      "2023-12-30 17:28:29 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.001 | 0.123\n",
      "2023-12-30 17:28:29 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.008 | 0.124\n",
      "2023-12-30 17:28:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.012 | 0.128\n",
      "2023-12-30 17:28:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:31 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.001 | 0.126\n",
      "2023-12-30 17:28:31 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.002 | 0.128\n",
      "2023-12-30 17:28:31 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.001 | 0.130\n",
      "2023-12-30 17:28:32 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.001 | 0.130\n",
      "2023-12-30 17:28:32 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.001 | 0.132\n",
      "2023-12-30 17:28:32 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.001 | 0.130\n",
      "2023-12-30 17:28:32 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.002 | 0.129\n",
      "2023-12-30 17:28:33 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.002 | 0.126\n",
      "2023-12-30 17:28:33 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.001 | 0.128\n",
      "2023-12-30 17:28:33 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.003 | 0.123\n",
      "2023-12-30 17:28:33 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.002 | 0.129\n",
      "2023-12-30 17:28:34 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.002 | 0.130\n",
      "2023-12-30 17:28:34 - INFO     | Early stopping: no decrease (0.086 vs 0.129); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [02:59<00:00,  8.99s/it]\n",
      "2023-12-30 17:28:34 - INFO     | Best validation loss: 0.086\n",
      "2023-12-30 17:28:34 - INFO     | Best early stopping index/epoch: 11\n",
      "2023-12-30 17:28:34 - INFO     | Average Loss on test set: 0.105\n",
      "2023-12-30 17:28:36 - INFO     | Weighted Precision: 0.982, Recall: 0.982, F1: 0.982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▂▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▃▄▅▄▄▄▄▅▄▄▃▃▂▃▃▂▂▂▂▃▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▃▄▄▂▂▂▆▄▃▂▂▁▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_validation_loss</td><td>0.0857</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00031</td></tr><tr><td>step_learning_rate</td><td>0.00031</td></tr><tr><td>step_training_loss</td><td>0.00221</td></tr><tr><td>step_validation_loss</td><td>0.12954</td></tr><tr><td>test_loss</td><td>0.10521</td></tr><tr><td>weighted_f1</td><td>0.982</td></tr><tr><td>weighted_precision</td><td>0.98202</td></tr><tr><td>weighted_recall</td><td>0.982</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-sweep-3</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/k8t7kfsi' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/k8t7kfsi</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_172534-k8t7kfsi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: smh6vyzo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [8, 16]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_172847-smh6vyzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/smh6vyzo' target=\"_blank\">dark-sweep-4</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/smh6vyzo' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/smh6vyzo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [8, 16], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:28:47 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 17:28:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 19.901 | 97.834\n",
      "2023-12-30 17:28:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 5.661 | 2.306\n",
      "2023-12-30 17:28:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 2.290 | 2.265\n",
      "2023-12-30 17:28:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 2.236 | 2.192\n",
      "2023-12-30 17:28:49 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 2.141 | 2.065\n",
      "2023-12-30 17:28:49 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 1.943 | 1.775\n",
      "2023-12-30 17:28:49 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 1.314 | 0.936\n",
      "2023-12-30 17:28:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.742 | 0.595\n",
      "2023-12-30 17:28:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.619 | 0.495\n",
      "2023-12-30 17:28:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.497 | 0.428\n",
      "2023-12-30 17:28:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.449 | 0.386\n",
      "2023-12-30 17:28:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.388 | 0.353\n",
      "2023-12-30 17:28:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.357 | 0.320\n",
      "2023-12-30 17:28:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.368 | 0.326\n",
      "2023-12-30 17:28:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.339 | 0.318\n",
      "2023-12-30 17:28:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.299 | 0.294\n",
      "2023-12-30 17:28:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.321 | 0.295\n",
      "2023-12-30 17:28:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.273 | 0.275\n",
      "2023-12-30 17:28:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.271 | 0.266\n",
      "2023-12-30 17:28:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.250 | 0.260\n",
      "2023-12-30 17:28:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.268 | 0.240\n",
      "2023-12-30 17:28:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.259 | 0.255\n",
      "2023-12-30 17:28:54 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.252 | 0.246\n",
      "2023-12-30 17:28:54 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.253 | 0.260\n",
      "2023-12-30 17:28:54 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.274 | 0.233\n",
      "2023-12-30 17:28:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.234 | 0.231\n",
      "2023-12-30 17:28:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.259 | 0.222\n",
      "2023-12-30 17:28:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.217 | 0.226\n",
      "2023-12-30 17:28:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.225 | 0.212\n",
      "2023-12-30 17:28:56 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.217 | 0.198\n",
      "2023-12-30 17:28:56 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.185 | 0.207\n",
      "2023-12-30 17:28:56 - INFO     | Early stopping: loss decreased (inf -> 0.211; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:08<02:49,  8.90s/it]2023-12-30 17:28:56 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 17:28:57 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.233 | 0.200\n",
      "2023-12-30 17:28:57 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.209 | 0.198\n",
      "2023-12-30 17:28:57 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.180 | 0.196\n",
      "2023-12-30 17:28:57 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.199 | 0.188\n",
      "2023-12-30 17:28:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.198 | 0.174\n",
      "2023-12-30 17:28:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.193 | 0.188\n",
      "2023-12-30 17:28:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.190 | 0.181\n",
      "2023-12-30 17:28:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.164 | 0.178\n",
      "2023-12-30 17:28:59 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.167 | 0.190\n",
      "2023-12-30 17:28:59 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.163 | 0.171\n",
      "2023-12-30 17:28:59 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.160 | 0.176\n",
      "2023-12-30 17:29:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.159 | 0.180\n",
      "2023-12-30 17:29:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.174 | 0.172\n",
      "2023-12-30 17:29:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.173 | 0.165\n",
      "2023-12-30 17:29:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.159 | 0.166\n",
      "2023-12-30 17:29:01 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.165 | 0.153\n",
      "2023-12-30 17:29:01 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.175 | 0.163\n",
      "2023-12-30 17:29:01 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.191 | 0.186\n",
      "2023-12-30 17:29:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.156 | 0.151\n",
      "2023-12-30 17:29:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.170 | 0.158\n",
      "2023-12-30 17:29:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.173 | 0.156\n",
      "2023-12-30 17:29:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.144 | 0.152\n",
      "2023-12-30 17:29:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.166 | 0.146\n",
      "2023-12-30 17:29:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.140 | 0.164\n",
      "2023-12-30 17:29:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.156 | 0.132\n",
      "2023-12-30 17:29:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.160 | 0.146\n",
      "2023-12-30 17:29:04 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.160 | 0.146\n",
      "2023-12-30 17:29:04 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.150 | 0.140\n",
      "2023-12-30 17:29:04 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.141 | 0.143\n",
      "2023-12-30 17:29:05 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.111 | 0.137\n",
      "2023-12-30 17:29:05 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.142 | 0.160\n",
      "2023-12-30 17:29:05 - INFO     | Early stopping: loss decreased (0.211 -> 0.150; -29.1%). Caching model state.\n",
      " 10%|█         | 2/20 [00:17<02:40,  8.93s/it]2023-12-30 17:29:05 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 17:29:05 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.172 | 0.156\n",
      "2023-12-30 17:29:06 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.118 | 0.133\n",
      "2023-12-30 17:29:06 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.099 | 0.143\n",
      "2023-12-30 17:29:06 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.138 | 0.146\n",
      "2023-12-30 17:29:07 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.120 | 0.159\n",
      "2023-12-30 17:29:07 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.122 | 0.135\n",
      "2023-12-30 17:29:07 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.133 | 0.131\n",
      "2023-12-30 17:29:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.109 | 0.155\n",
      "2023-12-30 17:29:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.115 | 0.134\n",
      "2023-12-30 17:29:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.110 | 0.135\n",
      "2023-12-30 17:29:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.136 | 0.126\n",
      "2023-12-30 17:29:09 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.133 | 0.130\n",
      "2023-12-30 17:29:09 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.122 | 0.131\n",
      "2023-12-30 17:29:09 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.118 | 0.132\n",
      "2023-12-30 17:29:10 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.118 | 0.123\n",
      "2023-12-30 17:29:10 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.106 | 0.124\n",
      "2023-12-30 17:29:10 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.139 | 0.128\n",
      "2023-12-30 17:29:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.150 | 0.126\n",
      "2023-12-30 17:29:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.134 | 0.129\n",
      "2023-12-30 17:29:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.097 | 0.119\n",
      "2023-12-30 17:29:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.132 | 0.115\n",
      "2023-12-30 17:29:12 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.129 | 0.124\n",
      "2023-12-30 17:29:12 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.110 | 0.123\n",
      "2023-12-30 17:29:12 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.111 | 0.131\n",
      "2023-12-30 17:29:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.113 | 0.124\n",
      "2023-12-30 17:29:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.159 | 0.128\n",
      "2023-12-30 17:29:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.123 | 0.115\n",
      "2023-12-30 17:29:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.097 | 0.111\n",
      "2023-12-30 17:29:14 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.129 | 0.118\n",
      "2023-12-30 17:29:14 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.121 | 0.106\n",
      "2023-12-30 17:29:14 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.123 | 0.113\n",
      "2023-12-30 17:29:14 - INFO     | Early stopping: loss decreased (0.150 -> 0.115; -23.0%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:27<02:34,  9.08s/it]2023-12-30 17:29:14 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 17:29:15 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.210 | 0.108\n",
      "2023-12-30 17:29:15 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.085 | 0.118\n",
      "2023-12-30 17:29:15 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.100 | 0.121\n",
      "2023-12-30 17:29:16 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.073 | 0.110\n",
      "2023-12-30 17:29:16 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.103 | 0.112\n",
      "2023-12-30 17:29:16 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.100 | 0.134\n",
      "2023-12-30 17:29:16 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.124 | 0.112\n",
      "2023-12-30 17:29:17 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.087 | 0.106\n",
      "2023-12-30 17:29:17 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.113 | 0.104\n",
      "2023-12-30 17:29:17 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.088 | 0.131\n",
      "2023-12-30 17:29:17 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.089 | 0.120\n",
      "2023-12-30 17:29:18 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.102 | 0.112\n",
      "2023-12-30 17:29:18 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.107 | 0.113\n",
      "2023-12-30 17:29:18 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.123 | 0.107\n",
      "2023-12-30 17:29:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.081 | 0.104\n",
      "2023-12-30 17:29:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.115 | 0.106\n",
      "2023-12-30 17:29:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.109 | 0.101\n",
      "2023-12-30 17:29:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.093 | 0.108\n",
      "2023-12-30 17:29:20 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.097 | 0.103\n",
      "2023-12-30 17:29:20 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.103 | 0.109\n",
      "2023-12-30 17:29:20 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.085 | 0.109\n",
      "2023-12-30 17:29:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.113 | 0.104\n",
      "2023-12-30 17:29:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.096 | 0.101\n",
      "2023-12-30 17:29:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.103 | 0.107\n",
      "2023-12-30 17:29:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.096 | 0.099\n",
      "2023-12-30 17:29:22 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.091 | 0.106\n",
      "2023-12-30 17:29:22 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.104 | 0.096\n",
      "2023-12-30 17:29:22 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.109 | 0.102\n",
      "2023-12-30 17:29:22 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.083 | 0.103\n",
      "2023-12-30 17:29:23 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.116 | 0.102\n",
      "2023-12-30 17:29:23 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.088 | 0.097\n",
      "2023-12-30 17:29:23 - INFO     | Early stopping: no decrease (0.115 vs 0.129); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:35<02:23,  8.97s/it]2023-12-30 17:29:23 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 17:29:24 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.064 | 0.124\n",
      "2023-12-30 17:29:24 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.089 | 0.106\n",
      "2023-12-30 17:29:24 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.094 | 0.098\n",
      "2023-12-30 17:29:24 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.075 | 0.092\n",
      "2023-12-30 17:29:25 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.078 | 0.099\n",
      "2023-12-30 17:29:25 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.099 | 0.099\n",
      "2023-12-30 17:29:25 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.075 | 0.096\n",
      "2023-12-30 17:29:26 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.070 | 0.099\n",
      "2023-12-30 17:29:26 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.090 | 0.101\n",
      "2023-12-30 17:29:26 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.090 | 0.098\n",
      "2023-12-30 17:29:27 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.096 | 0.103\n",
      "2023-12-30 17:29:27 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.085 | 0.097\n",
      "2023-12-30 17:29:27 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.081 | 0.096\n",
      "2023-12-30 17:29:27 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.072 | 0.098\n",
      "2023-12-30 17:29:28 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.072 | 0.112\n",
      "2023-12-30 17:29:28 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.082 | 0.102\n",
      "2023-12-30 17:29:28 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.069 | 0.097\n",
      "2023-12-30 17:29:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.096 | 0.104\n",
      "2023-12-30 17:29:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.115 | 0.097\n",
      "2023-12-30 17:29:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.075 | 0.097\n",
      "2023-12-30 17:29:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.084 | 0.107\n",
      "2023-12-30 17:29:30 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.104 | 0.101\n",
      "2023-12-30 17:29:30 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.082 | 0.092\n",
      "2023-12-30 17:29:30 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.077 | 0.098\n",
      "2023-12-30 17:29:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.100 | 0.095\n",
      "2023-12-30 17:29:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.100 | 0.095\n",
      "2023-12-30 17:29:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.090 | 0.095\n",
      "2023-12-30 17:29:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.099 | 0.091\n",
      "2023-12-30 17:29:32 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.084 | 0.092\n",
      "2023-12-30 17:29:32 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.092 | 0.093\n",
      "2023-12-30 17:29:32 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.076 | 0.106\n",
      "2023-12-30 17:29:33 - INFO     | Early stopping: loss decreased (0.115 -> 0.092; -20.3%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:45<02:16,  9.10s/it]2023-12-30 17:29:33 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 17:29:33 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.188 | 0.095\n",
      "2023-12-30 17:29:33 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.057 | 0.090\n",
      "2023-12-30 17:29:33 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.071 | 0.093\n",
      "2023-12-30 17:29:34 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.081 | 0.093\n",
      "2023-12-30 17:29:34 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.063 | 0.093\n",
      "2023-12-30 17:29:34 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.083 | 0.093\n",
      "2023-12-30 17:29:35 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.062 | 0.106\n",
      "2023-12-30 17:29:35 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.071 | 0.101\n",
      "2023-12-30 17:29:35 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.064 | 0.094\n",
      "2023-12-30 17:29:35 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.060 | 0.104\n",
      "2023-12-30 17:29:36 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.082 | 0.102\n",
      "2023-12-30 17:29:36 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.082 | 0.089\n",
      "2023-12-30 17:29:36 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.079 | 0.096\n",
      "2023-12-30 17:29:37 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.054 | 0.093\n",
      "2023-12-30 17:29:37 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.088 | 0.089\n",
      "2023-12-30 17:29:37 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.088 | 0.093\n",
      "2023-12-30 17:29:37 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.081 | 0.085\n",
      "2023-12-30 17:29:38 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.065 | 0.100\n",
      "2023-12-30 17:29:38 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.083 | 0.091\n",
      "2023-12-30 17:29:38 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.073 | 0.086\n",
      "2023-12-30 17:29:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.063 | 0.091\n",
      "2023-12-30 17:29:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.068 | 0.093\n",
      "2023-12-30 17:29:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.100 | 0.095\n",
      "2023-12-30 17:29:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.070 | 0.088\n",
      "2023-12-30 17:29:40 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.092 | 0.089\n",
      "2023-12-30 17:29:40 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.077 | 0.095\n",
      "2023-12-30 17:29:40 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.101 | 0.089\n",
      "2023-12-30 17:29:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.090 | 0.083\n",
      "2023-12-30 17:29:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.071 | 0.083\n",
      "2023-12-30 17:29:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.078 | 0.087\n",
      "2023-12-30 17:29:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.071 | 0.090\n",
      "2023-12-30 17:29:42 - INFO     | Early stopping: loss decreased (0.092 -> 0.087; -5.2%). Caching model state.\n",
      " 30%|███       | 6/20 [00:54<02:07,  9.09s/it]2023-12-30 17:29:42 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 17:29:42 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.024 | 0.087\n",
      "2023-12-30 17:29:42 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.078 | 0.093\n",
      "2023-12-30 17:29:42 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.059 | 0.086\n",
      "2023-12-30 17:29:43 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.052 | 0.085\n",
      "2023-12-30 17:29:43 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.052 | 0.088\n",
      "2023-12-30 17:29:43 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.056 | 0.086\n",
      "2023-12-30 17:29:44 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.062 | 0.091\n",
      "2023-12-30 17:29:44 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.069 | 0.091\n",
      "2023-12-30 17:29:44 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.050 | 0.090\n",
      "2023-12-30 17:29:45 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.056 | 0.101\n",
      "2023-12-30 17:29:45 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.065 | 0.091\n",
      "2023-12-30 17:29:45 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.077 | 0.088\n",
      "2023-12-30 17:29:45 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.079 | 0.081\n",
      "2023-12-30 17:29:46 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.077 | 0.090\n",
      "2023-12-30 17:29:46 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.067 | 0.084\n",
      "2023-12-30 17:29:46 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.078 | 0.085\n",
      "2023-12-30 17:29:47 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.053 | 0.084\n",
      "2023-12-30 17:29:47 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.079 | 0.080\n",
      "2023-12-30 17:29:47 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.055 | 0.090\n",
      "2023-12-30 17:29:47 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.085 | 0.083\n",
      "2023-12-30 17:29:48 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.065 | 0.082\n",
      "2023-12-30 17:29:48 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.075 | 0.096\n",
      "2023-12-30 17:29:48 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.087 | 0.086\n",
      "2023-12-30 17:29:48 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.070 | 0.079\n",
      "2023-12-30 17:29:49 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.069 | 0.084\n",
      "2023-12-30 17:29:49 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.069 | 0.084\n",
      "2023-12-30 17:29:49 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.054 | 0.083\n",
      "2023-12-30 17:29:49 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.069 | 0.081\n",
      "2023-12-30 17:29:50 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.068 | 0.080\n",
      "2023-12-30 17:29:50 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.053 | 0.083\n",
      "2023-12-30 17:29:50 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.068 | 0.098\n",
      "2023-12-30 17:29:51 - INFO     | Early stopping: no decrease (0.087 vs 0.086); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:03<01:57,  9.04s/it]2023-12-30 17:29:51 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 17:29:51 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.046 | 0.089\n",
      "2023-12-30 17:29:51 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.059 | 0.085\n",
      "2023-12-30 17:29:51 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.060 | 0.086\n",
      "2023-12-30 17:29:52 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.060 | 0.085\n",
      "2023-12-30 17:29:52 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.053 | 0.083\n",
      "2023-12-30 17:29:52 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.044 | 0.082\n",
      "2023-12-30 17:29:52 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.063 | 0.081\n",
      "2023-12-30 17:29:53 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.063 | 0.088\n",
      "2023-12-30 17:29:53 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.059 | 0.084\n",
      "2023-12-30 17:29:53 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.059 | 0.086\n",
      "2023-12-30 17:29:54 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.069 | 0.084\n",
      "2023-12-30 17:29:54 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.060 | 0.085\n",
      "2023-12-30 17:29:54 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.061 | 0.085\n",
      "2023-12-30 17:29:54 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.052 | 0.082\n",
      "2023-12-30 17:29:55 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.064 | 0.087\n",
      "2023-12-30 17:29:55 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.095 | 0.083\n",
      "2023-12-30 17:29:55 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.071 | 0.084\n",
      "2023-12-30 17:29:56 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.060 | 0.079\n",
      "2023-12-30 17:29:56 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.057 | 0.082\n",
      "2023-12-30 17:29:56 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.061 | 0.083\n",
      "2023-12-30 17:29:56 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.053 | 0.088\n",
      "2023-12-30 17:29:57 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.075 | 0.088\n",
      "2023-12-30 17:29:57 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.068 | 0.089\n",
      "2023-12-30 17:29:57 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.055 | 0.080\n",
      "2023-12-30 17:29:58 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.082 | 0.080\n",
      "2023-12-30 17:29:58 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.056 | 0.078\n",
      "2023-12-30 17:29:58 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.065 | 0.085\n",
      "2023-12-30 17:29:58 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.064 | 0.080\n",
      "2023-12-30 17:29:59 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.058 | 0.078\n",
      "2023-12-30 17:29:59 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.061 | 0.083\n",
      "2023-12-30 17:29:59 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.050 | 0.081\n",
      "2023-12-30 17:29:59 - INFO     | Early stopping: loss decreased (0.087 -> 0.080; -8.7%). Caching model state.\n",
      " 40%|████      | 8/20 [01:12<01:47,  9.00s/it]2023-12-30 17:29:59 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 17:30:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.026 | 0.081\n",
      "2023-12-30 17:30:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.047 | 0.088\n",
      "2023-12-30 17:30:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.045 | 0.079\n",
      "2023-12-30 17:30:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.040 | 0.079\n",
      "2023-12-30 17:30:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.059 | 0.080\n",
      "2023-12-30 17:30:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.055 | 0.081\n",
      "2023-12-30 17:30:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.065 | 0.085\n",
      "2023-12-30 17:30:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.046 | 0.081\n",
      "2023-12-30 17:30:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.068 | 0.080\n",
      "2023-12-30 17:30:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.058 | 0.084\n",
      "2023-12-30 17:30:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.063 | 0.082\n",
      "2023-12-30 17:30:03 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.056 | 0.081\n",
      "2023-12-30 17:30:03 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.060 | 0.082\n",
      "2023-12-30 17:30:03 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.041 | 0.083\n",
      "2023-12-30 17:30:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.052 | 0.083\n",
      "2023-12-30 17:30:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.052 | 0.077\n",
      "2023-12-30 17:30:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.053 | 0.086\n",
      "2023-12-30 17:30:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.053 | 0.087\n",
      "2023-12-30 17:30:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.068 | 0.079\n",
      "2023-12-30 17:30:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.059 | 0.080\n",
      "2023-12-30 17:30:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.076 | 0.078\n",
      "2023-12-30 17:30:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.069 | 0.082\n",
      "2023-12-30 17:30:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.070 | 0.078\n",
      "2023-12-30 17:30:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.054 | 0.084\n",
      "2023-12-30 17:30:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.070 | 0.078\n",
      "2023-12-30 17:30:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.080 | 0.077\n",
      "2023-12-30 17:30:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.054 | 0.081\n",
      "2023-12-30 17:30:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.065 | 0.075\n",
      "2023-12-30 17:30:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.047 | 0.075\n",
      "2023-12-30 17:30:08 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.074 | 0.078\n",
      "2023-12-30 17:30:08 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.051 | 0.075\n",
      "2023-12-30 17:30:08 - INFO     | Early stopping: loss decreased (0.080 -> 0.074; -6.5%). Caching model state.\n",
      " 45%|████▌     | 9/20 [01:21<01:38,  8.96s/it]2023-12-30 17:30:08 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 17:30:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.053 | 0.075\n",
      "2023-12-30 17:30:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.043 | 0.075\n",
      "2023-12-30 17:30:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.050 | 0.077\n",
      "2023-12-30 17:30:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.055 | 0.075\n",
      "2023-12-30 17:30:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.033 | 0.075\n",
      "2023-12-30 17:30:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.061 | 0.081\n",
      "2023-12-30 17:30:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.062 | 0.086\n",
      "2023-12-30 17:30:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.055 | 0.088\n",
      "2023-12-30 17:30:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.049 | 0.080\n",
      "2023-12-30 17:30:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.046 | 0.078\n",
      "2023-12-30 17:30:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.053 | 0.083\n",
      "2023-12-30 17:30:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.060 | 0.081\n",
      "2023-12-30 17:30:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.050 | 0.089\n",
      "2023-12-30 17:30:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.057 | 0.091\n",
      "2023-12-30 17:30:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.049 | 0.081\n",
      "2023-12-30 17:30:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.051 | 0.079\n",
      "2023-12-30 17:30:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.044 | 0.077\n",
      "2023-12-30 17:30:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.055 | 0.081\n",
      "2023-12-30 17:30:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.059 | 0.083\n",
      "2023-12-30 17:30:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.051 | 0.076\n",
      "2023-12-30 17:30:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.060 | 0.080\n",
      "2023-12-30 17:30:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.044 | 0.079\n",
      "2023-12-30 17:30:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.050 | 0.081\n",
      "2023-12-30 17:30:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.058 | 0.079\n",
      "2023-12-30 17:30:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.043 | 0.086\n",
      "2023-12-30 17:30:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.059 | 0.079\n",
      "2023-12-30 17:30:16 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.055 | 0.087\n",
      "2023-12-30 17:30:16 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.080 | 0.075\n",
      "2023-12-30 17:30:16 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.061 | 0.079\n",
      "2023-12-30 17:30:17 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.051 | 0.081\n",
      "2023-12-30 17:30:17 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.068 | 0.076\n",
      "2023-12-30 17:30:17 - INFO     | Early stopping: no decrease (0.074 vs 0.086); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:29<01:29,  8.91s/it]2023-12-30 17:30:17 - INFO     | Epoch: 10 | Learning Rate: 0.005\n",
      "2023-12-30 17:30:17 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 560064 examples: 0.047 | 0.083\n",
      "2023-12-30 17:30:18 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 561920 examples: 0.044 | 0.078\n",
      "2023-12-30 17:30:18 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 563776 examples: 0.047 | 0.082\n",
      "2023-12-30 17:30:18 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 565632 examples: 0.054 | 0.088\n",
      "2023-12-30 17:30:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 567488 examples: 0.045 | 0.080\n",
      "2023-12-30 17:30:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 569344 examples: 0.043 | 0.081\n",
      "2023-12-30 17:30:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 571200 examples: 0.045 | 0.076\n",
      "2023-12-30 17:30:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 573056 examples: 0.047 | 0.077\n",
      "2023-12-30 17:30:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 574912 examples: 0.053 | 0.079\n",
      "2023-12-30 17:30:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 576768 examples: 0.054 | 0.075\n",
      "2023-12-30 17:30:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 578624 examples: 0.036 | 0.079\n",
      "2023-12-30 17:30:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 580480 examples: 0.043 | 0.080\n",
      "2023-12-30 17:30:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 582336 examples: 0.052 | 0.077\n",
      "2023-12-30 17:30:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 584192 examples: 0.058 | 0.085\n",
      "2023-12-30 17:30:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 586048 examples: 0.037 | 0.075\n",
      "2023-12-30 17:30:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 587904 examples: 0.036 | 0.087\n",
      "2023-12-30 17:30:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 589760 examples: 0.039 | 0.082\n",
      "2023-12-30 17:30:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 591616 examples: 0.047 | 0.075\n",
      "2023-12-30 17:30:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 593472 examples: 0.061 | 0.078\n",
      "2023-12-30 17:30:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 595328 examples: 0.059 | 0.092\n",
      "2023-12-30 17:30:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 597184 examples: 0.053 | 0.075\n",
      "2023-12-30 17:30:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 599040 examples: 0.065 | 0.082\n",
      "2023-12-30 17:30:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 600896 examples: 0.059 | 0.079\n",
      "2023-12-30 17:30:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 602752 examples: 0.051 | 0.074\n",
      "2023-12-30 17:30:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 604608 examples: 0.048 | 0.076\n",
      "2023-12-30 17:30:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 606464 examples: 0.047 | 0.078\n",
      "2023-12-30 17:30:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 608320 examples: 0.043 | 0.071\n",
      "2023-12-30 17:30:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 610176 examples: 0.069 | 0.077\n",
      "2023-12-30 17:30:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 612032 examples: 0.063 | 0.080\n",
      "2023-12-30 17:30:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 613888 examples: 0.067 | 0.074\n",
      "2023-12-30 17:30:26 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 615744 examples: 0.047 | 0.078\n",
      "2023-12-30 17:30:26 - INFO     | Early stopping: no decrease (0.074 vs 0.076); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:38<01:19,  8.88s/it]2023-12-30 17:30:26 - INFO     | Epoch: 11 | Learning Rate: 0.005\n",
      "2023-12-30 17:30:26 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 616064 examples: 0.008 | 0.075\n",
      "2023-12-30 17:30:27 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 617920 examples: 0.040 | 0.075\n",
      "2023-12-30 17:30:27 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 619776 examples: 0.049 | 0.084\n",
      "2023-12-30 17:30:27 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 621632 examples: 0.033 | 0.075\n",
      "2023-12-30 17:30:27 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 623488 examples: 0.039 | 0.079\n",
      "2023-12-30 17:30:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 625344 examples: 0.048 | 0.090\n",
      "2023-12-30 17:30:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 627200 examples: 0.067 | 0.083\n",
      "2023-12-30 17:30:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 629056 examples: 0.072 | 0.081\n",
      "2023-12-30 17:30:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 630912 examples: 0.053 | 0.074\n",
      "2023-12-30 17:30:29 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 632768 examples: 0.030 | 0.073\n",
      "2023-12-30 17:30:29 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 634624 examples: 0.046 | 0.078\n",
      "2023-12-30 17:30:29 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 636480 examples: 0.049 | 0.081\n",
      "2023-12-30 17:30:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 638336 examples: 0.039 | 0.077\n",
      "2023-12-30 17:30:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 640192 examples: 0.045 | 0.083\n",
      "2023-12-30 17:30:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 642048 examples: 0.060 | 0.083\n",
      "2023-12-30 17:30:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 643904 examples: 0.038 | 0.079\n",
      "2023-12-30 17:30:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 645760 examples: 0.052 | 0.078\n",
      "2023-12-30 17:30:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 647616 examples: 0.052 | 0.081\n",
      "2023-12-30 17:30:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 649472 examples: 0.044 | 0.074\n",
      "2023-12-30 17:30:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 651328 examples: 0.049 | 0.077\n",
      "2023-12-30 17:30:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 653184 examples: 0.038 | 0.077\n",
      "2023-12-30 17:30:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 655040 examples: 0.064 | 0.082\n",
      "2023-12-30 17:30:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 656896 examples: 0.049 | 0.084\n",
      "2023-12-30 17:30:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 658752 examples: 0.048 | 0.079\n",
      "2023-12-30 17:30:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 660608 examples: 0.054 | 0.073\n",
      "2023-12-30 17:30:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 662464 examples: 0.042 | 0.082\n",
      "2023-12-30 17:30:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 664320 examples: 0.040 | 0.083\n",
      "2023-12-30 17:30:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 666176 examples: 0.035 | 0.081\n",
      "2023-12-30 17:30:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 668032 examples: 0.042 | 0.079\n",
      "2023-12-30 17:30:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 669888 examples: 0.032 | 0.079\n",
      "2023-12-30 17:30:35 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 671744 examples: 0.043 | 0.077\n",
      "2023-12-30 17:30:35 - INFO     | Early stopping: no decrease (0.074 vs 0.074); counter: 3 out of 3\n",
      "2023-12-30 17:30:35 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:30:35 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 60%|██████    | 12/20 [01:47<01:11,  8.91s/it]2023-12-30 17:30:35 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 17:30:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.041 | 0.072\n",
      "2023-12-30 17:30:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.050 | 0.070\n",
      "2023-12-30 17:30:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.034 | 0.073\n",
      "2023-12-30 17:30:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.029 | 0.072\n",
      "2023-12-30 17:30:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.039 | 0.072\n",
      "2023-12-30 17:30:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.030 | 0.072\n",
      "2023-12-30 17:30:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.035 | 0.077\n",
      "2023-12-30 17:30:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.041 | 0.074\n",
      "2023-12-30 17:30:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.029 | 0.071\n",
      "2023-12-30 17:30:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.047 | 0.074\n",
      "2023-12-30 17:30:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.034 | 0.071\n",
      "2023-12-30 17:30:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.050 | 0.071\n",
      "2023-12-30 17:30:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.037 | 0.075\n",
      "2023-12-30 17:30:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.033 | 0.072\n",
      "2023-12-30 17:30:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.045 | 0.076\n",
      "2023-12-30 17:30:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.047 | 0.073\n",
      "2023-12-30 17:30:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.033 | 0.074\n",
      "2023-12-30 17:30:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.048 | 0.071\n",
      "2023-12-30 17:30:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.026 | 0.075\n",
      "2023-12-30 17:30:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.035 | 0.073\n",
      "2023-12-30 17:30:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.042 | 0.070\n",
      "2023-12-30 17:30:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.038 | 0.070\n",
      "2023-12-30 17:30:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.028 | 0.073\n",
      "2023-12-30 17:30:42 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.029 | 0.072\n",
      "2023-12-30 17:30:42 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.039 | 0.072\n",
      "2023-12-30 17:30:42 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.030 | 0.070\n",
      "2023-12-30 17:30:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.032 | 0.071\n",
      "2023-12-30 17:30:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.041 | 0.070\n",
      "2023-12-30 17:30:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.028 | 0.071\n",
      "2023-12-30 17:30:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.050 | 0.073\n",
      "2023-12-30 17:30:44 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.028 | 0.073\n",
      "2023-12-30 17:30:44 - INFO     | Early stopping: no decrease (0.074 vs 0.076); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [01:56<01:02,  8.99s/it]2023-12-30 17:30:44 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 17:30:44 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.002 | 0.076\n",
      "2023-12-30 17:30:45 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.035 | 0.072\n",
      "2023-12-30 17:30:45 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.028 | 0.073\n",
      "2023-12-30 17:30:45 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.028 | 0.076\n",
      "2023-12-30 17:30:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.041 | 0.074\n",
      "2023-12-30 17:30:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.029 | 0.072\n",
      "2023-12-30 17:30:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.029 | 0.074\n",
      "2023-12-30 17:30:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.033 | 0.076\n",
      "2023-12-30 17:30:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.038 | 0.072\n",
      "2023-12-30 17:30:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.038 | 0.072\n",
      "2023-12-30 17:30:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.033 | 0.073\n",
      "2023-12-30 17:30:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.036 | 0.072\n",
      "2023-12-30 17:30:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.031 | 0.073\n",
      "2023-12-30 17:30:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.032 | 0.071\n",
      "2023-12-30 17:30:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.040 | 0.071\n",
      "2023-12-30 17:30:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.028 | 0.071\n",
      "2023-12-30 17:30:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.031 | 0.072\n",
      "2023-12-30 17:30:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.031 | 0.074\n",
      "2023-12-30 17:30:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.028 | 0.070\n",
      "2023-12-30 17:30:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.026 | 0.069\n",
      "2023-12-30 17:30:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.023 | 0.071\n",
      "2023-12-30 17:30:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.042 | 0.070\n",
      "2023-12-30 17:30:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.049 | 0.076\n",
      "2023-12-30 17:30:51 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.031 | 0.071\n",
      "2023-12-30 17:30:51 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.038 | 0.071\n",
      "2023-12-30 17:30:51 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.042 | 0.070\n",
      "2023-12-30 17:30:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.040 | 0.071\n",
      "2023-12-30 17:30:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.043 | 0.074\n",
      "2023-12-30 17:30:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.042 | 0.071\n",
      "2023-12-30 17:30:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.053 | 0.073\n",
      "2023-12-30 17:30:53 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.030 | 0.070\n",
      "2023-12-30 17:30:53 - INFO     | Early stopping: loss decreased (0.074 -> 0.070; -5.8%). Caching model state.\n",
      " 70%|███████   | 14/20 [02:05<00:53,  8.94s/it]2023-12-30 17:30:53 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 17:30:53 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.025 | 0.070\n",
      "2023-12-30 17:30:53 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.031 | 0.070\n",
      "2023-12-30 17:30:54 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.026 | 0.072\n",
      "2023-12-30 17:30:54 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.030 | 0.074\n",
      "2023-12-30 17:30:54 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.033 | 0.071\n",
      "2023-12-30 17:30:55 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.025 | 0.071\n",
      "2023-12-30 17:30:55 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.024 | 0.074\n",
      "2023-12-30 17:30:55 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.030 | 0.072\n",
      "2023-12-30 17:30:55 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.030 | 0.072\n",
      "2023-12-30 17:30:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.035 | 0.070\n",
      "2023-12-30 17:30:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.032 | 0.070\n",
      "2023-12-30 17:30:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.036 | 0.070\n",
      "2023-12-30 17:30:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.041 | 0.073\n",
      "2023-12-30 17:30:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.038 | 0.073\n",
      "2023-12-30 17:30:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.037 | 0.071\n",
      "2023-12-30 17:30:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.050 | 0.072\n",
      "2023-12-30 17:30:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.046 | 0.076\n",
      "2023-12-30 17:30:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.037 | 0.071\n",
      "2023-12-30 17:30:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.031 | 0.071\n",
      "2023-12-30 17:30:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.038 | 0.070\n",
      "2023-12-30 17:30:59 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.042 | 0.076\n",
      "2023-12-30 17:30:59 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.027 | 0.073\n",
      "2023-12-30 17:30:59 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.032 | 0.078\n",
      "2023-12-30 17:31:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.035 | 0.071\n",
      "2023-12-30 17:31:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.033 | 0.070\n",
      "2023-12-30 17:31:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.027 | 0.073\n",
      "2023-12-30 17:31:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.031 | 0.071\n",
      "2023-12-30 17:31:01 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.023 | 0.072\n",
      "2023-12-30 17:31:01 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.035 | 0.072\n",
      "2023-12-30 17:31:01 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.049 | 0.071\n",
      "2023-12-30 17:31:02 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.027 | 0.069\n",
      "2023-12-30 17:31:02 - INFO     | Early stopping: no decrease (0.070 vs 0.069); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:14<00:44,  8.98s/it]2023-12-30 17:31:02 - INFO     | Epoch: 15 | Learning Rate: 0.003\n",
      "2023-12-30 17:31:02 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 840064 examples: 0.072 | 0.069\n",
      "2023-12-30 17:31:03 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 841920 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:03 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 843776 examples: 0.024 | 0.070\n",
      "2023-12-30 17:31:03 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 845632 examples: 0.023 | 0.070\n",
      "2023-12-30 17:31:04 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 847488 examples: 0.039 | 0.072\n",
      "2023-12-30 17:31:04 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 849344 examples: 0.028 | 0.071\n",
      "2023-12-30 17:31:04 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 851200 examples: 0.037 | 0.075\n",
      "2023-12-30 17:31:04 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 853056 examples: 0.038 | 0.072\n",
      "2023-12-30 17:31:05 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 854912 examples: 0.025 | 0.074\n",
      "2023-12-30 17:31:05 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 856768 examples: 0.028 | 0.074\n",
      "2023-12-30 17:31:05 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 858624 examples: 0.043 | 0.074\n",
      "2023-12-30 17:31:06 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 860480 examples: 0.031 | 0.074\n",
      "2023-12-30 17:31:06 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 862336 examples: 0.039 | 0.074\n",
      "2023-12-30 17:31:06 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 864192 examples: 0.026 | 0.074\n",
      "2023-12-30 17:31:06 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 866048 examples: 0.024 | 0.072\n",
      "2023-12-30 17:31:07 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 867904 examples: 0.044 | 0.072\n",
      "2023-12-30 17:31:07 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 869760 examples: 0.018 | 0.071\n",
      "2023-12-30 17:31:07 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 871616 examples: 0.034 | 0.077\n",
      "2023-12-30 17:31:08 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 873472 examples: 0.037 | 0.072\n",
      "2023-12-30 17:31:08 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 875328 examples: 0.028 | 0.072\n",
      "2023-12-30 17:31:08 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 877184 examples: 0.030 | 0.071\n",
      "2023-12-30 17:31:08 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 879040 examples: 0.033 | 0.075\n",
      "2023-12-30 17:31:09 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 880896 examples: 0.049 | 0.070\n",
      "2023-12-30 17:31:09 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 882752 examples: 0.028 | 0.071\n",
      "2023-12-30 17:31:09 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 884608 examples: 0.034 | 0.070\n",
      "2023-12-30 17:31:09 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 886464 examples: 0.039 | 0.074\n",
      "2023-12-30 17:31:10 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 888320 examples: 0.027 | 0.075\n",
      "2023-12-30 17:31:10 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 890176 examples: 0.037 | 0.077\n",
      "2023-12-30 17:31:10 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 892032 examples: 0.039 | 0.072\n",
      "2023-12-30 17:31:11 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 893888 examples: 0.032 | 0.073\n",
      "2023-12-30 17:31:11 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 895744 examples: 0.031 | 0.073\n",
      "2023-12-30 17:31:11 - INFO     | Early stopping: no decrease (0.070 vs 0.072); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:23<00:36,  9.06s/it]2023-12-30 17:31:11 - INFO     | Epoch: 16 | Learning Rate: 0.003\n",
      "2023-12-30 17:31:12 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 896064 examples: 0.004 | 0.072\n",
      "2023-12-30 17:31:12 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 897920 examples: 0.025 | 0.072\n",
      "2023-12-30 17:31:12 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 899776 examples: 0.036 | 0.075\n",
      "2023-12-30 17:31:12 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 901632 examples: 0.022 | 0.071\n",
      "2023-12-30 17:31:13 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 903488 examples: 0.019 | 0.070\n",
      "2023-12-30 17:31:13 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 905344 examples: 0.030 | 0.078\n",
      "2023-12-30 17:31:13 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 907200 examples: 0.027 | 0.073\n",
      "2023-12-30 17:31:13 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 909056 examples: 0.031 | 0.071\n",
      "2023-12-30 17:31:14 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 910912 examples: 0.031 | 0.075\n",
      "2023-12-30 17:31:14 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 912768 examples: 0.020 | 0.075\n",
      "2023-12-30 17:31:14 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 914624 examples: 0.041 | 0.075\n",
      "2023-12-30 17:31:15 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 916480 examples: 0.038 | 0.073\n",
      "2023-12-30 17:31:15 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 918336 examples: 0.031 | 0.072\n",
      "2023-12-30 17:31:15 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 920192 examples: 0.018 | 0.074\n",
      "2023-12-30 17:31:15 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 922048 examples: 0.033 | 0.073\n",
      "2023-12-30 17:31:16 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 923904 examples: 0.021 | 0.071\n",
      "2023-12-30 17:31:16 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 925760 examples: 0.042 | 0.072\n",
      "2023-12-30 17:31:16 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 927616 examples: 0.029 | 0.073\n",
      "2023-12-30 17:31:16 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 929472 examples: 0.034 | 0.072\n",
      "2023-12-30 17:31:17 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 931328 examples: 0.032 | 0.075\n",
      "2023-12-30 17:31:17 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 933184 examples: 0.028 | 0.073\n",
      "2023-12-30 17:31:17 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 935040 examples: 0.037 | 0.078\n",
      "2023-12-30 17:31:18 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 936896 examples: 0.022 | 0.076\n",
      "2023-12-30 17:31:18 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 938752 examples: 0.034 | 0.073\n",
      "2023-12-30 17:31:18 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 940608 examples: 0.038 | 0.072\n",
      "2023-12-30 17:31:19 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 942464 examples: 0.033 | 0.073\n",
      "2023-12-30 17:31:19 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 944320 examples: 0.040 | 0.075\n",
      "2023-12-30 17:31:19 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 946176 examples: 0.020 | 0.074\n",
      "2023-12-30 17:31:19 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 948032 examples: 0.030 | 0.073\n",
      "2023-12-30 17:31:20 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 949888 examples: 0.039 | 0.076\n",
      "2023-12-30 17:31:20 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 951744 examples: 0.041 | 0.074\n",
      "2023-12-30 17:31:20 - INFO     | Early stopping: no decrease (0.070 vs 0.074); counter: 3 out of 3\n",
      "2023-12-30 17:31:20 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:31:20 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 85%|████████▌ | 17/20 [02:32<00:27,  9.05s/it]2023-12-30 17:31:20 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 17:31:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.027 | 0.074\n",
      "2023-12-30 17:31:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.038 | 0.072\n",
      "2023-12-30 17:31:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.019 | 0.071\n",
      "2023-12-30 17:31:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.030 | 0.072\n",
      "2023-12-30 17:31:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.023 | 0.070\n",
      "2023-12-30 17:31:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.045 | 0.071\n",
      "2023-12-30 17:31:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.032 | 0.072\n",
      "2023-12-30 17:31:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.020 | 0.071\n",
      "2023-12-30 17:31:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.019 | 0.071\n",
      "2023-12-30 17:31:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.026 | 0.072\n",
      "2023-12-30 17:31:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.025 | 0.072\n",
      "2023-12-30 17:31:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.028 | 0.071\n",
      "2023-12-30 17:31:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.023 | 0.070\n",
      "2023-12-30 17:31:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.027 | 0.070\n",
      "2023-12-30 17:31:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.016 | 0.071\n",
      "2023-12-30 17:31:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.031 | 0.070\n",
      "2023-12-30 17:31:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.025 | 0.070\n",
      "2023-12-30 17:31:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.028 | 0.070\n",
      "2023-12-30 17:31:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.023 | 0.069\n",
      "2023-12-30 17:31:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.031 | 0.070\n",
      "2023-12-30 17:31:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.024 | 0.071\n",
      "2023-12-30 17:31:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.024 | 0.070\n",
      "2023-12-30 17:31:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.030 | 0.071\n",
      "2023-12-30 17:31:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.035 | 0.072\n",
      "2023-12-30 17:31:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.029 | 0.071\n",
      "2023-12-30 17:31:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.025 | 0.071\n",
      "2023-12-30 17:31:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.021 | 0.070\n",
      "2023-12-30 17:31:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.027 | 0.070\n",
      "2023-12-30 17:31:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.031 | 0.071\n",
      "2023-12-30 17:31:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.024 | 0.071\n",
      "2023-12-30 17:31:29 - INFO     | Early stopping: no decrease (0.070 vs 0.071); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [02:41<00:17,  8.97s/it]2023-12-30 17:31:29 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 17:31:29 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.002 | 0.071\n",
      "2023-12-30 17:31:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.035 | 0.070\n",
      "2023-12-30 17:31:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.025 | 0.069\n",
      "2023-12-30 17:31:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.030 | 0.072\n",
      "2023-12-30 17:31:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.018 | 0.071\n",
      "2023-12-30 17:31:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.019 | 0.072\n",
      "2023-12-30 17:31:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.036 | 0.072\n",
      "2023-12-30 17:31:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.018 | 0.071\n",
      "2023-12-30 17:31:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.019 | 0.072\n",
      "2023-12-30 17:31:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.027 | 0.073\n",
      "2023-12-30 17:31:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.033 | 0.072\n",
      "2023-12-30 17:31:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.030 | 0.074\n",
      "2023-12-30 17:31:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.018 | 0.071\n",
      "2023-12-30 17:31:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.018 | 0.071\n",
      "2023-12-30 17:31:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.038 | 0.070\n",
      "2023-12-30 17:31:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.020 | 0.071\n",
      "2023-12-30 17:31:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.014 | 0.072\n",
      "2023-12-30 17:31:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.024 | 0.071\n",
      "2023-12-30 17:31:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.025 | 0.071\n",
      "2023-12-30 17:31:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.027 | 0.070\n",
      "2023-12-30 17:31:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.019 | 0.070\n",
      "2023-12-30 17:31:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.024 | 0.071\n",
      "2023-12-30 17:31:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.030 | 0.070\n",
      "2023-12-30 17:31:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.027 | 0.072\n",
      "2023-12-30 17:31:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.029 | 0.071\n",
      "2023-12-30 17:31:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.031 | 0.070\n",
      "2023-12-30 17:31:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.026 | 0.071\n",
      "2023-12-30 17:31:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.042 | 0.070\n",
      "2023-12-30 17:31:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.028 | 0.071\n",
      "2023-12-30 17:31:38 - INFO     | Early stopping: no decrease (0.070 vs 0.071); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [02:50<00:08,  8.90s/it]2023-12-30 17:31:38 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 17:31:38 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.011 | 0.071\n",
      "2023-12-30 17:31:38 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.019 | 0.071\n",
      "2023-12-30 17:31:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.029 | 0.071\n",
      "2023-12-30 17:31:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.030 | 0.071\n",
      "2023-12-30 17:31:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.027 | 0.072\n",
      "2023-12-30 17:31:40 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:40 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.027 | 0.071\n",
      "2023-12-30 17:31:40 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.025 | 0.071\n",
      "2023-12-30 17:31:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.021 | 0.071\n",
      "2023-12-30 17:31:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.020 | 0.072\n",
      "2023-12-30 17:31:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.024 | 0.073\n",
      "2023-12-30 17:31:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.021 | 0.072\n",
      "2023-12-30 17:31:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.025 | 0.071\n",
      "2023-12-30 17:31:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.024 | 0.071\n",
      "2023-12-30 17:31:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.029 | 0.072\n",
      "2023-12-30 17:31:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.022 | 0.072\n",
      "2023-12-30 17:31:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.022 | 0.071\n",
      "2023-12-30 17:31:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.033 | 0.071\n",
      "2023-12-30 17:31:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.025 | 0.073\n",
      "2023-12-30 17:31:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.022 | 0.070\n",
      "2023-12-30 17:31:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.033 | 0.071\n",
      "2023-12-30 17:31:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.029 | 0.071\n",
      "2023-12-30 17:31:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.021 | 0.072\n",
      "2023-12-30 17:31:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.023 | 0.071\n",
      "2023-12-30 17:31:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.028 | 0.072\n",
      "2023-12-30 17:31:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.031 | 0.072\n",
      "2023-12-30 17:31:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.027 | 0.071\n",
      "2023-12-30 17:31:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.030 | 0.071\n",
      "2023-12-30 17:31:47 - INFO     | Early stopping: no decrease (0.070 vs 0.073); counter: 3 out of 3\n",
      "2023-12-30 17:31:47 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:31:47 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      "100%|██████████| 20/20 [02:59<00:00,  8.96s/it]\n",
      "2023-12-30 17:31:47 - INFO     | Best validation loss: 0.070\n",
      "2023-12-30 17:31:47 - INFO     | Best early stopping index/epoch: 13\n",
      "2023-12-30 17:31:47 - INFO     | Average Loss on test set: 0.078\n",
      "2023-12-30 17:31:49 - INFO     | Weighted Precision: 0.977, Recall: 0.977, F1: 0.977\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>████████████▃▃▃▃▃▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_validation_loss</td><td>0.07007</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00125</td></tr><tr><td>step_learning_rate</td><td>0.00125</td></tr><tr><td>step_training_loss</td><td>0.03031</td></tr><tr><td>step_validation_loss</td><td>0.07105</td></tr><tr><td>test_loss</td><td>0.0781</td></tr><tr><td>weighted_f1</td><td>0.97671</td></tr><tr><td>weighted_precision</td><td>0.97676</td></tr><tr><td>weighted_recall</td><td>0.97671</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-4</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/smh6vyzo' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/smh6vyzo</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_172847-smh6vyzo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d02l8pb9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [8, 16]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_173201-d02l8pb9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d02l8pb9' target=\"_blank\">autumn-sweep-5</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d02l8pb9' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d02l8pb9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [8, 16], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:32:02 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:02 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 21.456 | 13.184\n",
      "2023-12-30 17:32:02 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 3.852 | 0.799\n",
      "2023-12-30 17:32:03 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.609 | 0.412\n",
      "2023-12-30 17:32:03 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.381 | 0.336\n",
      "2023-12-30 17:32:03 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.306 | 0.305\n",
      "2023-12-30 17:32:04 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.312 | 0.274\n",
      "2023-12-30 17:32:04 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.323 | 0.232\n",
      "2023-12-30 17:32:04 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.237 | 0.213\n",
      "2023-12-30 17:32:04 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.215 | 0.209\n",
      "2023-12-30 17:32:05 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.191 | 0.188\n",
      "2023-12-30 17:32:05 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.183 | 0.202\n",
      "2023-12-30 17:32:05 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.152 | 0.169\n",
      "2023-12-30 17:32:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.135 | 0.176\n",
      "2023-12-30 17:32:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.202 | 0.178\n",
      "2023-12-30 17:32:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.164 | 0.161\n",
      "2023-12-30 17:32:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.159 | 0.152\n",
      "2023-12-30 17:32:07 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.154 | 0.177\n",
      "2023-12-30 17:32:07 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.154 | 0.147\n",
      "2023-12-30 17:32:07 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.154 | 0.135\n",
      "2023-12-30 17:32:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.146 | 0.216\n",
      "2023-12-30 17:32:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.144 | 0.147\n",
      "2023-12-30 17:32:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.150 | 0.142\n",
      "2023-12-30 17:32:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.147 | 0.134\n",
      "2023-12-30 17:32:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.116 | 0.126\n",
      "2023-12-30 17:32:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.128 | 0.147\n",
      "2023-12-30 17:32:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.132 | 0.132\n",
      "2023-12-30 17:32:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.118 | 0.121\n",
      "2023-12-30 17:32:10 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.108 | 0.121\n",
      "2023-12-30 17:32:10 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.116 | 0.129\n",
      "2023-12-30 17:32:10 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.146 | 0.112\n",
      "2023-12-30 17:32:11 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.100 | 0.121\n",
      "2023-12-30 17:32:11 - INFO     | Early stopping: loss decreased (inf -> 0.116; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:57,  9.37s/it]2023-12-30 17:32:11 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:11 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.020 | 0.114\n",
      "2023-12-30 17:32:11 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.114 | 0.111\n",
      "2023-12-30 17:32:12 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.075 | 0.126\n",
      "2023-12-30 17:32:12 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.080 | 0.119\n",
      "2023-12-30 17:32:12 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.112 | 0.125\n",
      "2023-12-30 17:32:13 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.089 | 0.109\n",
      "2023-12-30 17:32:13 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.071 | 0.103\n",
      "2023-12-30 17:32:13 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.120 | 0.119\n",
      "2023-12-30 17:32:13 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.089 | 0.117\n",
      "2023-12-30 17:32:14 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.095 | 0.105\n",
      "2023-12-30 17:32:14 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.098 | 0.107\n",
      "2023-12-30 17:32:14 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.116 | 0.107\n",
      "2023-12-30 17:32:15 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.097 | 0.094\n",
      "2023-12-30 17:32:15 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.111 | 0.095\n",
      "2023-12-30 17:32:15 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.099 | 0.098\n",
      "2023-12-30 17:32:15 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.084 | 0.087\n",
      "2023-12-30 17:32:16 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.085 | 0.093\n",
      "2023-12-30 17:32:16 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.088 | 0.088\n",
      "2023-12-30 17:32:16 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.060 | 0.096\n",
      "2023-12-30 17:32:17 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.068 | 0.091\n",
      "2023-12-30 17:32:17 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.089 | 0.094\n",
      "2023-12-30 17:32:17 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.094 | 0.108\n",
      "2023-12-30 17:32:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.096 | 0.106\n",
      "2023-12-30 17:32:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.074 | 0.114\n",
      "2023-12-30 17:32:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.097 | 0.101\n",
      "2023-12-30 17:32:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.082 | 0.098\n",
      "2023-12-30 17:32:19 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.098 | 0.087\n",
      "2023-12-30 17:32:19 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.066 | 0.083\n",
      "2023-12-30 17:32:19 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.084 | 0.079\n",
      "2023-12-30 17:32:20 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.080 | 0.083\n",
      "2023-12-30 17:32:20 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.056 | 0.088\n",
      "2023-12-30 17:32:20 - INFO     | Early stopping: loss decreased (0.116 -> 0.083; -28.3%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:47,  9.30s/it]2023-12-30 17:32:20 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:20 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.071 | 0.082\n",
      "2023-12-30 17:32:21 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.055 | 0.089\n",
      "2023-12-30 17:32:21 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.066 | 0.085\n",
      "2023-12-30 17:32:21 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.043 | 0.081\n",
      "2023-12-30 17:32:22 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.051 | 0.081\n",
      "2023-12-30 17:32:22 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.047 | 0.110\n",
      "2023-12-30 17:32:22 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.091 | 0.082\n",
      "2023-12-30 17:32:22 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.071 | 0.078\n",
      "2023-12-30 17:32:23 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.058 | 0.071\n",
      "2023-12-30 17:32:23 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.074 | 0.079\n",
      "2023-12-30 17:32:23 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.057 | 0.119\n",
      "2023-12-30 17:32:24 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.074 | 0.090\n",
      "2023-12-30 17:32:24 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.050 | 0.089\n",
      "2023-12-30 17:32:24 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.070 | 0.089\n",
      "2023-12-30 17:32:24 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.077 | 0.075\n",
      "2023-12-30 17:32:25 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.054 | 0.064\n",
      "2023-12-30 17:32:25 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.066 | 0.077\n",
      "2023-12-30 17:32:25 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.062 | 0.074\n",
      "2023-12-30 17:32:26 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.068 | 0.072\n",
      "2023-12-30 17:32:26 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.072 | 0.069\n",
      "2023-12-30 17:32:26 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.077 | 0.075\n",
      "2023-12-30 17:32:26 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.087 | 0.094\n",
      "2023-12-30 17:32:27 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.053 | 0.078\n",
      "2023-12-30 17:32:27 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.049 | 0.083\n",
      "2023-12-30 17:32:27 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.088 | 0.092\n",
      "2023-12-30 17:32:28 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.072 | 0.081\n",
      "2023-12-30 17:32:28 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.069 | 0.073\n",
      "2023-12-30 17:32:28 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.037 | 0.062\n",
      "2023-12-30 17:32:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.064 | 0.071\n",
      "2023-12-30 17:32:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.076 | 0.076\n",
      "2023-12-30 17:32:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.058 | 0.061\n",
      "2023-12-30 17:32:29 - INFO     | Early stopping: loss decreased (0.083 -> 0.061; -26.0%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:27<02:37,  9.26s/it]2023-12-30 17:32:29 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:30 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.022 | 0.063\n",
      "2023-12-30 17:32:30 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.045 | 0.078\n",
      "2023-12-30 17:32:30 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.046 | 0.070\n",
      "2023-12-30 17:32:31 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.073 | 0.084\n",
      "2023-12-30 17:32:31 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.060 | 0.093\n",
      "2023-12-30 17:32:31 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.048 | 0.080\n",
      "2023-12-30 17:32:31 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.047 | 0.090\n",
      "2023-12-30 17:32:32 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.048 | 0.077\n",
      "2023-12-30 17:32:32 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.058 | 0.068\n",
      "2023-12-30 17:32:32 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.044 | 0.093\n",
      "2023-12-30 17:32:32 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.049 | 0.083\n",
      "2023-12-30 17:32:33 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.060 | 0.092\n",
      "2023-12-30 17:32:33 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.071 | 0.087\n",
      "2023-12-30 17:32:33 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.053 | 0.076\n",
      "2023-12-30 17:32:34 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.049 | 0.076\n",
      "2023-12-30 17:32:34 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.076 | 0.080\n",
      "2023-12-30 17:32:34 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.077 | 0.071\n",
      "2023-12-30 17:32:35 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.063 | 0.087\n",
      "2023-12-30 17:32:35 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.052 | 0.081\n",
      "2023-12-30 17:32:35 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.062 | 0.077\n",
      "2023-12-30 17:32:35 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.060 | 0.066\n",
      "2023-12-30 17:32:36 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.053 | 0.068\n",
      "2023-12-30 17:32:36 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.063 | 0.072\n",
      "2023-12-30 17:32:36 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.052 | 0.074\n",
      "2023-12-30 17:32:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.085 | 0.071\n",
      "2023-12-30 17:32:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.052 | 0.075\n",
      "2023-12-30 17:32:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.051 | 0.065\n",
      "2023-12-30 17:32:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.060 | 0.066\n",
      "2023-12-30 17:32:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.071 | 0.083\n",
      "2023-12-30 17:32:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.088 | 0.065\n",
      "2023-12-30 17:32:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.042 | 0.058\n",
      "2023-12-30 17:32:39 - INFO     | Early stopping: no decrease (0.061 vs 0.059); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:37<02:28,  9.28s/it]2023-12-30 17:32:39 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:39 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.064 | 0.060\n",
      "2023-12-30 17:32:39 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.032 | 0.061\n",
      "2023-12-30 17:32:39 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.042 | 0.069\n",
      "2023-12-30 17:32:40 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.055 | 0.082\n",
      "2023-12-30 17:32:40 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.037 | 0.072\n",
      "2023-12-30 17:32:40 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.051 | 0.063\n",
      "2023-12-30 17:32:41 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.031 | 0.066\n",
      "2023-12-30 17:32:41 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.049 | 0.061\n",
      "2023-12-30 17:32:41 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.039 | 0.076\n",
      "2023-12-30 17:32:42 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.052 | 0.076\n",
      "2023-12-30 17:32:42 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.052 | 0.057\n",
      "2023-12-30 17:32:42 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.047 | 0.067\n",
      "2023-12-30 17:32:42 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.045 | 0.071\n",
      "2023-12-30 17:32:43 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.058 | 0.066\n",
      "2023-12-30 17:32:43 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.048 | 0.067\n",
      "2023-12-30 17:32:43 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.052 | 0.069\n",
      "2023-12-30 17:32:43 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.052 | 0.070\n",
      "2023-12-30 17:32:44 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.043 | 0.077\n",
      "2023-12-30 17:32:44 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.044 | 0.072\n",
      "2023-12-30 17:32:44 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.054 | 0.076\n",
      "2023-12-30 17:32:45 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.044 | 0.069\n",
      "2023-12-30 17:32:45 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.061 | 0.082\n",
      "2023-12-30 17:32:45 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.070 | 0.078\n",
      "2023-12-30 17:32:46 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.057 | 0.093\n",
      "2023-12-30 17:32:46 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.045 | 0.085\n",
      "2023-12-30 17:32:46 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.046 | 0.089\n",
      "2023-12-30 17:32:46 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.062 | 0.078\n",
      "2023-12-30 17:32:47 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.042 | 0.072\n",
      "2023-12-30 17:32:47 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.030 | 0.071\n",
      "2023-12-30 17:32:47 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.074 | 0.073\n",
      "2023-12-30 17:32:48 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.036 | 0.068\n",
      "2023-12-30 17:32:48 - INFO     | Early stopping: no decrease (0.061 vs 0.067); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:46<02:18,  9.22s/it]2023-12-30 17:32:48 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:48 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.006 | 0.068\n",
      "2023-12-30 17:32:48 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.041 | 0.066\n",
      "2023-12-30 17:32:49 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.055 | 0.066\n",
      "2023-12-30 17:32:49 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.040 | 0.064\n",
      "2023-12-30 17:32:49 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.041 | 0.064\n",
      "2023-12-30 17:32:49 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.043 | 0.070\n",
      "2023-12-30 17:32:50 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.038 | 0.064\n",
      "2023-12-30 17:32:50 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.057 | 0.078\n",
      "2023-12-30 17:32:50 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.036 | 0.073\n",
      "2023-12-30 17:32:51 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.051 | 0.077\n",
      "2023-12-30 17:32:51 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.064 | 0.064\n",
      "2023-12-30 17:32:51 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.042 | 0.070\n",
      "2023-12-30 17:32:51 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.050 | 0.075\n",
      "2023-12-30 17:32:52 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.049 | 0.080\n",
      "2023-12-30 17:32:52 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.057 | 0.067\n",
      "2023-12-30 17:32:52 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.050 | 0.081\n",
      "2023-12-30 17:32:53 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.058 | 0.083\n",
      "2023-12-30 17:32:53 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.049 | 0.075\n",
      "2023-12-30 17:32:53 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.045 | 0.078\n",
      "2023-12-30 17:32:53 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.060 | 0.079\n",
      "2023-12-30 17:32:54 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.044 | 0.073\n",
      "2023-12-30 17:32:54 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.054 | 0.078\n",
      "2023-12-30 17:32:54 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.041 | 0.075\n",
      "2023-12-30 17:32:55 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.035 | 0.082\n",
      "2023-12-30 17:32:55 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.041 | 0.072\n",
      "2023-12-30 17:32:55 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.063 | 0.063\n",
      "2023-12-30 17:32:56 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.057 | 0.070\n",
      "2023-12-30 17:32:56 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.056 | 0.078\n",
      "2023-12-30 17:32:56 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.041 | 0.076\n",
      "2023-12-30 17:32:56 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.062 | 0.078\n",
      "2023-12-30 17:32:57 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.042 | 0.070\n",
      "2023-12-30 17:32:57 - INFO     | Early stopping: no decrease (0.061 vs 0.067); counter: 3 out of 3\n",
      "2023-12-30 17:32:57 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:32:57 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 30%|███       | 6/20 [00:55<02:08,  9.21s/it]2023-12-30 17:32:57 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 17:32:57 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.013 | 0.069\n",
      "2023-12-30 17:32:58 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.030 | 0.063\n",
      "2023-12-30 17:32:58 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.027 | 0.059\n",
      "2023-12-30 17:32:58 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.036 | 0.058\n",
      "2023-12-30 17:32:58 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.041 | 0.065\n",
      "2023-12-30 17:32:59 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.028 | 0.069\n",
      "2023-12-30 17:32:59 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.029 | 0.055\n",
      "2023-12-30 17:32:59 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.026 | 0.067\n",
      "2023-12-30 17:32:59 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.031 | 0.061\n",
      "2023-12-30 17:33:00 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.017 | 0.061\n",
      "2023-12-30 17:33:00 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.028 | 0.056\n",
      "2023-12-30 17:33:00 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.033 | 0.061\n",
      "2023-12-30 17:33:01 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.020 | 0.066\n",
      "2023-12-30 17:33:01 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.031 | 0.055\n",
      "2023-12-30 17:33:01 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.020 | 0.060\n",
      "2023-12-30 17:33:01 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.037 | 0.064\n",
      "2023-12-30 17:33:02 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.033 | 0.062\n",
      "2023-12-30 17:33:02 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.036 | 0.060\n",
      "2023-12-30 17:33:02 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.024 | 0.060\n",
      "2023-12-30 17:33:03 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.027 | 0.060\n",
      "2023-12-30 17:33:03 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.031 | 0.059\n",
      "2023-12-30 17:33:03 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.017 | 0.060\n",
      "2023-12-30 17:33:03 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.023 | 0.075\n",
      "2023-12-30 17:33:04 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.035 | 0.069\n",
      "2023-12-30 17:33:04 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.024 | 0.066\n",
      "2023-12-30 17:33:04 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.031 | 0.069\n",
      "2023-12-30 17:33:05 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.025 | 0.067\n",
      "2023-12-30 17:33:05 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.058 | 0.062\n",
      "2023-12-30 17:33:05 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.029 | 0.064\n",
      "2023-12-30 17:33:05 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.043 | 0.073\n",
      "2023-12-30 17:33:06 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.032 | 0.065\n",
      "2023-12-30 17:33:06 - INFO     | Early stopping: no decrease (0.061 vs 0.065); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:04<01:59,  9.17s/it]2023-12-30 17:33:06 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 17:33:06 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.021 | 0.063\n",
      "2023-12-30 17:33:07 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.019 | 0.053\n",
      "2023-12-30 17:33:07 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.015 | 0.050\n",
      "2023-12-30 17:33:07 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.013 | 0.054\n",
      "2023-12-30 17:33:08 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.017 | 0.060\n",
      "2023-12-30 17:33:08 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.011 | 0.058\n",
      "2023-12-30 17:33:08 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.023 | 0.052\n",
      "2023-12-30 17:33:08 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.016 | 0.053\n",
      "2023-12-30 17:33:09 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.018 | 0.058\n",
      "2023-12-30 17:33:09 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.021 | 0.057\n",
      "2023-12-30 17:33:09 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.023 | 0.056\n",
      "2023-12-30 17:33:09 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.018 | 0.060\n",
      "2023-12-30 17:33:10 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.020 | 0.058\n",
      "2023-12-30 17:33:10 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.026 | 0.055\n",
      "2023-12-30 17:33:10 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.032 | 0.068\n",
      "2023-12-30 17:33:11 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.025 | 0.058\n",
      "2023-12-30 17:33:11 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.031 | 0.058\n",
      "2023-12-30 17:33:11 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.018 | 0.058\n",
      "2023-12-30 17:33:11 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.017 | 0.058\n",
      "2023-12-30 17:33:12 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.021 | 0.062\n",
      "2023-12-30 17:33:12 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.033 | 0.053\n",
      "2023-12-30 17:33:12 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.033 | 0.053\n",
      "2023-12-30 17:33:13 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.031 | 0.057\n",
      "2023-12-30 17:33:13 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.017 | 0.053\n",
      "2023-12-30 17:33:13 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.020 | 0.052\n",
      "2023-12-30 17:33:13 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.032 | 0.057\n",
      "2023-12-30 17:33:14 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.014 | 0.065\n",
      "2023-12-30 17:33:14 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.029 | 0.057\n",
      "2023-12-30 17:33:14 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.024 | 0.067\n",
      "2023-12-30 17:33:15 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.018 | 0.056\n",
      "2023-12-30 17:33:15 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.036 | 0.063\n",
      "2023-12-30 17:33:15 - INFO     | Early stopping: no decrease (0.061 vs 0.059); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:13<01:49,  9.13s/it]2023-12-30 17:33:15 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 17:33:15 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.102 | 0.058\n",
      "2023-12-30 17:33:16 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.015 | 0.052\n",
      "2023-12-30 17:33:16 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.014 | 0.052\n",
      "2023-12-30 17:33:16 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.020 | 0.061\n",
      "2023-12-30 17:33:17 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.009 | 0.055\n",
      "2023-12-30 17:33:17 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.020 | 0.060\n",
      "2023-12-30 17:33:17 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.012 | 0.054\n",
      "2023-12-30 17:33:18 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.022 | 0.055\n",
      "2023-12-30 17:33:18 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.008 | 0.057\n",
      "2023-12-30 17:33:18 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.013 | 0.057\n",
      "2023-12-30 17:33:18 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.012 | 0.061\n",
      "2023-12-30 17:33:19 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.024 | 0.058\n",
      "2023-12-30 17:33:19 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.013 | 0.053\n",
      "2023-12-30 17:33:19 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.014 | 0.060\n",
      "2023-12-30 17:33:20 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.015 | 0.052\n",
      "2023-12-30 17:33:20 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.019 | 0.072\n",
      "2023-12-30 17:33:20 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.027 | 0.060\n",
      "2023-12-30 17:33:20 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.021 | 0.060\n",
      "2023-12-30 17:33:21 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.025 | 0.078\n",
      "2023-12-30 17:33:21 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.020 | 0.056\n",
      "2023-12-30 17:33:21 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.022 | 0.058\n",
      "2023-12-30 17:33:22 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.021 | 0.067\n",
      "2023-12-30 17:33:22 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.030 | 0.063\n",
      "2023-12-30 17:33:22 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.019 | 0.059\n",
      "2023-12-30 17:33:22 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.008 | 0.056\n",
      "2023-12-30 17:33:23 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.029 | 0.094\n",
      "2023-12-30 17:33:23 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.016 | 0.058\n",
      "2023-12-30 17:33:23 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.018 | 0.057\n",
      "2023-12-30 17:33:23 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.017 | 0.059\n",
      "2023-12-30 17:33:24 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.029 | 0.065\n",
      "2023-12-30 17:33:24 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.029 | 0.072\n",
      "2023-12-30 17:33:24 - INFO     | Early stopping: no decrease (0.061 vs 0.072); counter: 3 out of 3\n",
      "2023-12-30 17:33:24 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:33:24 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 45%|████▌     | 9/20 [01:22<01:40,  9.17s/it]2023-12-30 17:33:24 - INFO     | Epoch: 9 | Learning Rate: 0.000\n",
      "2023-12-30 17:33:25 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 504064 examples: 0.005 | 0.070\n",
      "2023-12-30 17:33:25 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 505920 examples: 0.021 | 0.060\n",
      "2023-12-30 17:33:25 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 507776 examples: 0.009 | 0.056\n",
      "2023-12-30 17:33:26 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 509632 examples: 0.009 | 0.056\n",
      "2023-12-30 17:33:26 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 511488 examples: 0.005 | 0.054\n",
      "2023-12-30 17:33:26 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 513344 examples: 0.006 | 0.059\n",
      "2023-12-30 17:33:26 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 515200 examples: 0.030 | 0.060\n",
      "2023-12-30 17:33:27 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 517056 examples: 0.008 | 0.052\n",
      "2023-12-30 17:33:27 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 518912 examples: 0.007 | 0.056\n",
      "2023-12-30 17:33:27 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 520768 examples: 0.009 | 0.054\n",
      "2023-12-30 17:33:28 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 522624 examples: 0.009 | 0.055\n",
      "2023-12-30 17:33:28 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 524480 examples: 0.005 | 0.056\n",
      "2023-12-30 17:33:28 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 526336 examples: 0.006 | 0.058\n",
      "2023-12-30 17:33:28 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 528192 examples: 0.016 | 0.056\n",
      "2023-12-30 17:33:29 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 530048 examples: 0.005 | 0.055\n",
      "2023-12-30 17:33:29 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 531904 examples: 0.008 | 0.054\n",
      "2023-12-30 17:33:29 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 533760 examples: 0.009 | 0.054\n",
      "2023-12-30 17:33:29 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 535616 examples: 0.010 | 0.053\n",
      "2023-12-30 17:33:30 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 537472 examples: 0.011 | 0.055\n",
      "2023-12-30 17:33:30 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 539328 examples: 0.007 | 0.056\n",
      "2023-12-30 17:33:30 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 541184 examples: 0.007 | 0.059\n",
      "2023-12-30 17:33:31 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 543040 examples: 0.007 | 0.059\n",
      "2023-12-30 17:33:31 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 544896 examples: 0.012 | 0.056\n",
      "2023-12-30 17:33:31 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 546752 examples: 0.012 | 0.056\n",
      "2023-12-30 17:33:32 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 548608 examples: 0.004 | 0.061\n",
      "2023-12-30 17:33:32 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 550464 examples: 0.021 | 0.058\n",
      "2023-12-30 17:33:32 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 552320 examples: 0.011 | 0.060\n",
      "2023-12-30 17:33:32 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 554176 examples: 0.013 | 0.055\n",
      "2023-12-30 17:33:33 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 556032 examples: 0.015 | 0.056\n",
      "2023-12-30 17:33:33 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 557888 examples: 0.014 | 0.062\n",
      "2023-12-30 17:33:33 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 559744 examples: 0.012 | 0.056\n",
      "2023-12-30 17:33:34 - INFO     | Early stopping: loss decreased (0.061 -> 0.055; -10.4%). Caching model state.\n",
      " 50%|█████     | 10/20 [01:32<01:31,  9.18s/it]2023-12-30 17:33:34 - INFO     | Epoch: 10 | Learning Rate: 0.000\n",
      "2023-12-30 17:33:34 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 560064 examples: 0.013 | 0.055\n",
      "2023-12-30 17:33:34 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 561920 examples: 0.009 | 0.057\n",
      "2023-12-30 17:33:34 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 563776 examples: 0.003 | 0.060\n",
      "2023-12-30 17:33:35 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 565632 examples: 0.006 | 0.053\n",
      "2023-12-30 17:33:35 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 567488 examples: 0.008 | 0.054\n",
      "2023-12-30 17:33:35 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 569344 examples: 0.006 | 0.053\n",
      "2023-12-30 17:33:35 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 571200 examples: 0.002 | 0.053\n",
      "2023-12-30 17:33:36 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 573056 examples: 0.007 | 0.055\n",
      "2023-12-30 17:33:36 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 574912 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:36 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 576768 examples: 0.002 | 0.054\n",
      "2023-12-30 17:33:37 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 578624 examples: 0.008 | 0.052\n",
      "2023-12-30 17:33:37 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 580480 examples: 0.002 | 0.055\n",
      "2023-12-30 17:33:37 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 582336 examples: 0.005 | 0.054\n",
      "2023-12-30 17:33:38 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 584192 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:38 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 586048 examples: 0.007 | 0.053\n",
      "2023-12-30 17:33:38 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 587904 examples: 0.004 | 0.057\n",
      "2023-12-30 17:33:38 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 589760 examples: 0.003 | 0.056\n",
      "2023-12-30 17:33:39 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 591616 examples: 0.005 | 0.057\n",
      "2023-12-30 17:33:39 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 593472 examples: 0.005 | 0.056\n",
      "2023-12-30 17:33:39 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 595328 examples: 0.002 | 0.057\n",
      "2023-12-30 17:33:40 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 597184 examples: 0.004 | 0.056\n",
      "2023-12-30 17:33:40 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 599040 examples: 0.007 | 0.063\n",
      "2023-12-30 17:33:40 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 600896 examples: 0.007 | 0.055\n",
      "2023-12-30 17:33:40 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 602752 examples: 0.007 | 0.055\n",
      "2023-12-30 17:33:41 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 604608 examples: 0.004 | 0.055\n",
      "2023-12-30 17:33:41 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 606464 examples: 0.016 | 0.054\n",
      "2023-12-30 17:33:41 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 608320 examples: 0.013 | 0.059\n",
      "2023-12-30 17:33:42 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 610176 examples: 0.013 | 0.058\n",
      "2023-12-30 17:33:42 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 612032 examples: 0.009 | 0.059\n",
      "2023-12-30 17:33:42 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 613888 examples: 0.006 | 0.061\n",
      "2023-12-30 17:33:42 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 615744 examples: 0.012 | 0.060\n",
      "2023-12-30 17:33:43 - INFO     | Early stopping: no decrease (0.055 vs 0.059); counter: 1 out of 3\n",
      " 55%|█████▌    | 11/20 [01:41<01:22,  9.15s/it]2023-12-30 17:33:43 - INFO     | Epoch: 11 | Learning Rate: 0.000\n",
      "2023-12-30 17:33:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 616064 examples: 0.001 | 0.059\n",
      "2023-12-30 17:33:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 617920 examples: 0.004 | 0.058\n",
      "2023-12-30 17:33:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 619776 examples: 0.002 | 0.058\n",
      "2023-12-30 17:33:44 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 621632 examples: 0.014 | 0.056\n",
      "2023-12-30 17:33:44 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 623488 examples: 0.007 | 0.060\n",
      "2023-12-30 17:33:44 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 625344 examples: 0.006 | 0.056\n",
      "2023-12-30 17:33:45 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 627200 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:45 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 629056 examples: 0.002 | 0.058\n",
      "2023-12-30 17:33:45 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 630912 examples: 0.004 | 0.055\n",
      "2023-12-30 17:33:45 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 632768 examples: 0.004 | 0.055\n",
      "2023-12-30 17:33:46 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 634624 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:46 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 636480 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:46 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 638336 examples: 0.004 | 0.055\n",
      "2023-12-30 17:33:47 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 640192 examples: 0.003 | 0.058\n",
      "2023-12-30 17:33:47 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 642048 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:47 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 643904 examples: 0.003 | 0.054\n",
      "2023-12-30 17:33:47 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 645760 examples: 0.006 | 0.056\n",
      "2023-12-30 17:33:48 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 647616 examples: 0.005 | 0.060\n",
      "2023-12-30 17:33:48 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 649472 examples: 0.004 | 0.065\n",
      "2023-12-30 17:33:48 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 651328 examples: 0.003 | 0.059\n",
      "2023-12-30 17:33:49 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 653184 examples: 0.005 | 0.062\n",
      "2023-12-30 17:33:49 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 655040 examples: 0.003 | 0.060\n",
      "2023-12-30 17:33:49 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 656896 examples: 0.005 | 0.061\n",
      "2023-12-30 17:33:49 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 658752 examples: 0.003 | 0.062\n",
      "2023-12-30 17:33:50 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 660608 examples: 0.005 | 0.058\n",
      "2023-12-30 17:33:50 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 662464 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:50 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 664320 examples: 0.008 | 0.067\n",
      "2023-12-30 17:33:51 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 666176 examples: 0.003 | 0.059\n",
      "2023-12-30 17:33:51 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 668032 examples: 0.004 | 0.062\n",
      "2023-12-30 17:33:51 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 669888 examples: 0.006 | 0.061\n",
      "2023-12-30 17:33:51 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 671744 examples: 0.005 | 0.060\n",
      "2023-12-30 17:33:52 - INFO     | Early stopping: no decrease (0.055 vs 0.060); counter: 2 out of 3\n",
      " 60%|██████    | 12/20 [01:50<01:12,  9.12s/it]2023-12-30 17:33:52 - INFO     | Epoch: 12 | Learning Rate: 0.000\n",
      "2023-12-30 17:33:52 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 672064 examples: 0.262 | 0.060\n",
      "2023-12-30 17:33:52 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 673920 examples: 0.003 | 0.056\n",
      "2023-12-30 17:33:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 675776 examples: 0.005 | 0.057\n",
      "2023-12-30 17:33:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 677632 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 679488 examples: 0.003 | 0.055\n",
      "2023-12-30 17:33:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 681344 examples: 0.003 | 0.057\n",
      "2023-12-30 17:33:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 683200 examples: 0.002 | 0.059\n",
      "2023-12-30 17:33:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 685056 examples: 0.004 | 0.056\n",
      "2023-12-30 17:33:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 686912 examples: 0.002 | 0.057\n",
      "2023-12-30 17:33:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 688768 examples: 0.001 | 0.056\n",
      "2023-12-30 17:33:55 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 690624 examples: 0.002 | 0.062\n",
      "2023-12-30 17:33:55 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 692480 examples: 0.004 | 0.068\n",
      "2023-12-30 17:33:55 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 694336 examples: 0.006 | 0.072\n",
      "2023-12-30 17:33:56 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 696192 examples: 0.004 | 0.064\n",
      "2023-12-30 17:33:56 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 698048 examples: 0.006 | 0.067\n",
      "2023-12-30 17:33:56 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 699904 examples: 0.002 | 0.066\n",
      "2023-12-30 17:33:56 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 701760 examples: 0.002 | 0.064\n",
      "2023-12-30 17:33:57 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 703616 examples: 0.004 | 0.061\n",
      "2023-12-30 17:33:57 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 705472 examples: 0.006 | 0.066\n",
      "2023-12-30 17:33:57 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 707328 examples: 0.006 | 0.066\n",
      "2023-12-30 17:33:58 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 709184 examples: 0.006 | 0.061\n",
      "2023-12-30 17:33:58 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 711040 examples: 0.007 | 0.063\n",
      "2023-12-30 17:33:58 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 712896 examples: 0.009 | 0.058\n",
      "2023-12-30 17:33:58 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 714752 examples: 0.010 | 0.060\n",
      "2023-12-30 17:33:59 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 716608 examples: 0.007 | 0.059\n",
      "2023-12-30 17:33:59 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 718464 examples: 0.005 | 0.063\n",
      "2023-12-30 17:33:59 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 720320 examples: 0.004 | 0.060\n",
      "2023-12-30 17:33:59 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 722176 examples: 0.005 | 0.062\n",
      "2023-12-30 17:34:00 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 724032 examples: 0.006 | 0.064\n",
      "2023-12-30 17:34:00 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 725888 examples: 0.003 | 0.065\n",
      "2023-12-30 17:34:00 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 727744 examples: 0.009 | 0.061\n",
      "2023-12-30 17:34:01 - INFO     | Early stopping: no decrease (0.055 vs 0.060); counter: 3 out of 3\n",
      "2023-12-30 17:34:01 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:34:01 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      " 65%|██████▌   | 13/20 [01:59<01:03,  9.04s/it]2023-12-30 17:34:01 - INFO     | Epoch: 13 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:01 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 728064 examples: 0.000 | 0.061\n",
      "2023-12-30 17:34:01 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 729920 examples: 0.003 | 0.058\n",
      "2023-12-30 17:34:01 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 731776 examples: 0.009 | 0.057\n",
      "2023-12-30 17:34:02 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 733632 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:02 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 735488 examples: 0.001 | 0.059\n",
      "2023-12-30 17:34:02 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 737344 examples: 0.001 | 0.061\n",
      "2023-12-30 17:34:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 739200 examples: 0.001 | 0.062\n",
      "2023-12-30 17:34:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 741056 examples: 0.005 | 0.066\n",
      "2023-12-30 17:34:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 742912 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 744768 examples: 0.003 | 0.061\n",
      "2023-12-30 17:34:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 746624 examples: 0.003 | 0.061\n",
      "2023-12-30 17:34:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 748480 examples: 0.002 | 0.061\n",
      "2023-12-30 17:34:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 750336 examples: 0.006 | 0.063\n",
      "2023-12-30 17:34:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 752192 examples: 0.001 | 0.062\n",
      "2023-12-30 17:34:05 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 754048 examples: 0.002 | 0.062\n",
      "2023-12-30 17:34:05 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 755904 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:05 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 757760 examples: 0.002 | 0.059\n",
      "2023-12-30 17:34:06 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 759616 examples: 0.003 | 0.061\n",
      "2023-12-30 17:34:06 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 761472 examples: 0.002 | 0.062\n",
      "2023-12-30 17:34:06 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 763328 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:06 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 765184 examples: 0.001 | 0.062\n",
      "2023-12-30 17:34:07 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 767040 examples: 0.002 | 0.061\n",
      "2023-12-30 17:34:07 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 768896 examples: 0.001 | 0.059\n",
      "2023-12-30 17:34:07 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 770752 examples: 0.002 | 0.059\n",
      "2023-12-30 17:34:08 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 772608 examples: 0.003 | 0.068\n",
      "2023-12-30 17:34:08 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 774464 examples: 0.007 | 0.061\n",
      "2023-12-30 17:34:08 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 776320 examples: 0.002 | 0.059\n",
      "2023-12-30 17:34:08 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 778176 examples: 0.001 | 0.059\n",
      "2023-12-30 17:34:09 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 780032 examples: 0.001 | 0.057\n",
      "2023-12-30 17:34:09 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 781888 examples: 0.002 | 0.057\n",
      "2023-12-30 17:34:09 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 783744 examples: 0.004 | 0.060\n",
      "2023-12-30 17:34:10 - INFO     | Early stopping: no decrease (0.055 vs 0.059); counter: 1 out of 3\n",
      " 70%|███████   | 14/20 [02:08<00:54,  9.03s/it]2023-12-30 17:34:10 - INFO     | Epoch: 14 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 784064 examples: 0.000 | 0.059\n",
      "2023-12-30 17:34:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 785920 examples: 0.001 | 0.057\n",
      "2023-12-30 17:34:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 787776 examples: 0.001 | 0.057\n",
      "2023-12-30 17:34:11 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 789632 examples: 0.000 | 0.058\n",
      "2023-12-30 17:34:11 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 791488 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:11 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 793344 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 795200 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 797056 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 798912 examples: 0.000 | 0.059\n",
      "2023-12-30 17:34:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 800768 examples: 0.001 | 0.060\n",
      "2023-12-30 17:34:13 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 802624 examples: 0.001 | 0.059\n",
      "2023-12-30 17:34:13 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 804480 examples: 0.001 | 0.059\n",
      "2023-12-30 17:34:13 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 806336 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 808192 examples: 0.001 | 0.058\n",
      "2023-12-30 17:34:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 810048 examples: 0.001 | 0.059\n",
      "2023-12-30 17:34:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 811904 examples: 0.001 | 0.060\n",
      "2023-12-30 17:34:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 813760 examples: 0.002 | 0.060\n",
      "2023-12-30 17:34:15 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 815616 examples: 0.002 | 0.059\n",
      "2023-12-30 17:34:15 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 817472 examples: 0.002 | 0.062\n",
      "2023-12-30 17:34:15 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 819328 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:16 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 821184 examples: 0.001 | 0.062\n",
      "2023-12-30 17:34:16 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 823040 examples: 0.001 | 0.061\n",
      "2023-12-30 17:34:16 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 824896 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:16 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 826752 examples: 0.008 | 0.063\n",
      "2023-12-30 17:34:17 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 828608 examples: 0.003 | 0.068\n",
      "2023-12-30 17:34:17 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 830464 examples: 0.001 | 0.068\n",
      "2023-12-30 17:34:17 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 832320 examples: 0.001 | 0.067\n",
      "2023-12-30 17:34:17 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 834176 examples: 0.001 | 0.067\n",
      "2023-12-30 17:34:18 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 836032 examples: 0.003 | 0.072\n",
      "2023-12-30 17:34:18 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 837888 examples: 0.002 | 0.068\n",
      "2023-12-30 17:34:18 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 839744 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:19 - INFO     | Early stopping: no decrease (0.055 vs 0.066); counter: 2 out of 3\n",
      " 75%|███████▌  | 15/20 [02:17<00:45,  9.05s/it]2023-12-30 17:34:19 - INFO     | Epoch: 15 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 840064 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 841920 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 843776 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:20 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 845632 examples: 0.004 | 0.070\n",
      "2023-12-30 17:34:20 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 847488 examples: 0.001 | 0.061\n",
      "2023-12-30 17:34:20 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 849344 examples: 0.002 | 0.063\n",
      "2023-12-30 17:34:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 851200 examples: 0.000 | 0.063\n",
      "2023-12-30 17:34:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 853056 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 854912 examples: 0.000 | 0.063\n",
      "2023-12-30 17:34:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 856768 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:22 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 858624 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:22 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 860480 examples: 0.001 | 0.062\n",
      "2023-12-30 17:34:22 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 862336 examples: 0.001 | 0.062\n",
      "2023-12-30 17:34:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 864192 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 866048 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 867904 examples: 0.000 | 0.063\n",
      "2023-12-30 17:34:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 869760 examples: 0.000 | 0.063\n",
      "2023-12-30 17:34:24 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 871616 examples: 0.001 | 0.061\n",
      "2023-12-30 17:34:24 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 873472 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:24 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 875328 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:25 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 877184 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:25 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 879040 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:25 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 880896 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:25 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 882752 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:26 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 884608 examples: 0.001 | 0.064\n",
      "2023-12-30 17:34:26 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 886464 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:26 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 888320 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:27 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 890176 examples: 0.002 | 0.064\n",
      "2023-12-30 17:34:27 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 892032 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:27 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 893888 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:28 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 895744 examples: 0.001 | 0.064\n",
      "2023-12-30 17:34:28 - INFO     | Early stopping: no decrease (0.055 vs 0.064); counter: 3 out of 3\n",
      "2023-12-30 17:34:28 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:34:28 - INFO     | Reducing learning rate: 0.000125 -> 6.25e-05\n",
      " 80%|████████  | 16/20 [02:26<00:36,  9.13s/it]2023-12-30 17:34:28 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.001 | 0.064\n",
      "2023-12-30 17:34:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.001 | 0.064\n",
      "2023-12-30 17:34:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.001 | 0.063\n",
      "2023-12-30 17:34:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.002 | 0.066\n",
      "2023-12-30 17:34:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.001 | 0.067\n",
      "2023-12-30 17:34:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.001 | 0.066\n",
      "2023-12-30 17:34:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.001 | 0.065\n",
      "2023-12-30 17:34:37 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:37 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:37 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:37 - INFO     | Early stopping: no decrease (0.055 vs 0.064); counter: 1 out of 3\n",
      " 85%|████████▌ | 17/20 [02:35<00:27,  9.23s/it]2023-12-30 17:34:37 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.000 | 0.069\n",
      "2023-12-30 17:34:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.000 | 0.069\n",
      "2023-12-30 17:34:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.001 | 0.067\n",
      "2023-12-30 17:34:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.001 | 0.064\n",
      "2023-12-30 17:34:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:47 - INFO     | Early stopping: no decrease (0.055 vs 0.064); counter: 2 out of 3\n",
      " 90%|█████████ | 18/20 [02:45<00:18,  9.24s/it]2023-12-30 17:34:47 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.000 | 0.064\n",
      "2023-12-30 17:34:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.065\n",
      "2023-12-30 17:34:49 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:49 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:49 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.066\n",
      "2023-12-30 17:34:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:56 - INFO     | Early stopping: no decrease (0.055 vs 0.067); counter: 3 out of 3\n",
      "2023-12-30 17:34:56 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:34:56 - INFO     | Reducing learning rate: 6.25e-05 -> 3.125e-05\n",
      " 95%|█████████▌| 19/20 [02:54<00:09,  9.21s/it]2023-12-30 17:34:56 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 17:34:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.000 | 0.069\n",
      "2023-12-30 17:34:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.069\n",
      "2023-12-30 17:34:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.069\n",
      "2023-12-30 17:34:58 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:58 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:58 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.069\n",
      "2023-12-30 17:34:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.068\n",
      "2023-12-30 17:34:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.067\n",
      "2023-12-30 17:34:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.069\n",
      "2023-12-30 17:35:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.069\n",
      "2023-12-30 17:35:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.069\n",
      "2023-12-30 17:35:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.001 | 0.067\n",
      "2023-12-30 17:35:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.000 | 0.066\n",
      "2023-12-30 17:35:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.067\n",
      "2023-12-30 17:35:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.067\n",
      "2023-12-30 17:35:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.067\n",
      "2023-12-30 17:35:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.068\n",
      "2023-12-30 17:35:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.067\n",
      "2023-12-30 17:35:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.067\n",
      "2023-12-30 17:35:05 - INFO     | Early stopping: no decrease (0.055 vs 0.067); counter: 1 out of 3\n",
      "100%|██████████| 20/20 [03:03<00:00,  9.17s/it]\n",
      "2023-12-30 17:35:05 - INFO     | Best validation loss: 0.055\n",
      "2023-12-30 17:35:05 - INFO     | Best early stopping index/epoch: 9\n",
      "2023-12-30 17:35:05 - INFO     | Average Loss on test set: 0.063\n",
      "2023-12-30 17:35:07 - INFO     | Weighted Precision: 0.987, Recall: 0.987, F1: 0.987\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▃▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▂▄▃▃▂▃▃▃▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_validation_loss</td><td>0.05504</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>step_learning_rate</td><td>3e-05</td></tr><tr><td>step_training_loss</td><td>8e-05</td></tr><tr><td>step_validation_loss</td><td>0.06692</td></tr><tr><td>test_loss</td><td>0.06288</td></tr><tr><td>weighted_f1</td><td>0.98715</td></tr><tr><td>weighted_precision</td><td>0.98718</td></tr><tr><td>weighted_recall</td><td>0.98714</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-sweep-5</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d02l8pb9' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/d02l8pb9</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_173201-d02l8pb9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: omrgjney with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [8, 16]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_173516-omrgjney</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/omrgjney' target=\"_blank\">usual-sweep-6</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/omrgjney' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/omrgjney</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [8, 16], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:35:16 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 17:35:16 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 21.637 | 84.192\n",
      "2023-12-30 17:35:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 5.547 | 1.098\n",
      "2023-12-30 17:35:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.838 | 0.722\n",
      "2023-12-30 17:35:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.629 | 0.516\n",
      "2023-12-30 17:35:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.472 | 0.443\n",
      "2023-12-30 17:35:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.400 | 0.371\n",
      "2023-12-30 17:35:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.376 | 0.327\n",
      "2023-12-30 17:35:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.285 | 0.325\n",
      "2023-12-30 17:35:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.348 | 0.285\n",
      "2023-12-30 17:35:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.273 | 0.260\n",
      "2023-12-30 17:35:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.235 | 0.253\n",
      "2023-12-30 17:35:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.232 | 0.238\n",
      "2023-12-30 17:35:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.229 | 0.225\n",
      "2023-12-30 17:35:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.220 | 0.219\n",
      "2023-12-30 17:35:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.221 | 0.211\n",
      "2023-12-30 17:35:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.241 | 0.195\n",
      "2023-12-30 17:35:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.186 | 0.192\n",
      "2023-12-30 17:35:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.198 | 0.181\n",
      "2023-12-30 17:35:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.191 | 0.178\n",
      "2023-12-30 17:35:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.166 | 0.171\n",
      "2023-12-30 17:35:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.163 | 0.169\n",
      "2023-12-30 17:35:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.178 | 0.165\n",
      "2023-12-30 17:35:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.159 | 0.163\n",
      "2023-12-30 17:35:23 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.165 | 0.161\n",
      "2023-12-30 17:35:23 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.170 | 0.155\n",
      "2023-12-30 17:35:23 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.165 | 0.161\n",
      "2023-12-30 17:35:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.148 | 0.166\n",
      "2023-12-30 17:35:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.171 | 0.149\n",
      "2023-12-30 17:35:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.140 | 0.146\n",
      "2023-12-30 17:35:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.118 | 0.144\n",
      "2023-12-30 17:35:25 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.152 | 0.139\n",
      "2023-12-30 17:35:25 - INFO     | Early stopping: loss decreased (inf -> 0.137; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:08<02:48,  8.88s/it]2023-12-30 17:35:25 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 17:35:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.097 | 0.137\n",
      "2023-12-30 17:35:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.128 | 0.142\n",
      "2023-12-30 17:35:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.109 | 0.134\n",
      "2023-12-30 17:35:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.151 | 0.134\n",
      "2023-12-30 17:35:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.130 | 0.132\n",
      "2023-12-30 17:35:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.109 | 0.128\n",
      "2023-12-30 17:35:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.124 | 0.130\n",
      "2023-12-30 17:35:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.104 | 0.127\n",
      "2023-12-30 17:35:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.142 | 0.135\n",
      "2023-12-30 17:35:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.114 | 0.125\n",
      "2023-12-30 17:35:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.100 | 0.126\n",
      "2023-12-30 17:35:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.111 | 0.127\n",
      "2023-12-30 17:35:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.113 | 0.124\n",
      "2023-12-30 17:35:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.100 | 0.123\n",
      "2023-12-30 17:35:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.107 | 0.120\n",
      "2023-12-30 17:35:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.118 | 0.114\n",
      "2023-12-30 17:35:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.105 | 0.118\n",
      "2023-12-30 17:35:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.086 | 0.120\n",
      "2023-12-30 17:35:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.115 | 0.113\n",
      "2023-12-30 17:35:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.127 | 0.114\n",
      "2023-12-30 17:35:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.115 | 0.110\n",
      "2023-12-30 17:35:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.090 | 0.108\n",
      "2023-12-30 17:35:32 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.108 | 0.109\n",
      "2023-12-30 17:35:32 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.109 | 0.115\n",
      "2023-12-30 17:35:32 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.090 | 0.112\n",
      "2023-12-30 17:35:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.099 | 0.104\n",
      "2023-12-30 17:35:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.097 | 0.105\n",
      "2023-12-30 17:35:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.097 | 0.105\n",
      "2023-12-30 17:35:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.092 | 0.106\n",
      "2023-12-30 17:35:34 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.099 | 0.112\n",
      "2023-12-30 17:35:34 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.105 | 0.099\n",
      "2023-12-30 17:35:34 - INFO     | Early stopping: loss decreased (0.137 -> 0.099; -28.0%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:43,  9.07s/it]2023-12-30 17:35:34 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 17:35:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.065 | 0.098\n",
      "2023-12-30 17:35:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.079 | 0.099\n",
      "2023-12-30 17:35:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.087 | 0.098\n",
      "2023-12-30 17:35:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.097 | 0.096\n",
      "2023-12-30 17:35:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.096 | 0.098\n",
      "2023-12-30 17:35:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.089 | 0.099\n",
      "2023-12-30 17:35:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.098 | 0.095\n",
      "2023-12-30 17:35:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.097 | 0.095\n",
      "2023-12-30 17:35:37 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.093 | 0.101\n",
      "2023-12-30 17:35:37 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.085 | 0.098\n",
      "2023-12-30 17:35:37 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.111 | 0.099\n",
      "2023-12-30 17:35:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.079 | 0.092\n",
      "2023-12-30 17:35:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.078 | 0.095\n",
      "2023-12-30 17:35:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.086 | 0.097\n",
      "2023-12-30 17:35:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.081 | 0.093\n",
      "2023-12-30 17:35:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.080 | 0.096\n",
      "2023-12-30 17:35:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.078 | 0.091\n",
      "2023-12-30 17:35:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.082 | 0.098\n",
      "2023-12-30 17:35:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.068 | 0.091\n",
      "2023-12-30 17:35:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.069 | 0.093\n",
      "2023-12-30 17:35:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.093 | 0.088\n",
      "2023-12-30 17:35:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.074 | 0.094\n",
      "2023-12-30 17:35:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.082 | 0.091\n",
      "2023-12-30 17:35:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.069 | 0.088\n",
      "2023-12-30 17:35:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.092 | 0.088\n",
      "2023-12-30 17:35:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.084 | 0.089\n",
      "2023-12-30 17:35:42 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.100 | 0.089\n",
      "2023-12-30 17:35:42 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.082 | 0.086\n",
      "2023-12-30 17:35:42 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.072 | 0.088\n",
      "2023-12-30 17:35:43 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.068 | 0.091\n",
      "2023-12-30 17:35:43 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.066 | 0.086\n",
      "2023-12-30 17:35:43 - INFO     | Early stopping: loss decreased (0.099 -> 0.084; -15.4%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:26<02:32,  8.97s/it]2023-12-30 17:35:43 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 17:35:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.014 | 0.082\n",
      "2023-12-30 17:35:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.078 | 0.083\n",
      "2023-12-30 17:35:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.073 | 0.084\n",
      "2023-12-30 17:35:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.062 | 0.083\n",
      "2023-12-30 17:35:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.080 | 0.085\n",
      "2023-12-30 17:35:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.070 | 0.087\n",
      "2023-12-30 17:35:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.080 | 0.089\n",
      "2023-12-30 17:35:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.097 | 0.085\n",
      "2023-12-30 17:35:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.075 | 0.083\n",
      "2023-12-30 17:35:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.089 | 0.084\n",
      "2023-12-30 17:35:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.058 | 0.085\n",
      "2023-12-30 17:35:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.067 | 0.081\n",
      "2023-12-30 17:35:47 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.071 | 0.084\n",
      "2023-12-30 17:35:47 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.057 | 0.082\n",
      "2023-12-30 17:35:47 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.077 | 0.081\n",
      "2023-12-30 17:35:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.072 | 0.084\n",
      "2023-12-30 17:35:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.080 | 0.086\n",
      "2023-12-30 17:35:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.084 | 0.079\n",
      "2023-12-30 17:35:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.072 | 0.079\n",
      "2023-12-30 17:35:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.064 | 0.086\n",
      "2023-12-30 17:35:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.063 | 0.087\n",
      "2023-12-30 17:35:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.067 | 0.076\n",
      "2023-12-30 17:35:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.061 | 0.084\n",
      "2023-12-30 17:35:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.053 | 0.080\n",
      "2023-12-30 17:35:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.073 | 0.077\n",
      "2023-12-30 17:35:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.072 | 0.082\n",
      "2023-12-30 17:35:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.061 | 0.077\n",
      "2023-12-30 17:35:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.064 | 0.080\n",
      "2023-12-30 17:35:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.072 | 0.081\n",
      "2023-12-30 17:35:52 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.075 | 0.078\n",
      "2023-12-30 17:35:52 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.053 | 0.075\n",
      "2023-12-30 17:35:52 - INFO     | Early stopping: no decrease (0.084 vs 0.081); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:35<02:23,  8.99s/it]2023-12-30 17:35:52 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 17:35:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.069 | 0.079\n",
      "2023-12-30 17:35:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.071 | 0.075\n",
      "2023-12-30 17:35:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.065 | 0.080\n",
      "2023-12-30 17:35:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.067 | 0.074\n",
      "2023-12-30 17:35:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.055 | 0.085\n",
      "2023-12-30 17:35:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.055 | 0.086\n",
      "2023-12-30 17:35:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.070 | 0.081\n",
      "2023-12-30 17:35:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.057 | 0.075\n",
      "2023-12-30 17:35:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.059 | 0.075\n",
      "2023-12-30 17:35:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.068 | 0.081\n",
      "2023-12-30 17:35:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.051 | 0.077\n",
      "2023-12-30 17:35:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.061 | 0.072\n",
      "2023-12-30 17:35:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.057 | 0.073\n",
      "2023-12-30 17:35:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.046 | 0.073\n",
      "2023-12-30 17:35:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.057 | 0.075\n",
      "2023-12-30 17:35:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.040 | 0.075\n",
      "2023-12-30 17:35:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.071 | 0.073\n",
      "2023-12-30 17:35:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.073 | 0.074\n",
      "2023-12-30 17:35:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.065 | 0.072\n",
      "2023-12-30 17:35:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.050 | 0.075\n",
      "2023-12-30 17:35:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.060 | 0.073\n",
      "2023-12-30 17:35:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.058 | 0.071\n",
      "2023-12-30 17:35:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.060 | 0.073\n",
      "2023-12-30 17:35:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.063 | 0.075\n",
      "2023-12-30 17:35:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.065 | 0.072\n",
      "2023-12-30 17:35:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.069 | 0.071\n",
      "2023-12-30 17:36:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.079 | 0.073\n",
      "2023-12-30 17:36:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.064 | 0.069\n",
      "2023-12-30 17:36:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.061 | 0.073\n",
      "2023-12-30 17:36:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.067 | 0.069\n",
      "2023-12-30 17:36:01 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.048 | 0.072\n",
      "2023-12-30 17:36:01 - INFO     | Early stopping: loss decreased (0.084 -> 0.071; -14.5%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:44<02:14,  8.96s/it]2023-12-30 17:36:01 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.172 | 0.072\n",
      "2023-12-30 17:36:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.046 | 0.074\n",
      "2023-12-30 17:36:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.051 | 0.073\n",
      "2023-12-30 17:36:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.055 | 0.070\n",
      "2023-12-30 17:36:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.055 | 0.070\n",
      "2023-12-30 17:36:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.059 | 0.081\n",
      "2023-12-30 17:36:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.067 | 0.080\n",
      "2023-12-30 17:36:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.057 | 0.071\n",
      "2023-12-30 17:36:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.047 | 0.070\n",
      "2023-12-30 17:36:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.048 | 0.071\n",
      "2023-12-30 17:36:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.080 | 0.070\n",
      "2023-12-30 17:36:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.059 | 0.069\n",
      "2023-12-30 17:36:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.050 | 0.072\n",
      "2023-12-30 17:36:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.056 | 0.070\n",
      "2023-12-30 17:36:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.036 | 0.067\n",
      "2023-12-30 17:36:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.051 | 0.073\n",
      "2023-12-30 17:36:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.065 | 0.069\n",
      "2023-12-30 17:36:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.056 | 0.070\n",
      "2023-12-30 17:36:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.059 | 0.071\n",
      "2023-12-30 17:36:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.037 | 0.069\n",
      "2023-12-30 17:36:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.052 | 0.070\n",
      "2023-12-30 17:36:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.074 | 0.069\n",
      "2023-12-30 17:36:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.064 | 0.075\n",
      "2023-12-30 17:36:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.043 | 0.068\n",
      "2023-12-30 17:36:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.042 | 0.071\n",
      "2023-12-30 17:36:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.063 | 0.066\n",
      "2023-12-30 17:36:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.054 | 0.078\n",
      "2023-12-30 17:36:09 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.052 | 0.067\n",
      "2023-12-30 17:36:09 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.038 | 0.066\n",
      "2023-12-30 17:36:09 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.066 | 0.066\n",
      "2023-12-30 17:36:10 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.059 | 0.067\n",
      "2023-12-30 17:36:10 - INFO     | Early stopping: loss decreased (0.071 -> 0.066; -7.6%). Caching model state.\n",
      " 30%|███       | 6/20 [00:53<02:04,  8.91s/it]2023-12-30 17:36:10 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.065 | 0.067\n",
      "2023-12-30 17:36:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.045 | 0.065\n",
      "2023-12-30 17:36:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.048 | 0.065\n",
      "2023-12-30 17:36:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.061 | 0.063\n",
      "2023-12-30 17:36:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.047 | 0.070\n",
      "2023-12-30 17:36:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.056 | 0.068\n",
      "2023-12-30 17:36:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.055 | 0.066\n",
      "2023-12-30 17:36:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.038 | 0.066\n",
      "2023-12-30 17:36:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.043 | 0.068\n",
      "2023-12-30 17:36:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.048 | 0.064\n",
      "2023-12-30 17:36:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.041 | 0.065\n",
      "2023-12-30 17:36:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.035 | 0.066\n",
      "2023-12-30 17:36:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.051 | 0.065\n",
      "2023-12-30 17:36:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.043 | 0.064\n",
      "2023-12-30 17:36:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.051 | 0.066\n",
      "2023-12-30 17:36:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.041 | 0.068\n",
      "2023-12-30 17:36:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.059 | 0.063\n",
      "2023-12-30 17:36:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.060 | 0.066\n",
      "2023-12-30 17:36:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.039 | 0.067\n",
      "2023-12-30 17:36:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.035 | 0.065\n",
      "2023-12-30 17:36:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.059 | 0.068\n",
      "2023-12-30 17:36:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.062 | 0.067\n",
      "2023-12-30 17:36:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.048 | 0.068\n",
      "2023-12-30 17:36:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.052 | 0.071\n",
      "2023-12-30 17:36:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.057 | 0.073\n",
      "2023-12-30 17:36:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.052 | 0.066\n",
      "2023-12-30 17:36:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.037 | 0.065\n",
      "2023-12-30 17:36:18 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.067 | 0.063\n",
      "2023-12-30 17:36:18 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.049 | 0.065\n",
      "2023-12-30 17:36:18 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.054 | 0.071\n",
      "2023-12-30 17:36:19 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.058 | 0.070\n",
      "2023-12-30 17:36:19 - INFO     | Early stopping: no decrease (0.066 vs 0.068); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:02<01:56,  8.95s/it]2023-12-30 17:36:19 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.070 | 0.067\n",
      "2023-12-30 17:36:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.039 | 0.064\n",
      "2023-12-30 17:36:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.043 | 0.065\n",
      "2023-12-30 17:36:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.046 | 0.067\n",
      "2023-12-30 17:36:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.046 | 0.067\n",
      "2023-12-30 17:36:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.051 | 0.066\n",
      "2023-12-30 17:36:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.052 | 0.068\n",
      "2023-12-30 17:36:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.046 | 0.066\n",
      "2023-12-30 17:36:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.043 | 0.063\n",
      "2023-12-30 17:36:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.055 | 0.068\n",
      "2023-12-30 17:36:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.038 | 0.064\n",
      "2023-12-30 17:36:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.050 | 0.063\n",
      "2023-12-30 17:36:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.056 | 0.066\n",
      "2023-12-30 17:36:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.039 | 0.066\n",
      "2023-12-30 17:36:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.036 | 0.069\n",
      "2023-12-30 17:36:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.055 | 0.062\n",
      "2023-12-30 17:36:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.047 | 0.066\n",
      "2023-12-30 17:36:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.041 | 0.061\n",
      "2023-12-30 17:36:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.037 | 0.062\n",
      "2023-12-30 17:36:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.042 | 0.070\n",
      "2023-12-30 17:36:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.061 | 0.065\n",
      "2023-12-30 17:36:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.037 | 0.064\n",
      "2023-12-30 17:36:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.044 | 0.061\n",
      "2023-12-30 17:36:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.053 | 0.063\n",
      "2023-12-30 17:36:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.039 | 0.062\n",
      "2023-12-30 17:36:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.040 | 0.061\n",
      "2023-12-30 17:36:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.044 | 0.061\n",
      "2023-12-30 17:36:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.043 | 0.062\n",
      "2023-12-30 17:36:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.047 | 0.063\n",
      "2023-12-30 17:36:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.053 | 0.066\n",
      "2023-12-30 17:36:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.049 | 0.064\n",
      "2023-12-30 17:36:28 - INFO     | Early stopping: loss decreased (0.066 -> 0.062; -5.6%). Caching model state.\n",
      " 40%|████      | 8/20 [01:11<01:46,  8.88s/it]2023-12-30 17:36:28 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.127 | 0.063\n",
      "2023-12-30 17:36:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.034 | 0.061\n",
      "2023-12-30 17:36:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.045 | 0.067\n",
      "2023-12-30 17:36:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.056 | 0.063\n",
      "2023-12-30 17:36:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.040 | 0.062\n",
      "2023-12-30 17:36:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.035 | 0.062\n",
      "2023-12-30 17:36:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.030 | 0.061\n",
      "2023-12-30 17:36:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.040 | 0.067\n",
      "2023-12-30 17:36:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.048 | 0.068\n",
      "2023-12-30 17:36:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.031 | 0.062\n",
      "2023-12-30 17:36:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.043 | 0.064\n",
      "2023-12-30 17:36:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.045 | 0.065\n",
      "2023-12-30 17:36:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.055 | 0.068\n",
      "2023-12-30 17:36:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.047 | 0.062\n",
      "2023-12-30 17:36:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.041 | 0.061\n",
      "2023-12-30 17:36:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.035 | 0.062\n",
      "2023-12-30 17:36:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.057 | 0.063\n",
      "2023-12-30 17:36:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.045 | 0.061\n",
      "2023-12-30 17:36:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.039 | 0.062\n",
      "2023-12-30 17:36:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.047 | 0.060\n",
      "2023-12-30 17:36:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.038 | 0.064\n",
      "2023-12-30 17:36:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.039 | 0.060\n",
      "2023-12-30 17:36:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.043 | 0.064\n",
      "2023-12-30 17:36:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.042 | 0.062\n",
      "2023-12-30 17:36:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.055 | 0.062\n",
      "2023-12-30 17:36:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.031 | 0.061\n",
      "2023-12-30 17:36:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.049 | 0.061\n",
      "2023-12-30 17:36:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.037 | 0.062\n",
      "2023-12-30 17:36:36 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.030 | 0.060\n",
      "2023-12-30 17:36:36 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.043 | 0.059\n",
      "2023-12-30 17:36:36 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.032 | 0.060\n",
      "2023-12-30 17:36:36 - INFO     | Early stopping: no decrease (0.062 vs 0.063); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:20<01:37,  8.87s/it]2023-12-30 17:36:36 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.008 | 0.061\n",
      "2023-12-30 17:36:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.038 | 0.061\n",
      "2023-12-30 17:36:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.035 | 0.063\n",
      "2023-12-30 17:36:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.044 | 0.060\n",
      "2023-12-30 17:36:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.036 | 0.061\n",
      "2023-12-30 17:36:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.052 | 0.061\n",
      "2023-12-30 17:36:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.024 | 0.059\n",
      "2023-12-30 17:36:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.042 | 0.061\n",
      "2023-12-30 17:36:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.037 | 0.065\n",
      "2023-12-30 17:36:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.063 | 0.061\n",
      "2023-12-30 17:36:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.031 | 0.059\n",
      "2023-12-30 17:36:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.023 | 0.061\n",
      "2023-12-30 17:36:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.035 | 0.062\n",
      "2023-12-30 17:36:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.042 | 0.060\n",
      "2023-12-30 17:36:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.054 | 0.063\n",
      "2023-12-30 17:36:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.030 | 0.064\n",
      "2023-12-30 17:36:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.037 | 0.061\n",
      "2023-12-30 17:36:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.048 | 0.061\n",
      "2023-12-30 17:36:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.048 | 0.062\n",
      "2023-12-30 17:36:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.039 | 0.061\n",
      "2023-12-30 17:36:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.036 | 0.060\n",
      "2023-12-30 17:36:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.031 | 0.065\n",
      "2023-12-30 17:36:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.035 | 0.062\n",
      "2023-12-30 17:36:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.043 | 0.059\n",
      "2023-12-30 17:36:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.040 | 0.067\n",
      "2023-12-30 17:36:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.025 | 0.059\n",
      "2023-12-30 17:36:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.044 | 0.063\n",
      "2023-12-30 17:36:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.042 | 0.060\n",
      "2023-12-30 17:36:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.051 | 0.061\n",
      "2023-12-30 17:36:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.040 | 0.060\n",
      "2023-12-30 17:36:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.032 | 0.068\n",
      "2023-12-30 17:36:45 - INFO     | Early stopping: no decrease (0.062 vs 0.064); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:29<01:28,  8.87s/it]2023-12-30 17:36:45 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.012 | 0.064\n",
      "2023-12-30 17:36:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.029 | 0.060\n",
      "2023-12-30 17:36:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.037 | 0.060\n",
      "2023-12-30 17:36:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.031 | 0.061\n",
      "2023-12-30 17:36:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.032 | 0.059\n",
      "2023-12-30 17:36:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.041 | 0.059\n",
      "2023-12-30 17:36:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.030 | 0.060\n",
      "2023-12-30 17:36:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.046 | 0.063\n",
      "2023-12-30 17:36:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.034 | 0.065\n",
      "2023-12-30 17:36:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.038 | 0.062\n",
      "2023-12-30 17:36:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.034 | 0.063\n",
      "2023-12-30 17:36:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.042 | 0.059\n",
      "2023-12-30 17:36:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.036 | 0.061\n",
      "2023-12-30 17:36:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.040 | 0.063\n",
      "2023-12-30 17:36:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.039 | 0.060\n",
      "2023-12-30 17:36:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.031 | 0.060\n",
      "2023-12-30 17:36:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.044 | 0.061\n",
      "2023-12-30 17:36:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.045 | 0.059\n",
      "2023-12-30 17:36:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.030 | 0.059\n",
      "2023-12-30 17:36:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.042 | 0.059\n",
      "2023-12-30 17:36:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.050 | 0.062\n",
      "2023-12-30 17:36:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.041 | 0.057\n",
      "2023-12-30 17:36:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.037 | 0.062\n",
      "2023-12-30 17:36:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.049 | 0.062\n",
      "2023-12-30 17:36:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.033 | 0.058\n",
      "2023-12-30 17:36:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.028 | 0.064\n",
      "2023-12-30 17:36:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.025 | 0.058\n",
      "2023-12-30 17:36:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.037 | 0.059\n",
      "2023-12-30 17:36:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.032 | 0.059\n",
      "2023-12-30 17:36:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.042 | 0.059\n",
      "2023-12-30 17:36:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.029 | 0.058\n",
      "2023-12-30 17:36:54 - INFO     | Early stopping: no decrease (0.062 vs 0.061); counter: 3 out of 3\n",
      "2023-12-30 17:36:54 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:36:54 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 55%|█████▌    | 11/20 [01:37<01:19,  8.85s/it]2023-12-30 17:36:54 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 17:36:54 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.014 | 0.060\n",
      "2023-12-30 17:36:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.025 | 0.059\n",
      "2023-12-30 17:36:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.044 | 0.058\n",
      "2023-12-30 17:36:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.034 | 0.057\n",
      "2023-12-30 17:36:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.037 | 0.058\n",
      "2023-12-30 17:36:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.030 | 0.058\n",
      "2023-12-30 17:36:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.042 | 0.056\n",
      "2023-12-30 17:36:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.041 | 0.056\n",
      "2023-12-30 17:36:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.032 | 0.056\n",
      "2023-12-30 17:36:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.037 | 0.057\n",
      "2023-12-30 17:36:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.028 | 0.058\n",
      "2023-12-30 17:36:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.030 | 0.058\n",
      "2023-12-30 17:36:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.014 | 0.057\n",
      "2023-12-30 17:36:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.023 | 0.057\n",
      "2023-12-30 17:36:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.027 | 0.057\n",
      "2023-12-30 17:36:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.032 | 0.057\n",
      "2023-12-30 17:36:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.031 | 0.058\n",
      "2023-12-30 17:36:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.036 | 0.056\n",
      "2023-12-30 17:36:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.029 | 0.057\n",
      "2023-12-30 17:37:00 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.023 | 0.056\n",
      "2023-12-30 17:37:00 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.035 | 0.056\n",
      "2023-12-30 17:37:00 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.036 | 0.056\n",
      "2023-12-30 17:37:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.028 | 0.056\n",
      "2023-12-30 17:37:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.024 | 0.057\n",
      "2023-12-30 17:37:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.033 | 0.058\n",
      "2023-12-30 17:37:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.046 | 0.059\n",
      "2023-12-30 17:37:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.034 | 0.058\n",
      "2023-12-30 17:37:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.029 | 0.057\n",
      "2023-12-30 17:37:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.030 | 0.057\n",
      "2023-12-30 17:37:03 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.031 | 0.058\n",
      "2023-12-30 17:37:03 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.046 | 0.057\n",
      "2023-12-30 17:37:03 - INFO     | Early stopping: loss decreased (0.062 -> 0.058; -7.2%). Caching model state.\n",
      " 60%|██████    | 12/20 [01:46<01:11,  8.90s/it]2023-12-30 17:37:03 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 17:37:03 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.055 | 0.058\n",
      "2023-12-30 17:37:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.036 | 0.058\n",
      "2023-12-30 17:37:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.036 | 0.057\n",
      "2023-12-30 17:37:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.022 | 0.057\n",
      "2023-12-30 17:37:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.023 | 0.058\n",
      "2023-12-30 17:37:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.025 | 0.057\n",
      "2023-12-30 17:37:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.032 | 0.057\n",
      "2023-12-30 17:37:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.027 | 0.057\n",
      "2023-12-30 17:37:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.031 | 0.057\n",
      "2023-12-30 17:37:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.025 | 0.056\n",
      "2023-12-30 17:37:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.045 | 0.057\n",
      "2023-12-30 17:37:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.030 | 0.058\n",
      "2023-12-30 17:37:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.025 | 0.057\n",
      "2023-12-30 17:37:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.030 | 0.057\n",
      "2023-12-30 17:37:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.029 | 0.056\n",
      "2023-12-30 17:37:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.029 | 0.058\n",
      "2023-12-30 17:37:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.037 | 0.061\n",
      "2023-12-30 17:37:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.028 | 0.058\n",
      "2023-12-30 17:37:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.029 | 0.056\n",
      "2023-12-30 17:37:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.041 | 0.057\n",
      "2023-12-30 17:37:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.029 | 0.057\n",
      "2023-12-30 17:37:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.028 | 0.057\n",
      "2023-12-30 17:37:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.036 | 0.056\n",
      "2023-12-30 17:37:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.033 | 0.056\n",
      "2023-12-30 17:37:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.036 | 0.055\n",
      "2023-12-30 17:37:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.033 | 0.056\n",
      "2023-12-30 17:37:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.031 | 0.056\n",
      "2023-12-30 17:37:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.022 | 0.056\n",
      "2023-12-30 17:37:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.034 | 0.056\n",
      "2023-12-30 17:37:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.043 | 0.056\n",
      "2023-12-30 17:37:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.026 | 0.057\n",
      "2023-12-30 17:37:12 - INFO     | Early stopping: no decrease (0.058 vs 0.057); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [01:55<01:02,  8.90s/it]2023-12-30 17:37:12 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 17:37:12 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.017 | 0.057\n",
      "2023-12-30 17:37:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.029 | 0.056\n",
      "2023-12-30 17:37:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.023 | 0.056\n",
      "2023-12-30 17:37:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.027 | 0.056\n",
      "2023-12-30 17:37:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.052 | 0.057\n",
      "2023-12-30 17:37:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.025 | 0.058\n",
      "2023-12-30 17:37:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.028 | 0.059\n",
      "2023-12-30 17:37:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.023 | 0.057\n",
      "2023-12-30 17:37:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.027 | 0.056\n",
      "2023-12-30 17:37:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.032 | 0.058\n",
      "2023-12-30 17:37:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.023 | 0.057\n",
      "2023-12-30 17:37:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.027 | 0.057\n",
      "2023-12-30 17:37:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.027 | 0.057\n",
      "2023-12-30 17:37:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.030 | 0.057\n",
      "2023-12-30 17:37:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.032 | 0.056\n",
      "2023-12-30 17:37:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.018 | 0.056\n",
      "2023-12-30 17:37:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.022 | 0.056\n",
      "2023-12-30 17:37:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.028 | 0.057\n",
      "2023-12-30 17:37:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.027 | 0.056\n",
      "2023-12-30 17:37:18 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.031 | 0.056\n",
      "2023-12-30 17:37:18 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.042 | 0.057\n",
      "2023-12-30 17:37:18 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.029 | 0.057\n",
      "2023-12-30 17:37:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.044 | 0.055\n",
      "2023-12-30 17:37:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.026 | 0.058\n",
      "2023-12-30 17:37:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.028 | 0.056\n",
      "2023-12-30 17:37:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.036 | 0.057\n",
      "2023-12-30 17:37:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.053 | 0.058\n",
      "2023-12-30 17:37:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.027 | 0.056\n",
      "2023-12-30 17:37:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.029 | 0.057\n",
      "2023-12-30 17:37:21 - INFO     | Early stopping: no decrease (0.058 vs 0.056); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:04<00:53,  8.86s/it]2023-12-30 17:37:21 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 17:37:21 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.046 | 0.056\n",
      "2023-12-30 17:37:21 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.021 | 0.056\n",
      "2023-12-30 17:37:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.045 | 0.056\n",
      "2023-12-30 17:37:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.024 | 0.056\n",
      "2023-12-30 17:37:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.032 | 0.057\n",
      "2023-12-30 17:37:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.022 | 0.055\n",
      "2023-12-30 17:37:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.031 | 0.056\n",
      "2023-12-30 17:37:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.026 | 0.057\n",
      "2023-12-30 17:37:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.019 | 0.055\n",
      "2023-12-30 17:37:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.028 | 0.057\n",
      "2023-12-30 17:37:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.028 | 0.056\n",
      "2023-12-30 17:37:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.039 | 0.057\n",
      "2023-12-30 17:37:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.025 | 0.056\n",
      "2023-12-30 17:37:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.037 | 0.058\n",
      "2023-12-30 17:37:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.028 | 0.057\n",
      "2023-12-30 17:37:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.035 | 0.055\n",
      "2023-12-30 17:37:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.034 | 0.057\n",
      "2023-12-30 17:37:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.030 | 0.056\n",
      "2023-12-30 17:37:27 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.033 | 0.056\n",
      "2023-12-30 17:37:27 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:27 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.021 | 0.054\n",
      "2023-12-30 17:37:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.031 | 0.056\n",
      "2023-12-30 17:37:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.032 | 0.055\n",
      "2023-12-30 17:37:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.031 | 0.056\n",
      "2023-12-30 17:37:29 - INFO     | Early stopping: no decrease (0.058 vs 0.056); counter: 3 out of 3\n",
      "2023-12-30 17:37:29 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:37:29 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 75%|███████▌  | 15/20 [02:13<00:44,  8.81s/it]2023-12-30 17:37:29 - INFO     | Epoch: 15 | Learning Rate: 0.000\n",
      "2023-12-30 17:37:30 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 840064 examples: 0.062 | 0.056\n",
      "2023-12-30 17:37:30 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 841920 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:30 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 843776 examples: 0.023 | 0.056\n",
      "2023-12-30 17:37:31 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 845632 examples: 0.021 | 0.056\n",
      "2023-12-30 17:37:31 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 847488 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:31 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 849344 examples: 0.020 | 0.055\n",
      "2023-12-30 17:37:31 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 851200 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:32 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 853056 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:32 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 854912 examples: 0.037 | 0.055\n",
      "2023-12-30 17:37:32 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 856768 examples: 0.034 | 0.055\n",
      "2023-12-30 17:37:33 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 858624 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:33 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 860480 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:33 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 862336 examples: 0.021 | 0.055\n",
      "2023-12-30 17:37:34 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 864192 examples: 0.038 | 0.055\n",
      "2023-12-30 17:37:34 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 866048 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:34 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 867904 examples: 0.038 | 0.055\n",
      "2023-12-30 17:37:34 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 869760 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:35 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 871616 examples: 0.025 | 0.055\n",
      "2023-12-30 17:37:35 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 873472 examples: 0.019 | 0.055\n",
      "2023-12-30 17:37:35 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 875328 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:35 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 877184 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:36 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 879040 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:36 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 880896 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:36 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 882752 examples: 0.025 | 0.055\n",
      "2023-12-30 17:37:37 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 884608 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:37 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 886464 examples: 0.022 | 0.055\n",
      "2023-12-30 17:37:37 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 888320 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:37 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 890176 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:38 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 892032 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:38 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 893888 examples: 0.032 | 0.055\n",
      "2023-12-30 17:37:38 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 895744 examples: 0.032 | 0.055\n",
      "2023-12-30 17:37:39 - INFO     | Early stopping: no decrease (0.058 vs 0.055); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:22<00:35,  8.89s/it]2023-12-30 17:37:39 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 17:37:39 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.060 | 0.056\n",
      "2023-12-30 17:37:39 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.026 | 0.055\n",
      "2023-12-30 17:37:39 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:40 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.025 | 0.056\n",
      "2023-12-30 17:37:40 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.022 | 0.056\n",
      "2023-12-30 17:37:40 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.022 | 0.055\n",
      "2023-12-30 17:37:41 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.031 | 0.056\n",
      "2023-12-30 17:37:41 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.033 | 0.055\n",
      "2023-12-30 17:37:41 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.022 | 0.055\n",
      "2023-12-30 17:37:41 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.019 | 0.055\n",
      "2023-12-30 17:37:42 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.026 | 0.056\n",
      "2023-12-30 17:37:42 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:42 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:43 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:43 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:43 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:43 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.028 | 0.056\n",
      "2023-12-30 17:37:44 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:44 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:44 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:44 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.026 | 0.056\n",
      "2023-12-30 17:37:45 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:45 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:45 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:45 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.028 | 0.055\n",
      "2023-12-30 17:37:46 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:46 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:46 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.021 | 0.055\n",
      "2023-12-30 17:37:47 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.026 | 0.055\n",
      "2023-12-30 17:37:47 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.035 | 0.055\n",
      "2023-12-30 17:37:47 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:47 - INFO     | Early stopping: no decrease (0.058 vs 0.055); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:31<00:26,  8.89s/it]2023-12-30 17:37:47 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 17:37:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.034 | 0.055\n",
      "2023-12-30 17:37:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.021 | 0.055\n",
      "2023-12-30 17:37:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.033 | 0.055\n",
      "2023-12-30 17:37:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.024 | 0.055\n",
      "2023-12-30 17:37:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.021 | 0.055\n",
      "2023-12-30 17:37:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.020 | 0.055\n",
      "2023-12-30 17:37:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.031 | 0.055\n",
      "2023-12-30 17:37:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.031 | 0.054\n",
      "2023-12-30 17:37:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.017 | 0.054\n",
      "2023-12-30 17:37:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.034 | 0.055\n",
      "2023-12-30 17:37:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.026 | 0.055\n",
      "2023-12-30 17:37:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.022 | 0.055\n",
      "2023-12-30 17:37:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.021 | 0.055\n",
      "2023-12-30 17:37:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.020 | 0.055\n",
      "2023-12-30 17:37:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.034 | 0.055\n",
      "2023-12-30 17:37:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.014 | 0.055\n",
      "2023-12-30 17:37:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.034 | 0.055\n",
      "2023-12-30 17:37:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.019 | 0.055\n",
      "2023-12-30 17:37:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.021 | 0.056\n",
      "2023-12-30 17:37:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.021 | 0.055\n",
      "2023-12-30 17:37:55 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.025 | 0.055\n",
      "2023-12-30 17:37:55 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.034 | 0.055\n",
      "2023-12-30 17:37:55 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.035 | 0.055\n",
      "2023-12-30 17:37:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.040 | 0.055\n",
      "2023-12-30 17:37:56 - INFO     | Early stopping: no decrease (0.058 vs 0.055); counter: 3 out of 3\n",
      "2023-12-30 17:37:56 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:37:56 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      " 90%|█████████ | 18/20 [02:40<00:17,  8.91s/it]2023-12-30 17:37:56 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 17:37:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.014 | 0.055\n",
      "2023-12-30 17:37:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.029 | 0.055\n",
      "2023-12-30 17:37:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.023 | 0.055\n",
      "2023-12-30 17:37:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.033 | 0.056\n",
      "2023-12-30 17:37:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.027 | 0.055\n",
      "2023-12-30 17:37:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.026 | 0.055\n",
      "2023-12-30 17:37:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.014 | 0.055\n",
      "2023-12-30 17:37:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.030 | 0.055\n",
      "2023-12-30 17:37:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.035 | 0.055\n",
      "2023-12-30 17:37:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.020 | 0.055\n",
      "2023-12-30 17:37:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.026 | 0.055\n",
      "2023-12-30 17:38:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.023 | 0.055\n",
      "2023-12-30 17:38:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.027 | 0.055\n",
      "2023-12-30 17:38:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.026 | 0.055\n",
      "2023-12-30 17:38:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.030 | 0.055\n",
      "2023-12-30 17:38:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.028 | 0.054\n",
      "2023-12-30 17:38:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.038 | 0.055\n",
      "2023-12-30 17:38:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.028 | 0.055\n",
      "2023-12-30 17:38:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.026 | 0.055\n",
      "2023-12-30 17:38:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.023 | 0.055\n",
      "2023-12-30 17:38:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.021 | 0.055\n",
      "2023-12-30 17:38:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.020 | 0.055\n",
      "2023-12-30 17:38:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.029 | 0.055\n",
      "2023-12-30 17:38:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.022 | 0.055\n",
      "2023-12-30 17:38:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.030 | 0.055\n",
      "2023-12-30 17:38:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.030 | 0.055\n",
      "2023-12-30 17:38:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.017 | 0.054\n",
      "2023-12-30 17:38:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.017 | 0.055\n",
      "2023-12-30 17:38:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.030 | 0.055\n",
      "2023-12-30 17:38:05 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.018 | 0.055\n",
      "2023-12-30 17:38:05 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.016 | 0.055\n",
      "2023-12-30 17:38:05 - INFO     | Early stopping: loss decreased (0.058 -> 0.055; -5.5%). Caching model state.\n",
      " 95%|█████████▌| 19/20 [02:48<00:08,  8.85s/it]2023-12-30 17:38:05 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 17:38:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.017 | 0.055\n",
      "2023-12-30 17:38:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.021 | 0.055\n",
      "2023-12-30 17:38:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.033 | 0.055\n",
      "2023-12-30 17:38:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.038 | 0.055\n",
      "2023-12-30 17:38:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.027 | 0.055\n",
      "2023-12-30 17:38:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.028 | 0.055\n",
      "2023-12-30 17:38:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.022 | 0.055\n",
      "2023-12-30 17:38:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.030 | 0.055\n",
      "2023-12-30 17:38:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.018 | 0.055\n",
      "2023-12-30 17:38:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.020 | 0.055\n",
      "2023-12-30 17:38:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.026 | 0.055\n",
      "2023-12-30 17:38:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.021 | 0.055\n",
      "2023-12-30 17:38:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.021 | 0.055\n",
      "2023-12-30 17:38:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.021 | 0.055\n",
      "2023-12-30 17:38:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.027 | 0.055\n",
      "2023-12-30 17:38:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.026 | 0.055\n",
      "2023-12-30 17:38:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.022 | 0.055\n",
      "2023-12-30 17:38:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.023 | 0.055\n",
      "2023-12-30 17:38:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.025 | 0.055\n",
      "2023-12-30 17:38:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.025 | 0.055\n",
      "2023-12-30 17:38:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.023 | 0.055\n",
      "2023-12-30 17:38:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.035 | 0.055\n",
      "2023-12-30 17:38:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.029 | 0.055\n",
      "2023-12-30 17:38:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.030 | 0.055\n",
      "2023-12-30 17:38:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.028 | 0.055\n",
      "2023-12-30 17:38:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.025 | 0.055\n",
      "2023-12-30 17:38:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.029 | 0.055\n",
      "2023-12-30 17:38:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.022 | 0.055\n",
      "2023-12-30 17:38:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.022 | 0.055\n",
      "2023-12-30 17:38:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.022 | 0.054\n",
      "2023-12-30 17:38:14 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.021 | 0.054\n",
      "2023-12-30 17:38:14 - INFO     | Early stopping: no decrease (0.055 vs 0.054); counter: 1 out of 3\n",
      "100%|██████████| 20/20 [02:57<00:00,  8.89s/it]\n",
      "2023-12-30 17:38:14 - INFO     | Best validation loss: 0.055\n",
      "2023-12-30 17:38:14 - INFO     | Best early stopping index/epoch: 18\n",
      "2023-12-30 17:38:14 - INFO     | Average Loss on test set: 0.057\n",
      "2023-12-30 17:38:16 - INFO     | Weighted Precision: 0.982, Recall: 0.982, F1: 0.982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>███████████▄▄▄▄▂▂▂▁▁</td></tr><tr><td>step_learning_rate</td><td>██████████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▇▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_validation_loss</td><td>0.05457</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00013</td></tr><tr><td>step_learning_rate</td><td>0.00013</td></tr><tr><td>step_training_loss</td><td>0.02054</td></tr><tr><td>step_validation_loss</td><td>0.05428</td></tr><tr><td>test_loss</td><td>0.05743</td></tr><tr><td>weighted_f1</td><td>0.98215</td></tr><tr><td>weighted_precision</td><td>0.98219</td></tr><tr><td>weighted_recall</td><td>0.98214</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-sweep-6</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/omrgjney' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/omrgjney</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_173516-omrgjney/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: je0h7ksw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 32]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_173828-je0h7ksw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/je0h7ksw' target=\"_blank\">hopeful-sweep-7</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/je0h7ksw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/je0h7ksw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 32], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:38:28 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 17:38:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 27.998 | 153.244\n",
      "2023-12-30 17:38:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 11.366 | 1.498\n",
      "2023-12-30 17:38:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 1.009 | 0.680\n",
      "2023-12-30 17:38:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 0.551 | 0.451\n",
      "2023-12-30 17:38:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 0.415 | 0.387\n",
      "2023-12-30 17:38:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 0.389 | 0.313\n",
      "2023-12-30 17:38:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 0.317 | 0.307\n",
      "2023-12-30 17:38:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 0.270 | 0.280\n",
      "2023-12-30 17:38:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 0.251 | 0.299\n",
      "2023-12-30 17:38:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 0.290 | 0.226\n",
      "2023-12-30 17:38:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 0.233 | 0.278\n",
      "2023-12-30 17:38:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 0.290 | 0.263\n",
      "2023-12-30 17:38:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 0.222 | 0.221\n",
      "2023-12-30 17:38:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.267 | 0.209\n",
      "2023-12-30 17:38:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.216 | 0.244\n",
      "2023-12-30 17:38:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.252 | 0.240\n",
      "2023-12-30 17:38:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.200 | 0.237\n",
      "2023-12-30 17:38:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.242 | 0.218\n",
      "2023-12-30 17:38:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.195 | 0.208\n",
      "2023-12-30 17:38:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.166 | 0.233\n",
      "2023-12-30 17:38:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.218 | 0.227\n",
      "2023-12-30 17:38:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.200 | 0.235\n",
      "2023-12-30 17:38:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.212 | 0.195\n",
      "2023-12-30 17:38:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.205 | 0.207\n",
      "2023-12-30 17:38:36 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.211 | 0.211\n",
      "2023-12-30 17:38:36 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.200 | 0.187\n",
      "2023-12-30 17:38:36 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.227 | 0.222\n",
      "2023-12-30 17:38:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.208 | 0.252\n",
      "2023-12-30 17:38:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.236 | 0.210\n",
      "2023-12-30 17:38:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.180 | 0.209\n",
      "2023-12-30 17:38:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.190 | 0.194\n",
      "2023-12-30 17:38:38 - INFO     | Early stopping: loss decreased (inf -> 0.173; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:57,  9.32s/it]2023-12-30 17:38:38 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 17:38:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.077 | 0.175\n",
      "2023-12-30 17:38:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.172 | 0.174\n",
      "2023-12-30 17:38:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.198 | 0.194\n",
      "2023-12-30 17:38:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.159 | 0.236\n",
      "2023-12-30 17:38:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.253 | 0.285\n",
      "2023-12-30 17:38:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.263 | 0.242\n",
      "2023-12-30 17:38:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.316 | 0.296\n",
      "2023-12-30 17:38:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.289 | 0.301\n",
      "2023-12-30 17:38:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.253 | 0.227\n",
      "2023-12-30 17:38:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.250 | 0.234\n",
      "2023-12-30 17:38:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.186 | 0.202\n",
      "2023-12-30 17:38:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.171 | 0.210\n",
      "2023-12-30 17:38:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.217 | 0.216\n",
      "2023-12-30 17:38:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.214 | 0.205\n",
      "2023-12-30 17:38:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.219 | 0.186\n",
      "2023-12-30 17:38:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.172 | 0.228\n",
      "2023-12-30 17:38:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.176 | 0.236\n",
      "2023-12-30 17:38:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.225 | 0.239\n",
      "2023-12-30 17:38:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.235 | 0.187\n",
      "2023-12-30 17:38:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.192 | 0.190\n",
      "2023-12-30 17:38:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.227 | 0.210\n",
      "2023-12-30 17:38:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.195 | 0.195\n",
      "2023-12-30 17:38:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.213 | 0.227\n",
      "2023-12-30 17:38:45 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.202 | 0.246\n",
      "2023-12-30 17:38:45 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.216 | 0.220\n",
      "2023-12-30 17:38:45 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.228 | 0.200\n",
      "2023-12-30 17:38:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.204 | 0.236\n",
      "2023-12-30 17:38:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.221 | 0.200\n",
      "2023-12-30 17:38:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.223 | 0.187\n",
      "2023-12-30 17:38:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.173 | 0.195\n",
      "2023-12-30 17:38:47 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.202 | 0.209\n",
      "2023-12-30 17:38:47 - INFO     | Early stopping: no decrease (0.173 vs 0.227); counter: 1 out of 3\n",
      " 10%|█         | 2/20 [00:18<02:48,  9.37s/it]2023-12-30 17:38:47 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 17:38:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.093 | 0.220\n",
      "2023-12-30 17:38:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.186 | 0.195\n",
      "2023-12-30 17:38:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.174 | 0.219\n",
      "2023-12-30 17:38:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.196 | 0.203\n",
      "2023-12-30 17:38:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.229 | 0.241\n",
      "2023-12-30 17:38:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.187 | 0.198\n",
      "2023-12-30 17:38:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.202 | 0.200\n",
      "2023-12-30 17:38:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.235 | 0.281\n",
      "2023-12-30 17:38:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.220 | 0.265\n",
      "2023-12-30 17:38:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.225 | 0.211\n",
      "2023-12-30 17:38:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.184 | 0.177\n",
      "2023-12-30 17:38:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.195 | 0.245\n",
      "2023-12-30 17:38:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.241 | 0.266\n",
      "2023-12-30 17:38:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.217 | 0.188\n",
      "2023-12-30 17:38:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.151 | 0.206\n",
      "2023-12-30 17:38:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.254 | 0.209\n",
      "2023-12-30 17:38:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.178 | 0.256\n",
      "2023-12-30 17:38:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.217 | 0.262\n",
      "2023-12-30 17:38:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.207 | 0.224\n",
      "2023-12-30 17:38:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.202 | 0.214\n",
      "2023-12-30 17:38:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.211 | 0.195\n",
      "2023-12-30 17:38:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.183 | 0.251\n",
      "2023-12-30 17:38:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.235 | 0.264\n",
      "2023-12-30 17:38:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.174 | 0.221\n",
      "2023-12-30 17:38:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.167 | 0.184\n",
      "2023-12-30 17:38:55 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.194 | 0.230\n",
      "2023-12-30 17:38:55 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.210 | 0.238\n",
      "2023-12-30 17:38:55 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.269 | 0.273\n",
      "2023-12-30 17:38:56 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.261 | 0.188\n",
      "2023-12-30 17:38:56 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.212 | 0.229\n",
      "2023-12-30 17:38:56 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.206 | 0.198\n",
      "2023-12-30 17:38:57 - INFO     | Early stopping: no decrease (0.173 vs 0.208); counter: 2 out of 3\n",
      " 15%|█▌        | 3/20 [00:28<02:39,  9.41s/it]2023-12-30 17:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 17:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.063 | 0.212\n",
      "2023-12-30 17:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.160 | 0.207\n",
      "2023-12-30 17:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.186 | 0.191\n",
      "2023-12-30 17:38:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.196 | 0.209\n",
      "2023-12-30 17:38:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.204 | 0.208\n",
      "2023-12-30 17:38:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.161 | 0.217\n",
      "2023-12-30 17:38:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.208 | 0.277\n",
      "2023-12-30 17:38:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.205 | 0.217\n",
      "2023-12-30 17:38:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.209 | 0.250\n",
      "2023-12-30 17:38:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.262 | 0.221\n",
      "2023-12-30 17:39:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.227 | 0.210\n",
      "2023-12-30 17:39:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.192 | 0.210\n",
      "2023-12-30 17:39:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.204 | 0.205\n",
      "2023-12-30 17:39:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.274 | 0.244\n",
      "2023-12-30 17:39:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.221 | 0.249\n",
      "2023-12-30 17:39:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.195 | 0.201\n",
      "2023-12-30 17:39:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.167 | 0.213\n",
      "2023-12-30 17:39:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.200 | 0.242\n",
      "2023-12-30 17:39:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.228 | 0.216\n",
      "2023-12-30 17:39:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.179 | 0.198\n",
      "2023-12-30 17:39:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.243 | 0.192\n",
      "2023-12-30 17:39:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.208 | 0.226\n",
      "2023-12-30 17:39:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.186 | 0.255\n",
      "2023-12-30 17:39:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.231 | 0.260\n",
      "2023-12-30 17:39:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.212 | 0.197\n",
      "2023-12-30 17:39:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.197 | 0.196\n",
      "2023-12-30 17:39:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.226 | 0.185\n",
      "2023-12-30 17:39:05 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.200 | 0.224\n",
      "2023-12-30 17:39:05 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.192 | 0.209\n",
      "2023-12-30 17:39:05 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.146 | 0.212\n",
      "2023-12-30 17:39:06 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.234 | 0.187\n",
      "2023-12-30 17:39:06 - INFO     | Early stopping: no decrease (0.173 vs 0.213); counter: 3 out of 3\n",
      "2023-12-30 17:39:06 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:39:06 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 20%|██        | 4/20 [00:37<02:30,  9.41s/it]2023-12-30 17:39:06 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 17:39:06 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.180 | 0.193\n",
      "2023-12-30 17:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.167 | 0.181\n",
      "2023-12-30 17:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.138 | 0.164\n",
      "2023-12-30 17:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.151 | 0.221\n",
      "2023-12-30 17:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.219 | 0.208\n",
      "2023-12-30 17:39:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.157 | 0.159\n",
      "2023-12-30 17:39:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.143 | 0.167\n",
      "2023-12-30 17:39:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.133 | 0.166\n",
      "2023-12-30 17:39:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.131 | 0.160\n",
      "2023-12-30 17:39:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.146 | 0.152\n",
      "2023-12-30 17:39:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.145 | 0.168\n",
      "2023-12-30 17:39:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.122 | 0.151\n",
      "2023-12-30 17:39:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.137 | 0.137\n",
      "2023-12-30 17:39:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.158 | 0.145\n",
      "2023-12-30 17:39:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.131 | 0.140\n",
      "2023-12-30 17:39:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.124 | 0.146\n",
      "2023-12-30 17:39:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.129 | 0.140\n",
      "2023-12-30 17:39:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.136 | 0.140\n",
      "2023-12-30 17:39:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.135 | 0.139\n",
      "2023-12-30 17:39:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.137 | 0.170\n",
      "2023-12-30 17:39:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.154 | 0.143\n",
      "2023-12-30 17:39:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.103 | 0.148\n",
      "2023-12-30 17:39:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.161 | 0.168\n",
      "2023-12-30 17:39:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.126 | 0.138\n",
      "2023-12-30 17:39:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.143 | 0.141\n",
      "2023-12-30 17:39:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.128 | 0.138\n",
      "2023-12-30 17:39:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.141 | 0.163\n",
      "2023-12-30 17:39:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.163 | 0.209\n",
      "2023-12-30 17:39:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.139 | 0.190\n",
      "2023-12-30 17:39:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.156 | 0.161\n",
      "2023-12-30 17:39:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.146 | 0.183\n",
      "2023-12-30 17:39:15 - INFO     | Early stopping: loss decreased (0.173 -> 0.161; -6.6%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:46<02:20,  9.34s/it]2023-12-30 17:39:15 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 17:39:15 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.324 | 0.152\n",
      "2023-12-30 17:39:16 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.128 | 0.135\n",
      "2023-12-30 17:39:16 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.141 | 0.148\n",
      "2023-12-30 17:39:16 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.179 | 0.216\n",
      "2023-12-30 17:39:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.147 | 0.185\n",
      "2023-12-30 17:39:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.172 | 0.166\n",
      "2023-12-30 17:39:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.140 | 0.189\n",
      "2023-12-30 17:39:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.127 | 0.167\n",
      "2023-12-30 17:39:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.138 | 0.164\n",
      "2023-12-30 17:39:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.141 | 0.158\n",
      "2023-12-30 17:39:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.134 | 0.164\n",
      "2023-12-30 17:39:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.131 | 0.151\n",
      "2023-12-30 17:39:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.131 | 0.166\n",
      "2023-12-30 17:39:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.142 | 0.143\n",
      "2023-12-30 17:39:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.105 | 0.142\n",
      "2023-12-30 17:39:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.114 | 0.194\n",
      "2023-12-30 17:39:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.131 | 0.138\n",
      "2023-12-30 17:39:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.127 | 0.146\n",
      "2023-12-30 17:39:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.110 | 0.131\n",
      "2023-12-30 17:39:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.141 | 0.144\n",
      "2023-12-30 17:39:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.121 | 0.133\n",
      "2023-12-30 17:39:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.113 | 0.147\n",
      "2023-12-30 17:39:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.139 | 0.158\n",
      "2023-12-30 17:39:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.125 | 0.211\n",
      "2023-12-30 17:39:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.180 | 0.166\n",
      "2023-12-30 17:39:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.146 | 0.143\n",
      "2023-12-30 17:39:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.098 | 0.137\n",
      "2023-12-30 17:39:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.148 | 0.159\n",
      "2023-12-30 17:39:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.127 | 0.164\n",
      "2023-12-30 17:39:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.143 | 0.163\n",
      "2023-12-30 17:39:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.133 | 0.140\n",
      "2023-12-30 17:39:24 - INFO     | Early stopping: loss decreased (0.161 -> 0.132; -18.1%). Caching model state.\n",
      " 30%|███       | 6/20 [00:56<02:10,  9.34s/it]2023-12-30 17:39:24 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 17:39:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.182 | 0.137\n",
      "2023-12-30 17:39:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.135 | 0.145\n",
      "2023-12-30 17:39:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.111 | 0.140\n",
      "2023-12-30 17:39:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.115 | 0.132\n",
      "2023-12-30 17:39:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.105 | 0.148\n",
      "2023-12-30 17:39:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.124 | 0.165\n",
      "2023-12-30 17:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.102 | 0.160\n",
      "2023-12-30 17:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.146 | 0.171\n",
      "2023-12-30 17:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.154 | 0.189\n",
      "2023-12-30 17:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.153 | 0.178\n",
      "2023-12-30 17:39:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.107 | 0.163\n",
      "2023-12-30 17:39:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.097 | 0.162\n",
      "2023-12-30 17:39:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.111 | 0.180\n",
      "2023-12-30 17:39:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.134 | 0.167\n",
      "2023-12-30 17:39:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.139 | 0.150\n",
      "2023-12-30 17:39:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.132 | 0.177\n",
      "2023-12-30 17:39:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.117 | 0.169\n",
      "2023-12-30 17:39:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.143 | 0.151\n",
      "2023-12-30 17:39:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.171 | 0.159\n",
      "2023-12-30 17:39:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.128 | 0.162\n",
      "2023-12-30 17:39:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.151 | 0.159\n",
      "2023-12-30 17:39:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.120 | 0.158\n",
      "2023-12-30 17:39:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.129 | 0.175\n",
      "2023-12-30 17:39:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.130 | 0.156\n",
      "2023-12-30 17:39:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.136 | 0.153\n",
      "2023-12-30 17:39:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.125 | 0.156\n",
      "2023-12-30 17:39:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.116 | 0.176\n",
      "2023-12-30 17:39:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.173 | 0.151\n",
      "2023-12-30 17:39:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.156 | 0.148\n",
      "2023-12-30 17:39:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.139 | 0.149\n",
      "2023-12-30 17:39:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.161 | 0.152\n",
      "2023-12-30 17:39:34 - INFO     | Early stopping: no decrease (0.132 vs 0.159); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:05<02:00,  9.26s/it]2023-12-30 17:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 17:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.154 | 0.161\n",
      "2023-12-30 17:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.116 | 0.150\n",
      "2023-12-30 17:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.106 | 0.145\n",
      "2023-12-30 17:39:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.123 | 0.145\n",
      "2023-12-30 17:39:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.110 | 0.130\n",
      "2023-12-30 17:39:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.096 | 0.173\n",
      "2023-12-30 17:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.138 | 0.142\n",
      "2023-12-30 17:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.111 | 0.140\n",
      "2023-12-30 17:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.105 | 0.137\n",
      "2023-12-30 17:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.131 | 0.140\n",
      "2023-12-30 17:39:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.115 | 0.140\n",
      "2023-12-30 17:39:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.129 | 0.149\n",
      "2023-12-30 17:39:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.139 | 0.155\n",
      "2023-12-30 17:39:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.107 | 0.168\n",
      "2023-12-30 17:39:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.131 | 0.149\n",
      "2023-12-30 17:39:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.091 | 0.158\n",
      "2023-12-30 17:39:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.152 | 0.140\n",
      "2023-12-30 17:39:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.088 | 0.142\n",
      "2023-12-30 17:39:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.140 | 0.138\n",
      "2023-12-30 17:39:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.139 | 0.168\n",
      "2023-12-30 17:39:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.133 | 0.196\n",
      "2023-12-30 17:39:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.179 | 0.154\n",
      "2023-12-30 17:39:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.155 | 0.177\n",
      "2023-12-30 17:39:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.155 | 0.179\n",
      "2023-12-30 17:39:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.183 | 0.160\n",
      "2023-12-30 17:39:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.121 | 0.174\n",
      "2023-12-30 17:39:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.149 | 0.205\n",
      "2023-12-30 17:39:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.116 | 0.182\n",
      "2023-12-30 17:39:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.172 | 0.188\n",
      "2023-12-30 17:39:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.171 | 0.172\n",
      "2023-12-30 17:39:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.118 | 0.136\n",
      "2023-12-30 17:39:43 - INFO     | Early stopping: no decrease (0.132 vs 0.131); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:14<01:50,  9.23s/it]2023-12-30 17:39:43 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 17:39:43 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.091 | 0.134\n",
      "2023-12-30 17:39:43 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.128 | 0.134\n",
      "2023-12-30 17:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.119 | 0.131\n",
      "2023-12-30 17:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.101 | 0.130\n",
      "2023-12-30 17:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.090 | 0.148\n",
      "2023-12-30 17:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.125 | 0.159\n",
      "2023-12-30 17:39:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.098 | 0.164\n",
      "2023-12-30 17:39:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.117 | 0.158\n",
      "2023-12-30 17:39:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.130 | 0.181\n",
      "2023-12-30 17:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.143 | 0.141\n",
      "2023-12-30 17:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.101 | 0.143\n",
      "2023-12-30 17:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.112 | 0.161\n",
      "2023-12-30 17:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.116 | 0.149\n",
      "2023-12-30 17:39:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.120 | 0.150\n",
      "2023-12-30 17:39:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.088 | 0.155\n",
      "2023-12-30 17:39:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.150 | 0.185\n",
      "2023-12-30 17:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.134 | 0.153\n",
      "2023-12-30 17:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.137 | 0.134\n",
      "2023-12-30 17:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.127 | 0.146\n",
      "2023-12-30 17:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.131 | 0.138\n",
      "2023-12-30 17:39:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.082 | 0.155\n",
      "2023-12-30 17:39:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.128 | 0.170\n",
      "2023-12-30 17:39:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.108 | 0.129\n",
      "2023-12-30 17:39:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.109 | 0.139\n",
      "2023-12-30 17:39:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.128 | 0.150\n",
      "2023-12-30 17:39:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.114 | 0.154\n",
      "2023-12-30 17:39:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.123 | 0.130\n",
      "2023-12-30 17:39:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.136 | 0.141\n",
      "2023-12-30 17:39:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.126 | 0.149\n",
      "2023-12-30 17:39:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.143 | 0.149\n",
      "2023-12-30 17:39:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.123 | 0.138\n",
      "2023-12-30 17:39:52 - INFO     | Early stopping: no decrease (0.132 vs 0.130); counter: 3 out of 3\n",
      "2023-12-30 17:39:52 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:39:52 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 45%|████▌     | 9/20 [01:23<01:41,  9.19s/it]2023-12-30 17:39:52 - INFO     | Epoch: 9 | Learning Rate: 0.003\n",
      "2023-12-30 17:39:52 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 504064 examples: 0.017 | 0.127\n",
      "2023-12-30 17:39:52 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 505920 examples: 0.105 | 0.136\n",
      "2023-12-30 17:39:53 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 507776 examples: 0.071 | 0.158\n",
      "2023-12-30 17:39:53 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 509632 examples: 0.064 | 0.123\n",
      "2023-12-30 17:39:53 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 511488 examples: 0.072 | 0.125\n",
      "2023-12-30 17:39:54 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 513344 examples: 0.095 | 0.135\n",
      "2023-12-30 17:39:54 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 515200 examples: 0.071 | 0.132\n",
      "2023-12-30 17:39:54 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 517056 examples: 0.077 | 0.133\n",
      "2023-12-30 17:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 518912 examples: 0.091 | 0.137\n",
      "2023-12-30 17:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 520768 examples: 0.098 | 0.133\n",
      "2023-12-30 17:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 522624 examples: 0.118 | 0.126\n",
      "2023-12-30 17:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 524480 examples: 0.096 | 0.131\n",
      "2023-12-30 17:39:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 526336 examples: 0.106 | 0.122\n",
      "2023-12-30 17:39:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 528192 examples: 0.085 | 0.146\n",
      "2023-12-30 17:39:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 530048 examples: 0.115 | 0.128\n",
      "2023-12-30 17:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 531904 examples: 0.096 | 0.124\n",
      "2023-12-30 17:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 533760 examples: 0.080 | 0.125\n",
      "2023-12-30 17:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 535616 examples: 0.071 | 0.129\n",
      "2023-12-30 17:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 537472 examples: 0.090 | 0.114\n",
      "2023-12-30 17:39:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 539328 examples: 0.051 | 0.117\n",
      "2023-12-30 17:39:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 541184 examples: 0.096 | 0.120\n",
      "2023-12-30 17:39:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 543040 examples: 0.077 | 0.124\n",
      "2023-12-30 17:39:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 544896 examples: 0.094 | 0.118\n",
      "2023-12-30 17:39:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 546752 examples: 0.102 | 0.119\n",
      "2023-12-30 17:39:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 548608 examples: 0.091 | 0.120\n",
      "2023-12-30 17:39:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 550464 examples: 0.064 | 0.131\n",
      "2023-12-30 17:40:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 552320 examples: 0.101 | 0.125\n",
      "2023-12-30 17:40:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 554176 examples: 0.119 | 0.114\n",
      "2023-12-30 17:40:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 556032 examples: 0.107 | 0.127\n",
      "2023-12-30 17:40:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 557888 examples: 0.124 | 0.123\n",
      "2023-12-30 17:40:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 559744 examples: 0.088 | 0.124\n",
      "2023-12-30 17:40:01 - INFO     | Early stopping: no decrease (0.132 vs 0.131); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:32<01:32,  9.23s/it]2023-12-30 17:40:01 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 17:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.035 | 0.132\n",
      "2023-12-30 17:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.106 | 0.117\n",
      "2023-12-30 17:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.058 | 0.108\n",
      "2023-12-30 17:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.076 | 0.113\n",
      "2023-12-30 17:40:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.073 | 0.113\n",
      "2023-12-30 17:40:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.072 | 0.118\n",
      "2023-12-30 17:40:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.084 | 0.110\n",
      "2023-12-30 17:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.091 | 0.132\n",
      "2023-12-30 17:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.057 | 0.110\n",
      "2023-12-30 17:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.071 | 0.108\n",
      "2023-12-30 17:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.056 | 0.128\n",
      "2023-12-30 17:40:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.069 | 0.125\n",
      "2023-12-30 17:40:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.058 | 0.117\n",
      "2023-12-30 17:40:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.082 | 0.134\n",
      "2023-12-30 17:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.075 | 0.125\n",
      "2023-12-30 17:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.089 | 0.127\n",
      "2023-12-30 17:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.079 | 0.120\n",
      "2023-12-30 17:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.070 | 0.120\n",
      "2023-12-30 17:40:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.067 | 0.120\n",
      "2023-12-30 17:40:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.073 | 0.123\n",
      "2023-12-30 17:40:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.085 | 0.124\n",
      "2023-12-30 17:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.076 | 0.121\n",
      "2023-12-30 17:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.087 | 0.115\n",
      "2023-12-30 17:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.068 | 0.112\n",
      "2023-12-30 17:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.098 | 0.117\n",
      "2023-12-30 17:40:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.065 | 0.115\n",
      "2023-12-30 17:40:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.079 | 0.113\n",
      "2023-12-30 17:40:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.093 | 0.110\n",
      "2023-12-30 17:40:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.080 | 0.121\n",
      "2023-12-30 17:40:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.091 | 0.120\n",
      "2023-12-30 17:40:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.064 | 0.109\n",
      "2023-12-30 17:40:10 - INFO     | Early stopping: loss decreased (0.132 -> 0.108; -17.9%). Caching model state.\n",
      " 55%|█████▌    | 11/20 [01:42<01:23,  9.23s/it]2023-12-30 17:40:10 - INFO     | Epoch: 11 | Learning Rate: 0.003\n",
      "2023-12-30 17:40:11 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 616064 examples: 0.143 | 0.108\n",
      "2023-12-30 17:40:11 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 617920 examples: 0.068 | 0.105\n",
      "2023-12-30 17:40:11 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 619776 examples: 0.065 | 0.113\n",
      "2023-12-30 17:40:12 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 621632 examples: 0.073 | 0.121\n",
      "2023-12-30 17:40:12 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 623488 examples: 0.075 | 0.112\n",
      "2023-12-30 17:40:12 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 625344 examples: 0.076 | 0.110\n",
      "2023-12-30 17:40:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 627200 examples: 0.044 | 0.108\n",
      "2023-12-30 17:40:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 629056 examples: 0.074 | 0.115\n",
      "2023-12-30 17:40:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 630912 examples: 0.058 | 0.126\n",
      "2023-12-30 17:40:14 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 632768 examples: 0.055 | 0.117\n",
      "2023-12-30 17:40:14 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 634624 examples: 0.070 | 0.130\n",
      "2023-12-30 17:40:14 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 636480 examples: 0.088 | 0.115\n",
      "2023-12-30 17:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 638336 examples: 0.061 | 0.123\n",
      "2023-12-30 17:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 640192 examples: 0.081 | 0.115\n",
      "2023-12-30 17:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 642048 examples: 0.071 | 0.112\n",
      "2023-12-30 17:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 643904 examples: 0.072 | 0.120\n",
      "2023-12-30 17:40:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 645760 examples: 0.071 | 0.114\n",
      "2023-12-30 17:40:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 647616 examples: 0.066 | 0.117\n",
      "2023-12-30 17:40:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 649472 examples: 0.075 | 0.122\n",
      "2023-12-30 17:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 651328 examples: 0.081 | 0.135\n",
      "2023-12-30 17:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 653184 examples: 0.077 | 0.123\n",
      "2023-12-30 17:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 655040 examples: 0.087 | 0.134\n",
      "2023-12-30 17:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 656896 examples: 0.070 | 0.126\n",
      "2023-12-30 17:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 658752 examples: 0.058 | 0.121\n",
      "2023-12-30 17:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 660608 examples: 0.059 | 0.129\n",
      "2023-12-30 17:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 662464 examples: 0.083 | 0.126\n",
      "2023-12-30 17:40:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 664320 examples: 0.079 | 0.111\n",
      "2023-12-30 17:40:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 666176 examples: 0.079 | 0.119\n",
      "2023-12-30 17:40:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 668032 examples: 0.054 | 0.133\n",
      "2023-12-30 17:40:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 669888 examples: 0.048 | 0.129\n",
      "2023-12-30 17:40:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 671744 examples: 0.100 | 0.126\n",
      "2023-12-30 17:40:20 - INFO     | Early stopping: no decrease (0.108 vs 0.122); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [01:51<01:15,  9.42s/it]2023-12-30 17:40:20 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 17:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.056 | 0.122\n",
      "2023-12-30 17:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.088 | 0.128\n",
      "2023-12-30 17:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.060 | 0.112\n",
      "2023-12-30 17:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.063 | 0.115\n",
      "2023-12-30 17:40:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.055 | 0.108\n",
      "2023-12-30 17:40:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.053 | 0.119\n",
      "2023-12-30 17:40:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.060 | 0.131\n",
      "2023-12-30 17:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.054 | 0.129\n",
      "2023-12-30 17:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.072 | 0.121\n",
      "2023-12-30 17:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.061 | 0.129\n",
      "2023-12-30 17:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.057 | 0.133\n",
      "2023-12-30 17:40:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.065 | 0.142\n",
      "2023-12-30 17:40:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.074 | 0.129\n",
      "2023-12-30 17:40:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.077 | 0.133\n",
      "2023-12-30 17:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.069 | 0.131\n",
      "2023-12-30 17:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.074 | 0.143\n",
      "2023-12-30 17:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.060 | 0.124\n",
      "2023-12-30 17:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.071 | 0.140\n",
      "2023-12-30 17:40:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.071 | 0.135\n",
      "2023-12-30 17:40:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.055 | 0.128\n",
      "2023-12-30 17:40:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.074 | 0.123\n",
      "2023-12-30 17:40:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.084 | 0.165\n",
      "2023-12-30 17:40:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.075 | 0.138\n",
      "2023-12-30 17:40:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.081 | 0.132\n",
      "2023-12-30 17:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.110 | 0.120\n",
      "2023-12-30 17:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.077 | 0.120\n",
      "2023-12-30 17:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.062 | 0.113\n",
      "2023-12-30 17:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.081 | 0.116\n",
      "2023-12-30 17:40:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.069 | 0.116\n",
      "2023-12-30 17:40:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.079 | 0.118\n",
      "2023-12-30 17:40:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.076 | 0.117\n",
      "2023-12-30 17:40:30 - INFO     | Early stopping: no decrease (0.108 vs 0.115); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:01<01:05,  9.38s/it]2023-12-30 17:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 17:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.098 | 0.114\n",
      "2023-12-30 17:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.050 | 0.110\n",
      "2023-12-30 17:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.064 | 0.107\n",
      "2023-12-30 17:40:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.067 | 0.110\n",
      "2023-12-30 17:40:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.057 | 0.117\n",
      "2023-12-30 17:40:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.057 | 0.115\n",
      "2023-12-30 17:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.059 | 0.125\n",
      "2023-12-30 17:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.070 | 0.120\n",
      "2023-12-30 17:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.045 | 0.116\n",
      "2023-12-30 17:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.065 | 0.117\n",
      "2023-12-30 17:40:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.051 | 0.113\n",
      "2023-12-30 17:40:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.081 | 0.112\n",
      "2023-12-30 17:40:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.055 | 0.110\n",
      "2023-12-30 17:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.057 | 0.132\n",
      "2023-12-30 17:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.054 | 0.121\n",
      "2023-12-30 17:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.062 | 0.126\n",
      "2023-12-30 17:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.063 | 0.133\n",
      "2023-12-30 17:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.071 | 0.128\n",
      "2023-12-30 17:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.080 | 0.139\n",
      "2023-12-30 17:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.084 | 0.135\n",
      "2023-12-30 17:40:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.094 | 0.121\n",
      "2023-12-30 17:40:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.079 | 0.136\n",
      "2023-12-30 17:40:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.094 | 0.126\n",
      "2023-12-30 17:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.082 | 0.115\n",
      "2023-12-30 17:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.063 | 0.149\n",
      "2023-12-30 17:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.081 | 0.126\n",
      "2023-12-30 17:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.094 | 0.128\n",
      "2023-12-30 17:40:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.067 | 0.125\n",
      "2023-12-30 17:40:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.064 | 0.113\n",
      "2023-12-30 17:40:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.060 | 0.114\n",
      "2023-12-30 17:40:39 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.061 | 0.118\n",
      "2023-12-30 17:40:39 - INFO     | Early stopping: no decrease (0.108 vs 0.120); counter: 3 out of 3\n",
      "2023-12-30 17:40:39 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:40:39 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 70%|███████   | 14/20 [02:10<00:56,  9.35s/it]2023-12-30 17:40:39 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 17:40:39 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.009 | 0.121\n",
      "2023-12-30 17:40:39 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.056 | 0.126\n",
      "2023-12-30 17:40:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.045 | 0.120\n",
      "2023-12-30 17:40:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.055 | 0.116\n",
      "2023-12-30 17:40:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.046 | 0.121\n",
      "2023-12-30 17:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.043 | 0.118\n",
      "2023-12-30 17:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.049 | 0.110\n",
      "2023-12-30 17:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.040 | 0.107\n",
      "2023-12-30 17:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.039 | 0.112\n",
      "2023-12-30 17:40:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.050 | 0.107\n",
      "2023-12-30 17:40:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.032 | 0.110\n",
      "2023-12-30 17:40:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.049 | 0.108\n",
      "2023-12-30 17:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.041 | 0.106\n",
      "2023-12-30 17:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.057 | 0.112\n",
      "2023-12-30 17:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.034 | 0.115\n",
      "2023-12-30 17:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.060 | 0.120\n",
      "2023-12-30 17:40:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.037 | 0.112\n",
      "2023-12-30 17:40:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.043 | 0.116\n",
      "2023-12-30 17:40:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.041 | 0.114\n",
      "2023-12-30 17:40:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.064 | 0.108\n",
      "2023-12-30 17:40:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.042 | 0.104\n",
      "2023-12-30 17:40:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.042 | 0.111\n",
      "2023-12-30 17:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.048 | 0.112\n",
      "2023-12-30 17:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.063 | 0.110\n",
      "2023-12-30 17:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.051 | 0.115\n",
      "2023-12-30 17:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.031 | 0.117\n",
      "2023-12-30 17:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.063 | 0.113\n",
      "2023-12-30 17:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.035 | 0.112\n",
      "2023-12-30 17:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.051 | 0.107\n",
      "2023-12-30 17:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.053 | 0.109\n",
      "2023-12-30 17:40:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.050 | 0.106\n",
      "2023-12-30 17:40:48 - INFO     | Early stopping: no decrease (0.108 vs 0.107); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:19<00:46,  9.32s/it]2023-12-30 17:40:48 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 17:40:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.013 | 0.107\n",
      "2023-12-30 17:40:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.050 | 0.116\n",
      "2023-12-30 17:40:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.027 | 0.111\n",
      "2023-12-30 17:40:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.034 | 0.108\n",
      "2023-12-30 17:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.023 | 0.109\n",
      "2023-12-30 17:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.028 | 0.115\n",
      "2023-12-30 17:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.040 | 0.114\n",
      "2023-12-30 17:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.045 | 0.111\n",
      "2023-12-30 17:40:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.031 | 0.107\n",
      "2023-12-30 17:40:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.041 | 0.105\n",
      "2023-12-30 17:40:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.043 | 0.109\n",
      "2023-12-30 17:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.034 | 0.104\n",
      "2023-12-30 17:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.041 | 0.109\n",
      "2023-12-30 17:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.032 | 0.117\n",
      "2023-12-30 17:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.056 | 0.111\n",
      "2023-12-30 17:40:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.044 | 0.127\n",
      "2023-12-30 17:40:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.046 | 0.111\n",
      "2023-12-30 17:40:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.026 | 0.120\n",
      "2023-12-30 17:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.049 | 0.113\n",
      "2023-12-30 17:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.031 | 0.110\n",
      "2023-12-30 17:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.037 | 0.108\n",
      "2023-12-30 17:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.037 | 0.115\n",
      "2023-12-30 17:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.044 | 0.114\n",
      "2023-12-30 17:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.053 | 0.114\n",
      "2023-12-30 17:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.033 | 0.112\n",
      "2023-12-30 17:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.057 | 0.110\n",
      "2023-12-30 17:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.036 | 0.117\n",
      "2023-12-30 17:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.048 | 0.105\n",
      "2023-12-30 17:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.038 | 0.103\n",
      "2023-12-30 17:40:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.054 | 0.104\n",
      "2023-12-30 17:40:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.040 | 0.114\n",
      "2023-12-30 17:40:57 - INFO     | Early stopping: no decrease (0.108 vs 0.114); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:28<00:37,  9.30s/it]2023-12-30 17:40:57 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 17:40:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.002 | 0.113\n",
      "2023-12-30 17:40:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.034 | 0.110\n",
      "2023-12-30 17:40:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.035 | 0.123\n",
      "2023-12-30 17:40:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.028 | 0.121\n",
      "2023-12-30 17:40:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.026 | 0.115\n",
      "2023-12-30 17:40:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.039 | 0.119\n",
      "2023-12-30 17:40:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.042 | 0.121\n",
      "2023-12-30 17:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.056 | 0.112\n",
      "2023-12-30 17:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.024 | 0.108\n",
      "2023-12-30 17:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.026 | 0.112\n",
      "2023-12-30 17:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.033 | 0.113\n",
      "2023-12-30 17:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.041 | 0.115\n",
      "2023-12-30 17:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.023 | 0.110\n",
      "2023-12-30 17:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.032 | 0.115\n",
      "2023-12-30 17:41:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.029 | 0.115\n",
      "2023-12-30 17:41:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.032 | 0.112\n",
      "2023-12-30 17:41:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.041 | 0.113\n",
      "2023-12-30 17:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.031 | 0.115\n",
      "2023-12-30 17:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.034 | 0.113\n",
      "2023-12-30 17:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.043 | 0.121\n",
      "2023-12-30 17:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.054 | 0.114\n",
      "2023-12-30 17:41:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.043 | 0.112\n",
      "2023-12-30 17:41:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.045 | 0.119\n",
      "2023-12-30 17:41:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.042 | 0.120\n",
      "2023-12-30 17:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.039 | 0.116\n",
      "2023-12-30 17:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.036 | 0.116\n",
      "2023-12-30 17:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.039 | 0.115\n",
      "2023-12-30 17:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.043 | 0.121\n",
      "2023-12-30 17:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.039 | 0.113\n",
      "2023-12-30 17:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.044 | 0.114\n",
      "2023-12-30 17:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.035 | 0.119\n",
      "2023-12-30 17:41:07 - INFO     | Early stopping: no decrease (0.108 vs 0.119); counter: 3 out of 3\n",
      "2023-12-30 17:41:07 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:41:07 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 85%|████████▌ | 17/20 [02:38<00:27,  9.28s/it]2023-12-30 17:41:07 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 17:41:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.062 | 0.119\n",
      "2023-12-30 17:41:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.024 | 0.122\n",
      "2023-12-30 17:41:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.029 | 0.119\n",
      "2023-12-30 17:41:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.022 | 0.120\n",
      "2023-12-30 17:41:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.037 | 0.123\n",
      "2023-12-30 17:41:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.030 | 0.123\n",
      "2023-12-30 17:41:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.027 | 0.119\n",
      "2023-12-30 17:41:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.029 | 0.122\n",
      "2023-12-30 17:41:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.020 | 0.117\n",
      "2023-12-30 17:41:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.022 | 0.116\n",
      "2023-12-30 17:41:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.023 | 0.117\n",
      "2023-12-30 17:41:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.029 | 0.117\n",
      "2023-12-30 17:41:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.016 | 0.119\n",
      "2023-12-30 17:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.039 | 0.116\n",
      "2023-12-30 17:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.019 | 0.114\n",
      "2023-12-30 17:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.025 | 0.114\n",
      "2023-12-30 17:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.039 | 0.118\n",
      "2023-12-30 17:41:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.020 | 0.118\n",
      "2023-12-30 17:41:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.024 | 0.118\n",
      "2023-12-30 17:41:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.013 | 0.116\n",
      "2023-12-30 17:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.034 | 0.113\n",
      "2023-12-30 17:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.033 | 0.114\n",
      "2023-12-30 17:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.020 | 0.113\n",
      "2023-12-30 17:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.018 | 0.112\n",
      "2023-12-30 17:41:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.022 | 0.114\n",
      "2023-12-30 17:41:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.030 | 0.111\n",
      "2023-12-30 17:41:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.026 | 0.112\n",
      "2023-12-30 17:41:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.023 | 0.112\n",
      "2023-12-30 17:41:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.028 | 0.114\n",
      "2023-12-30 17:41:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.026 | 0.116\n",
      "2023-12-30 17:41:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.027 | 0.116\n",
      "2023-12-30 17:41:16 - INFO     | Early stopping: no decrease (0.108 vs 0.116); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [02:47<00:18,  9.27s/it]2023-12-30 17:41:16 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 17:41:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.000 | 0.116\n",
      "2023-12-30 17:41:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.015 | 0.110\n",
      "2023-12-30 17:41:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.016 | 0.110\n",
      "2023-12-30 17:41:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.030 | 0.114\n",
      "2023-12-30 17:41:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.027 | 0.115\n",
      "2023-12-30 17:41:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.028 | 0.115\n",
      "2023-12-30 17:41:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.026 | 0.112\n",
      "2023-12-30 17:41:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.017 | 0.112\n",
      "2023-12-30 17:41:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.017 | 0.116\n",
      "2023-12-30 17:41:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.019 | 0.118\n",
      "2023-12-30 17:41:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.029 | 0.117\n",
      "2023-12-30 17:41:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.012 | 0.116\n",
      "2023-12-30 17:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.019 | 0.116\n",
      "2023-12-30 17:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.020 | 0.117\n",
      "2023-12-30 17:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.011 | 0.118\n",
      "2023-12-30 17:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.016 | 0.117\n",
      "2023-12-30 17:41:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.028 | 0.113\n",
      "2023-12-30 17:41:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.015 | 0.118\n",
      "2023-12-30 17:41:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.035 | 0.115\n",
      "2023-12-30 17:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.019 | 0.117\n",
      "2023-12-30 17:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.023 | 0.120\n",
      "2023-12-30 17:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.025 | 0.121\n",
      "2023-12-30 17:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.030 | 0.118\n",
      "2023-12-30 17:41:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.018 | 0.120\n",
      "2023-12-30 17:41:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.021 | 0.123\n",
      "2023-12-30 17:41:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.037 | 0.117\n",
      "2023-12-30 17:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.025 | 0.112\n",
      "2023-12-30 17:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.033 | 0.114\n",
      "2023-12-30 17:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.020 | 0.117\n",
      "2023-12-30 17:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.023 | 0.121\n",
      "2023-12-30 17:41:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.039 | 0.119\n",
      "2023-12-30 17:41:25 - INFO     | Early stopping: no decrease (0.108 vs 0.119); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [02:56<00:09,  9.26s/it]2023-12-30 17:41:25 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 17:41:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.000 | 0.119\n",
      "2023-12-30 17:41:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.016 | 0.117\n",
      "2023-12-30 17:41:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.018 | 0.119\n",
      "2023-12-30 17:41:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.016 | 0.116\n",
      "2023-12-30 17:41:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.017 | 0.116\n",
      "2023-12-30 17:41:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.017 | 0.118\n",
      "2023-12-30 17:41:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.013 | 0.118\n",
      "2023-12-30 17:41:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.011 | 0.116\n",
      "2023-12-30 17:41:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.023 | 0.120\n",
      "2023-12-30 17:41:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.015 | 0.121\n",
      "2023-12-30 17:41:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.020 | 0.120\n",
      "2023-12-30 17:41:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.023 | 0.118\n",
      "2023-12-30 17:41:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.020 | 0.118\n",
      "2023-12-30 17:41:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.016 | 0.117\n",
      "2023-12-30 17:41:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.032 | 0.118\n",
      "2023-12-30 17:41:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.019 | 0.116\n",
      "2023-12-30 17:41:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.008 | 0.114\n",
      "2023-12-30 17:41:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.021 | 0.121\n",
      "2023-12-30 17:41:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.012 | 0.117\n",
      "2023-12-30 17:41:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.022 | 0.118\n",
      "2023-12-30 17:41:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.021 | 0.119\n",
      "2023-12-30 17:41:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.021 | 0.122\n",
      "2023-12-30 17:41:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.015 | 0.123\n",
      "2023-12-30 17:41:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.020 | 0.125\n",
      "2023-12-30 17:41:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.027 | 0.124\n",
      "2023-12-30 17:41:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.018 | 0.123\n",
      "2023-12-30 17:41:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.024 | 0.124\n",
      "2023-12-30 17:41:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.040 | 0.122\n",
      "2023-12-30 17:41:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.023 | 0.120\n",
      "2023-12-30 17:41:34 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.029 | 0.118\n",
      "2023-12-30 17:41:34 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.026 | 0.116\n",
      "2023-12-30 17:41:34 - INFO     | Early stopping: no decrease (0.108 vs 0.117); counter: 3 out of 3\n",
      "2023-12-30 17:41:34 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:41:34 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      "100%|██████████| 20/20 [03:05<00:00,  9.29s/it]\n",
      "2023-12-30 17:41:34 - INFO     | Best validation loss: 0.108\n",
      "2023-12-30 17:41:34 - INFO     | Best early stopping index/epoch: 10\n",
      "2023-12-30 17:41:34 - INFO     | Average Loss on test set: 0.123\n",
      "/tmp/ipykernel_28696/1581398370.py:221: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots()\n",
      "2023-12-30 17:41:36 - INFO     | Weighted Precision: 0.975, Recall: 0.975, F1: 0.975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>████▄▄▄▄▄▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▇▃▅▆▇▆▇▅▄▅▄▄▄▄▅▄▄▃▃▃▂▂▃▂▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▆▄▆▄▇▄▅▆▃▄▂▃▃▂▄▃▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_validation_loss</td><td>0.10831</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.02563</td></tr><tr><td>step_validation_loss</td><td>0.11601</td></tr><tr><td>test_loss</td><td>0.12299</td></tr><tr><td>weighted_f1</td><td>0.97527</td></tr><tr><td>weighted_precision</td><td>0.97534</td></tr><tr><td>weighted_recall</td><td>0.97529</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-sweep-7</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/je0h7ksw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/je0h7ksw</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_173828-je0h7ksw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i2hyee2e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 32]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_174146-i2hyee2e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i2hyee2e' target=\"_blank\">balmy-sweep-8</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i2hyee2e' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i2hyee2e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 32], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:41:46 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 17:41:47 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 17.728 | 1019.894\n",
      "2023-12-30 17:41:47 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 32.853 | 2.296\n",
      "2023-12-30 17:41:47 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 2.296 | 2.295\n",
      "2023-12-30 17:41:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 2.295 | 2.294\n",
      "2023-12-30 17:41:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 2.298 | 2.295\n",
      "2023-12-30 17:41:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 2.294 | 2.291\n",
      "2023-12-30 17:41:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 2.295 | 2.291\n",
      "2023-12-30 17:41:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 2.288 | 2.290\n",
      "2023-12-30 17:41:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 2.293 | 2.288\n",
      "2023-12-30 17:41:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 2.289 | 2.287\n",
      "2023-12-30 17:41:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 2.292 | 2.281\n",
      "2023-12-30 17:41:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 2.281 | 2.277\n",
      "2023-12-30 17:41:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 2.289 | 2.279\n",
      "2023-12-30 17:41:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 2.280 | 2.275\n",
      "2023-12-30 17:41:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 2.279 | 2.287\n",
      "2023-12-30 17:41:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 2.277 | 2.264\n",
      "2023-12-30 17:41:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 2.266 | 2.268\n",
      "2023-12-30 17:41:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 2.271 | 2.252\n",
      "2023-12-30 17:41:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 2.250 | 2.243\n",
      "2023-12-30 17:41:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 2.243 | 2.225\n",
      "2023-12-30 17:41:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 2.199 | 2.141\n",
      "2023-12-30 17:41:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 1.946 | 1.580\n",
      "2023-12-30 17:41:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 1.066 | 0.736\n",
      "2023-12-30 17:41:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.657 | 0.580\n",
      "2023-12-30 17:41:54 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.574 | 0.914\n",
      "2023-12-30 17:41:54 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.506 | 0.420\n",
      "2023-12-30 17:41:54 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.411 | 0.333\n",
      "2023-12-30 17:41:55 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.388 | 0.342\n",
      "2023-12-30 17:41:55 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.335 | 0.288\n",
      "2023-12-30 17:41:55 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.330 | 0.285\n",
      "2023-12-30 17:41:55 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.267 | 0.290\n",
      "2023-12-30 17:41:56 - INFO     | Early stopping: loss decreased (inf -> 0.274; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:55,  9.26s/it]2023-12-30 17:41:56 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 17:41:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.219 | 0.248\n",
      "2023-12-30 17:41:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.276 | 0.250\n",
      "2023-12-30 17:41:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.216 | 0.300\n",
      "2023-12-30 17:41:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.258 | 0.223\n",
      "2023-12-30 17:41:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.190 | 0.246\n",
      "2023-12-30 17:41:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.271 | 0.223\n",
      "2023-12-30 17:41:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.192 | 0.194\n",
      "2023-12-30 17:41:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.212 | 0.193\n",
      "2023-12-30 17:41:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.177 | 0.184\n",
      "2023-12-30 17:41:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.175 | 0.174\n",
      "2023-12-30 17:41:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.152 | 0.195\n",
      "2023-12-30 17:41:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.199 | 0.198\n",
      "2023-12-30 17:41:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.183 | 0.183\n",
      "2023-12-30 17:42:00 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.174 | 0.167\n",
      "2023-12-30 17:42:00 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.168 | 0.165\n",
      "2023-12-30 17:42:00 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.175 | 0.149\n",
      "2023-12-30 17:42:00 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.168 | 0.153\n",
      "2023-12-30 17:42:01 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.151 | 0.145\n",
      "2023-12-30 17:42:01 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.141 | 0.165\n",
      "2023-12-30 17:42:01 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.172 | 0.160\n",
      "2023-12-30 17:42:02 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.168 | 0.168\n",
      "2023-12-30 17:42:02 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.150 | 0.138\n",
      "2023-12-30 17:42:02 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.156 | 0.153\n",
      "2023-12-30 17:42:02 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.149 | 0.156\n",
      "2023-12-30 17:42:03 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.146 | 0.149\n",
      "2023-12-30 17:42:03 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.146 | 0.143\n",
      "2023-12-30 17:42:03 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.153 | 0.151\n",
      "2023-12-30 17:42:03 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.151 | 0.124\n",
      "2023-12-30 17:42:04 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.122 | 0.129\n",
      "2023-12-30 17:42:04 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.140 | 0.128\n",
      "2023-12-30 17:42:04 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.156 | 0.147\n",
      "2023-12-30 17:42:05 - INFO     | Early stopping: loss decreased (0.274 -> 0.132; -51.8%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:43,  9.10s/it]2023-12-30 17:42:05 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 17:42:05 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.138 | 0.130\n",
      "2023-12-30 17:42:05 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.124 | 0.115\n",
      "2023-12-30 17:42:05 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.101 | 0.119\n",
      "2023-12-30 17:42:06 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.125 | 0.124\n",
      "2023-12-30 17:42:06 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.123 | 0.118\n",
      "2023-12-30 17:42:06 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.126 | 0.127\n",
      "2023-12-30 17:42:07 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.114 | 0.113\n",
      "2023-12-30 17:42:07 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.141 | 0.116\n",
      "2023-12-30 17:42:07 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.112 | 0.115\n",
      "2023-12-30 17:42:08 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.092 | 0.123\n",
      "2023-12-30 17:42:08 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.117 | 0.114\n",
      "2023-12-30 17:42:08 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.114 | 0.114\n",
      "2023-12-30 17:42:08 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.112 | 0.114\n",
      "2023-12-30 17:42:09 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.111 | 0.108\n",
      "2023-12-30 17:42:09 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.114 | 0.124\n",
      "2023-12-30 17:42:09 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.088 | 0.117\n",
      "2023-12-30 17:42:09 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.094 | 0.110\n",
      "2023-12-30 17:42:10 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.099 | 0.106\n",
      "2023-12-30 17:42:10 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.116 | 0.115\n",
      "2023-12-30 17:42:10 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.087 | 0.111\n",
      "2023-12-30 17:42:11 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.107 | 0.104\n",
      "2023-12-30 17:42:11 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.086 | 0.101\n",
      "2023-12-30 17:42:11 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.118 | 0.105\n",
      "2023-12-30 17:42:12 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.097 | 0.116\n",
      "2023-12-30 17:42:12 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.113 | 0.093\n",
      "2023-12-30 17:42:12 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.098 | 0.117\n",
      "2023-12-30 17:42:12 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.104 | 0.099\n",
      "2023-12-30 17:42:13 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.098 | 0.110\n",
      "2023-12-30 17:42:13 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.091 | 0.109\n",
      "2023-12-30 17:42:13 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.083 | 0.106\n",
      "2023-12-30 17:42:14 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.098 | 0.098\n",
      "2023-12-30 17:42:14 - INFO     | Early stopping: loss decreased (0.132 -> 0.107; -19.4%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:27<02:35,  9.12s/it]2023-12-30 17:42:14 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 17:42:14 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.020 | 0.105\n",
      "2023-12-30 17:42:14 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.077 | 0.095\n",
      "2023-12-30 17:42:15 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.074 | 0.097\n",
      "2023-12-30 17:42:15 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.095 | 0.100\n",
      "2023-12-30 17:42:15 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.083 | 0.111\n",
      "2023-12-30 17:42:16 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.090 | 0.111\n",
      "2023-12-30 17:42:16 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.065 | 0.092\n",
      "2023-12-30 17:42:16 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.084 | 0.095\n",
      "2023-12-30 17:42:17 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.071 | 0.098\n",
      "2023-12-30 17:42:17 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.084 | 0.097\n",
      "2023-12-30 17:42:17 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.077 | 0.097\n",
      "2023-12-30 17:42:17 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.091 | 0.111\n",
      "2023-12-30 17:42:18 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.085 | 0.094\n",
      "2023-12-30 17:42:18 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.070 | 0.095\n",
      "2023-12-30 17:42:18 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.070 | 0.096\n",
      "2023-12-30 17:42:19 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.076 | 0.083\n",
      "2023-12-30 17:42:19 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.092 | 0.088\n",
      "2023-12-30 17:42:19 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.089 | 0.097\n",
      "2023-12-30 17:42:19 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.087 | 0.082\n",
      "2023-12-30 17:42:20 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.066 | 0.080\n",
      "2023-12-30 17:42:20 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.077 | 0.090\n",
      "2023-12-30 17:42:20 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.108 | 0.097\n",
      "2023-12-30 17:42:21 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.085 | 0.086\n",
      "2023-12-30 17:42:21 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.069 | 0.081\n",
      "2023-12-30 17:42:21 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.082 | 0.089\n",
      "2023-12-30 17:42:21 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.098 | 0.082\n",
      "2023-12-30 17:42:22 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.071 | 0.089\n",
      "2023-12-30 17:42:22 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.091 | 0.084\n",
      "2023-12-30 17:42:22 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.081 | 0.075\n",
      "2023-12-30 17:42:23 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.080 | 0.077\n",
      "2023-12-30 17:42:23 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.071 | 0.072\n",
      "2023-12-30 17:42:23 - INFO     | Early stopping: loss decreased (0.107 -> 0.076; -28.5%). Caching model state.\n",
      " 20%|██        | 4/20 [00:36<02:27,  9.22s/it]2023-12-30 17:42:23 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 17:42:23 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.084 | 0.074\n",
      "2023-12-30 17:42:24 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.060 | 0.084\n",
      "2023-12-30 17:42:24 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.068 | 0.077\n",
      "2023-12-30 17:42:24 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.052 | 0.081\n",
      "2023-12-30 17:42:25 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.075 | 0.091\n",
      "2023-12-30 17:42:25 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.072 | 0.082\n",
      "2023-12-30 17:42:25 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.080 | 0.099\n",
      "2023-12-30 17:42:25 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.058 | 0.075\n",
      "2023-12-30 17:42:26 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.072 | 0.086\n",
      "2023-12-30 17:42:26 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.051 | 0.068\n",
      "2023-12-30 17:42:26 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.076 | 0.075\n",
      "2023-12-30 17:42:27 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.061 | 0.088\n",
      "2023-12-30 17:42:27 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.072 | 0.076\n",
      "2023-12-30 17:42:27 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.068 | 0.081\n",
      "2023-12-30 17:42:27 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.063 | 0.072\n",
      "2023-12-30 17:42:28 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.053 | 0.075\n",
      "2023-12-30 17:42:28 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.048 | 0.078\n",
      "2023-12-30 17:42:28 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.061 | 0.073\n",
      "2023-12-30 17:42:29 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.064 | 0.072\n",
      "2023-12-30 17:42:29 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.053 | 0.073\n",
      "2023-12-30 17:42:29 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.046 | 0.075\n",
      "2023-12-30 17:42:29 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.073 | 0.072\n",
      "2023-12-30 17:42:30 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.063 | 0.070\n",
      "2023-12-30 17:42:30 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.065 | 0.092\n",
      "2023-12-30 17:42:30 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.051 | 0.085\n",
      "2023-12-30 17:42:31 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.057 | 0.088\n",
      "2023-12-30 17:42:31 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.068 | 0.088\n",
      "2023-12-30 17:42:31 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.053 | 0.082\n",
      "2023-12-30 17:42:31 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.076 | 0.069\n",
      "2023-12-30 17:42:32 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.071 | 0.070\n",
      "2023-12-30 17:42:32 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.071 | 0.069\n",
      "2023-12-30 17:42:32 - INFO     | Early stopping: loss decreased (0.076 -> 0.071; -7.3%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:45<02:17,  9.18s/it]2023-12-30 17:42:32 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 17:42:33 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.079 | 0.071\n",
      "2023-12-30 17:42:33 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.052 | 0.069\n",
      "2023-12-30 17:42:33 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.063 | 0.072\n",
      "2023-12-30 17:42:33 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.057 | 0.082\n",
      "2023-12-30 17:42:34 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.049 | 0.066\n",
      "2023-12-30 17:42:34 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.049 | 0.066\n",
      "2023-12-30 17:42:34 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.052 | 0.069\n",
      "2023-12-30 17:42:34 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.072 | 0.065\n",
      "2023-12-30 17:42:35 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.047 | 0.065\n",
      "2023-12-30 17:42:35 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.039 | 0.059\n",
      "2023-12-30 17:42:35 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.065 | 0.066\n",
      "2023-12-30 17:42:36 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.048 | 0.061\n",
      "2023-12-30 17:42:36 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.052 | 0.066\n",
      "2023-12-30 17:42:36 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.051 | 0.061\n",
      "2023-12-30 17:42:37 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.052 | 0.077\n",
      "2023-12-30 17:42:37 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.041 | 0.070\n",
      "2023-12-30 17:42:37 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.066 | 0.077\n",
      "2023-12-30 17:42:37 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.061 | 0.065\n",
      "2023-12-30 17:42:38 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.043 | 0.062\n",
      "2023-12-30 17:42:38 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.047 | 0.063\n",
      "2023-12-30 17:42:38 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.065 | 0.063\n",
      "2023-12-30 17:42:39 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.054 | 0.071\n",
      "2023-12-30 17:42:39 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.087 | 0.070\n",
      "2023-12-30 17:42:39 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.049 | 0.070\n",
      "2023-12-30 17:42:39 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.056 | 0.080\n",
      "2023-12-30 17:42:40 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.042 | 0.068\n",
      "2023-12-30 17:42:40 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.072 | 0.070\n",
      "2023-12-30 17:42:40 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.059 | 0.065\n",
      "2023-12-30 17:42:40 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.044 | 0.073\n",
      "2023-12-30 17:42:41 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.056 | 0.072\n",
      "2023-12-30 17:42:41 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.065 | 0.073\n",
      "2023-12-30 17:42:41 - INFO     | Early stopping: no decrease (0.071 vs 0.071); counter: 1 out of 3\n",
      " 30%|███       | 6/20 [00:54<02:07,  9.14s/it]2023-12-30 17:42:41 - INFO     | Epoch: 6 | Learning Rate: 0.010\n",
      "2023-12-30 17:42:42 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 336064 examples: 0.043 | 0.072\n",
      "2023-12-30 17:42:42 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 337920 examples: 0.042 | 0.065\n",
      "2023-12-30 17:42:42 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 339776 examples: 0.044 | 0.065\n",
      "2023-12-30 17:42:42 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 341632 examples: 0.042 | 0.064\n",
      "2023-12-30 17:42:43 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 343488 examples: 0.052 | 0.067\n",
      "2023-12-30 17:42:43 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 345344 examples: 0.038 | 0.082\n",
      "2023-12-30 17:42:43 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 347200 examples: 0.044 | 0.086\n",
      "2023-12-30 17:42:44 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 349056 examples: 0.057 | 0.085\n",
      "2023-12-30 17:42:44 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 350912 examples: 0.032 | 0.069\n",
      "2023-12-30 17:42:44 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 352768 examples: 0.034 | 0.061\n",
      "2023-12-30 17:42:44 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 354624 examples: 0.035 | 0.077\n",
      "2023-12-30 17:42:45 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 356480 examples: 0.037 | 0.068\n",
      "2023-12-30 17:42:45 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 358336 examples: 0.052 | 0.065\n",
      "2023-12-30 17:42:45 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 360192 examples: 0.056 | 0.063\n",
      "2023-12-30 17:42:46 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 362048 examples: 0.043 | 0.067\n",
      "2023-12-30 17:42:46 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 363904 examples: 0.039 | 0.067\n",
      "2023-12-30 17:42:46 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 365760 examples: 0.059 | 0.068\n",
      "2023-12-30 17:42:46 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 367616 examples: 0.071 | 0.068\n",
      "2023-12-30 17:42:47 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 369472 examples: 0.064 | 0.059\n",
      "2023-12-30 17:42:47 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 371328 examples: 0.042 | 0.065\n",
      "2023-12-30 17:42:47 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 373184 examples: 0.045 | 0.071\n",
      "2023-12-30 17:42:48 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 375040 examples: 0.058 | 0.061\n",
      "2023-12-30 17:42:48 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 376896 examples: 0.039 | 0.064\n",
      "2023-12-30 17:42:48 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 378752 examples: 0.054 | 0.070\n",
      "2023-12-30 17:42:48 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 380608 examples: 0.046 | 0.095\n",
      "2023-12-30 17:42:49 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 382464 examples: 0.039 | 0.067\n",
      "2023-12-30 17:42:49 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 384320 examples: 0.055 | 0.069\n",
      "2023-12-30 17:42:49 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 386176 examples: 0.060 | 0.062\n",
      "2023-12-30 17:42:50 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 388032 examples: 0.051 | 0.067\n",
      "2023-12-30 17:42:50 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 389888 examples: 0.067 | 0.064\n",
      "2023-12-30 17:42:50 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 391744 examples: 0.054 | 0.076\n",
      "2023-12-30 17:42:50 - INFO     | Early stopping: no decrease (0.071 vs 0.068); counter: 2 out of 3\n",
      " 35%|███▌      | 7/20 [01:04<01:58,  9.15s/it]2023-12-30 17:42:50 - INFO     | Epoch: 7 | Learning Rate: 0.010\n",
      "2023-12-30 17:42:51 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 392064 examples: 0.024 | 0.069\n",
      "2023-12-30 17:42:51 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 393920 examples: 0.041 | 0.063\n",
      "2023-12-30 17:42:51 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 395776 examples: 0.042 | 0.068\n",
      "2023-12-30 17:42:52 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 397632 examples: 0.043 | 0.067\n",
      "2023-12-30 17:42:52 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 399488 examples: 0.040 | 0.066\n",
      "2023-12-30 17:42:52 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 401344 examples: 0.036 | 0.069\n",
      "2023-12-30 17:42:52 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 403200 examples: 0.036 | 0.068\n",
      "2023-12-30 17:42:53 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 405056 examples: 0.033 | 0.060\n",
      "2023-12-30 17:42:53 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 406912 examples: 0.039 | 0.062\n",
      "2023-12-30 17:42:53 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 408768 examples: 0.047 | 0.057\n",
      "2023-12-30 17:42:54 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 410624 examples: 0.035 | 0.066\n",
      "2023-12-30 17:42:54 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 412480 examples: 0.057 | 0.062\n",
      "2023-12-30 17:42:54 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 414336 examples: 0.061 | 0.067\n",
      "2023-12-30 17:42:54 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 416192 examples: 0.024 | 0.064\n",
      "2023-12-30 17:42:55 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 418048 examples: 0.051 | 0.064\n",
      "2023-12-30 17:42:55 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 419904 examples: 0.049 | 0.073\n",
      "2023-12-30 17:42:55 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 421760 examples: 0.050 | 0.069\n",
      "2023-12-30 17:42:56 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 423616 examples: 0.044 | 0.071\n",
      "2023-12-30 17:42:56 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 425472 examples: 0.056 | 0.074\n",
      "2023-12-30 17:42:56 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 427328 examples: 0.035 | 0.077\n",
      "2023-12-30 17:42:57 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 429184 examples: 0.042 | 0.059\n",
      "2023-12-30 17:42:57 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 431040 examples: 0.037 | 0.065\n",
      "2023-12-30 17:42:57 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 432896 examples: 0.046 | 0.059\n",
      "2023-12-30 17:42:57 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 434752 examples: 0.049 | 0.062\n",
      "2023-12-30 17:42:58 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 436608 examples: 0.053 | 0.066\n",
      "2023-12-30 17:42:58 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 438464 examples: 0.048 | 0.061\n",
      "2023-12-30 17:42:58 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 440320 examples: 0.042 | 0.063\n",
      "2023-12-30 17:42:59 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 442176 examples: 0.044 | 0.061\n",
      "2023-12-30 17:42:59 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 444032 examples: 0.048 | 0.068\n",
      "2023-12-30 17:42:59 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 445888 examples: 0.045 | 0.069\n",
      "2023-12-30 17:42:59 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 447744 examples: 0.049 | 0.068\n",
      "2023-12-30 17:43:00 - INFO     | Early stopping: no decrease (0.071 vs 0.072); counter: 3 out of 3\n",
      "2023-12-30 17:43:00 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:43:00 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 40%|████      | 8/20 [01:13<01:49,  9.16s/it]2023-12-30 17:43:00 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 17:43:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.083 | 0.069\n",
      "2023-12-30 17:43:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.035 | 0.059\n",
      "2023-12-30 17:43:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.024 | 0.057\n",
      "2023-12-30 17:43:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.033 | 0.058\n",
      "2023-12-30 17:43:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.034 | 0.066\n",
      "2023-12-30 17:43:01 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.028 | 0.058\n",
      "2023-12-30 17:43:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.020 | 0.055\n",
      "2023-12-30 17:43:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.025 | 0.058\n",
      "2023-12-30 17:43:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.028 | 0.059\n",
      "2023-12-30 17:43:02 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.028 | 0.058\n",
      "2023-12-30 17:43:03 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.025 | 0.056\n",
      "2023-12-30 17:43:03 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.028 | 0.060\n",
      "2023-12-30 17:43:03 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.034 | 0.058\n",
      "2023-12-30 17:43:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.029 | 0.058\n",
      "2023-12-30 17:43:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.028 | 0.060\n",
      "2023-12-30 17:43:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.045 | 0.058\n",
      "2023-12-30 17:43:04 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.030 | 0.058\n",
      "2023-12-30 17:43:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.034 | 0.056\n",
      "2023-12-30 17:43:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.028 | 0.062\n",
      "2023-12-30 17:43:05 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.050 | 0.057\n",
      "2023-12-30 17:43:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.018 | 0.059\n",
      "2023-12-30 17:43:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.025 | 0.060\n",
      "2023-12-30 17:43:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.034 | 0.060\n",
      "2023-12-30 17:43:06 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.027 | 0.059\n",
      "2023-12-30 17:43:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.037 | 0.064\n",
      "2023-12-30 17:43:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.039 | 0.058\n",
      "2023-12-30 17:43:07 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.033 | 0.055\n",
      "2023-12-30 17:43:08 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.033 | 0.056\n",
      "2023-12-30 17:43:08 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.043 | 0.056\n",
      "2023-12-30 17:43:08 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.034 | 0.056\n",
      "2023-12-30 17:43:09 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.028 | 0.058\n",
      "2023-12-30 17:43:09 - INFO     | Early stopping: loss decreased (0.071 -> 0.058; -18.2%). Caching model state.\n",
      " 45%|████▌     | 9/20 [01:22<01:40,  9.16s/it]2023-12-30 17:43:09 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 17:43:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.106 | 0.056\n",
      "2023-12-30 17:43:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.017 | 0.053\n",
      "2023-12-30 17:43:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.017 | 0.057\n",
      "2023-12-30 17:43:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.024 | 0.056\n",
      "2023-12-30 17:43:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.032 | 0.053\n",
      "2023-12-30 17:43:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.028 | 0.055\n",
      "2023-12-30 17:43:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.021 | 0.055\n",
      "2023-12-30 17:43:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.026 | 0.056\n",
      "2023-12-30 17:43:11 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.020 | 0.059\n",
      "2023-12-30 17:43:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.029 | 0.057\n",
      "2023-12-30 17:43:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.017 | 0.057\n",
      "2023-12-30 17:43:12 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.018 | 0.058\n",
      "2023-12-30 17:43:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.020 | 0.056\n",
      "2023-12-30 17:43:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.034 | 0.055\n",
      "2023-12-30 17:43:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.023 | 0.058\n",
      "2023-12-30 17:43:13 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.029 | 0.059\n",
      "2023-12-30 17:43:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.048 | 0.058\n",
      "2023-12-30 17:43:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.036 | 0.059\n",
      "2023-12-30 17:43:14 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.027 | 0.057\n",
      "2023-12-30 17:43:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.031 | 0.058\n",
      "2023-12-30 17:43:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.016 | 0.062\n",
      "2023-12-30 17:43:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.040 | 0.059\n",
      "2023-12-30 17:43:15 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.032 | 0.057\n",
      "2023-12-30 17:43:16 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.025 | 0.060\n",
      "2023-12-30 17:43:16 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.052 | 0.061\n",
      "2023-12-30 17:43:16 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.030 | 0.056\n",
      "2023-12-30 17:43:17 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.031 | 0.060\n",
      "2023-12-30 17:43:17 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.025 | 0.063\n",
      "2023-12-30 17:43:17 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.032 | 0.058\n",
      "2023-12-30 17:43:17 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.035 | 0.056\n",
      "2023-12-30 17:43:18 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.018 | 0.057\n",
      "2023-12-30 17:43:18 - INFO     | Early stopping: no decrease (0.058 vs 0.056); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:31<01:31,  9.17s/it]2023-12-30 17:43:18 - INFO     | Epoch: 10 | Learning Rate: 0.005\n",
      "2023-12-30 17:43:18 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 560064 examples: 0.017 | 0.056\n",
      "2023-12-30 17:43:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 561920 examples: 0.013 | 0.060\n",
      "2023-12-30 17:43:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 563776 examples: 0.031 | 0.057\n",
      "2023-12-30 17:43:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 565632 examples: 0.017 | 0.060\n",
      "2023-12-30 17:43:19 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 567488 examples: 0.022 | 0.060\n",
      "2023-12-30 17:43:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 569344 examples: 0.029 | 0.060\n",
      "2023-12-30 17:43:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 571200 examples: 0.018 | 0.060\n",
      "2023-12-30 17:43:20 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 573056 examples: 0.026 | 0.057\n",
      "2023-12-30 17:43:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 574912 examples: 0.016 | 0.061\n",
      "2023-12-30 17:43:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 576768 examples: 0.029 | 0.065\n",
      "2023-12-30 17:43:21 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 578624 examples: 0.036 | 0.062\n",
      "2023-12-30 17:43:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 580480 examples: 0.028 | 0.064\n",
      "2023-12-30 17:43:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 582336 examples: 0.021 | 0.061\n",
      "2023-12-30 17:43:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 584192 examples: 0.022 | 0.059\n",
      "2023-12-30 17:43:22 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 586048 examples: 0.028 | 0.059\n",
      "2023-12-30 17:43:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 587904 examples: 0.032 | 0.057\n",
      "2023-12-30 17:43:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 589760 examples: 0.020 | 0.061\n",
      "2023-12-30 17:43:23 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 591616 examples: 0.043 | 0.059\n",
      "2023-12-30 17:43:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 593472 examples: 0.022 | 0.056\n",
      "2023-12-30 17:43:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 595328 examples: 0.035 | 0.059\n",
      "2023-12-30 17:43:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 597184 examples: 0.013 | 0.059\n",
      "2023-12-30 17:43:24 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 599040 examples: 0.019 | 0.058\n",
      "2023-12-30 17:43:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 600896 examples: 0.038 | 0.055\n",
      "2023-12-30 17:43:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 602752 examples: 0.034 | 0.061\n",
      "2023-12-30 17:43:25 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 604608 examples: 0.023 | 0.063\n",
      "2023-12-30 17:43:26 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 606464 examples: 0.021 | 0.059\n",
      "2023-12-30 17:43:26 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 608320 examples: 0.030 | 0.060\n",
      "2023-12-30 17:43:26 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 610176 examples: 0.023 | 0.057\n",
      "2023-12-30 17:43:26 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 612032 examples: 0.025 | 0.061\n",
      "2023-12-30 17:43:27 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 613888 examples: 0.042 | 0.062\n",
      "2023-12-30 17:43:27 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 615744 examples: 0.029 | 0.061\n",
      "2023-12-30 17:43:27 - INFO     | Early stopping: no decrease (0.058 vs 0.057); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:40<01:22,  9.19s/it]2023-12-30 17:43:27 - INFO     | Epoch: 11 | Learning Rate: 0.005\n",
      "2023-12-30 17:43:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 616064 examples: 0.186 | 0.058\n",
      "2023-12-30 17:43:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 617920 examples: 0.025 | 0.059\n",
      "2023-12-30 17:43:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 619776 examples: 0.013 | 0.057\n",
      "2023-12-30 17:43:28 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 621632 examples: 0.029 | 0.062\n",
      "2023-12-30 17:43:29 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 623488 examples: 0.018 | 0.061\n",
      "2023-12-30 17:43:29 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 625344 examples: 0.031 | 0.064\n",
      "2023-12-30 17:43:29 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 627200 examples: 0.024 | 0.061\n",
      "2023-12-30 17:43:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 629056 examples: 0.019 | 0.056\n",
      "2023-12-30 17:43:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 630912 examples: 0.018 | 0.059\n",
      "2023-12-30 17:43:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 632768 examples: 0.019 | 0.057\n",
      "2023-12-30 17:43:30 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 634624 examples: 0.025 | 0.057\n",
      "2023-12-30 17:43:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 636480 examples: 0.025 | 0.057\n",
      "2023-12-30 17:43:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 638336 examples: 0.018 | 0.061\n",
      "2023-12-30 17:43:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 640192 examples: 0.017 | 0.057\n",
      "2023-12-30 17:43:31 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 642048 examples: 0.027 | 0.060\n",
      "2023-12-30 17:43:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 643904 examples: 0.036 | 0.060\n",
      "2023-12-30 17:43:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 645760 examples: 0.021 | 0.062\n",
      "2023-12-30 17:43:32 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 647616 examples: 0.027 | 0.059\n",
      "2023-12-30 17:43:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 649472 examples: 0.031 | 0.060\n",
      "2023-12-30 17:43:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 651328 examples: 0.019 | 0.058\n",
      "2023-12-30 17:43:33 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 653184 examples: 0.023 | 0.060\n",
      "2023-12-30 17:43:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 655040 examples: 0.029 | 0.061\n",
      "2023-12-30 17:43:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 656896 examples: 0.018 | 0.057\n",
      "2023-12-30 17:43:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 658752 examples: 0.035 | 0.057\n",
      "2023-12-30 17:43:34 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 660608 examples: 0.013 | 0.061\n",
      "2023-12-30 17:43:35 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 662464 examples: 0.040 | 0.059\n",
      "2023-12-30 17:43:35 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 664320 examples: 0.028 | 0.058\n",
      "2023-12-30 17:43:35 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 666176 examples: 0.026 | 0.059\n",
      "2023-12-30 17:43:36 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 668032 examples: 0.039 | 0.060\n",
      "2023-12-30 17:43:36 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 669888 examples: 0.024 | 0.060\n",
      "2023-12-30 17:43:36 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 671744 examples: 0.015 | 0.061\n",
      "2023-12-30 17:43:36 - INFO     | Early stopping: no decrease (0.058 vs 0.060); counter: 3 out of 3\n",
      "2023-12-30 17:43:36 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:43:36 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 60%|██████    | 12/20 [01:49<01:13,  9.16s/it]2023-12-30 17:43:36 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 17:43:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.015 | 0.060\n",
      "2023-12-30 17:43:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.016 | 0.058\n",
      "2023-12-30 17:43:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.017 | 0.058\n",
      "2023-12-30 17:43:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.019 | 0.058\n",
      "2023-12-30 17:43:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.019 | 0.058\n",
      "2023-12-30 17:43:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.022 | 0.058\n",
      "2023-12-30 17:43:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.015 | 0.059\n",
      "2023-12-30 17:43:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.012 | 0.059\n",
      "2023-12-30 17:43:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.019 | 0.061\n",
      "2023-12-30 17:43:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.015 | 0.060\n",
      "2023-12-30 17:43:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.020 | 0.061\n",
      "2023-12-30 17:43:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.029 | 0.059\n",
      "2023-12-30 17:43:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.021 | 0.058\n",
      "2023-12-30 17:43:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.015 | 0.059\n",
      "2023-12-30 17:43:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.018 | 0.059\n",
      "2023-12-30 17:43:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.020 | 0.059\n",
      "2023-12-30 17:43:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.024 | 0.058\n",
      "2023-12-30 17:43:41 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.019 | 0.060\n",
      "2023-12-30 17:43:42 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.025 | 0.058\n",
      "2023-12-30 17:43:42 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.017 | 0.058\n",
      "2023-12-30 17:43:42 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.015 | 0.057\n",
      "2023-12-30 17:43:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.015 | 0.058\n",
      "2023-12-30 17:43:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.034 | 0.058\n",
      "2023-12-30 17:43:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.017 | 0.057\n",
      "2023-12-30 17:43:43 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.018 | 0.058\n",
      "2023-12-30 17:43:44 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.017 | 0.060\n",
      "2023-12-30 17:43:44 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.031 | 0.061\n",
      "2023-12-30 17:43:44 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.017 | 0.060\n",
      "2023-12-30 17:43:45 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.015 | 0.058\n",
      "2023-12-30 17:43:45 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.017 | 0.059\n",
      "2023-12-30 17:43:45 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.031 | 0.059\n",
      "2023-12-30 17:43:45 - INFO     | Early stopping: no decrease (0.058 vs 0.058); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [01:59<01:03,  9.14s/it]2023-12-30 17:43:45 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 17:43:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.000 | 0.058\n",
      "2023-12-30 17:43:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.014 | 0.057\n",
      "2023-12-30 17:43:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.015 | 0.058\n",
      "2023-12-30 17:43:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.022 | 0.057\n",
      "2023-12-30 17:43:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.011 | 0.060\n",
      "2023-12-30 17:43:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.019 | 0.061\n",
      "2023-12-30 17:43:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.020 | 0.061\n",
      "2023-12-30 17:43:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.016 | 0.059\n",
      "2023-12-30 17:43:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.016 | 0.059\n",
      "2023-12-30 17:43:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.011 | 0.058\n",
      "2023-12-30 17:43:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.013 | 0.057\n",
      "2023-12-30 17:43:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.013 | 0.057\n",
      "2023-12-30 17:43:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.035 | 0.059\n",
      "2023-12-30 17:43:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.017 | 0.057\n",
      "2023-12-30 17:43:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.017 | 0.059\n",
      "2023-12-30 17:43:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.021 | 0.057\n",
      "2023-12-30 17:43:50 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.021 | 0.059\n",
      "2023-12-30 17:43:51 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.019 | 0.057\n",
      "2023-12-30 17:43:51 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.016 | 0.056\n",
      "2023-12-30 17:43:51 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.027 | 0.057\n",
      "2023-12-30 17:43:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.017 | 0.062\n",
      "2023-12-30 17:43:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.013 | 0.061\n",
      "2023-12-30 17:43:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.018 | 0.058\n",
      "2023-12-30 17:43:52 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.024 | 0.059\n",
      "2023-12-30 17:43:53 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.020 | 0.061\n",
      "2023-12-30 17:43:53 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.020 | 0.060\n",
      "2023-12-30 17:43:53 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.022 | 0.063\n",
      "2023-12-30 17:43:54 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.020 | 0.063\n",
      "2023-12-30 17:43:54 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.020 | 0.059\n",
      "2023-12-30 17:43:54 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.010 | 0.059\n",
      "2023-12-30 17:43:54 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.020 | 0.060\n",
      "2023-12-30 17:43:55 - INFO     | Early stopping: no decrease (0.058 vs 0.060); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:08<00:55,  9.18s/it]2023-12-30 17:43:55 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 17:43:55 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.014 | 0.060\n",
      "2023-12-30 17:43:55 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.018 | 0.059\n",
      "2023-12-30 17:43:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.012 | 0.058\n",
      "2023-12-30 17:43:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.010 | 0.059\n",
      "2023-12-30 17:43:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.025 | 0.059\n",
      "2023-12-30 17:43:56 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.013 | 0.060\n",
      "2023-12-30 17:43:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.015 | 0.059\n",
      "2023-12-30 17:43:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.011 | 0.059\n",
      "2023-12-30 17:43:57 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.008 | 0.060\n",
      "2023-12-30 17:43:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.017 | 0.059\n",
      "2023-12-30 17:43:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.015 | 0.061\n",
      "2023-12-30 17:43:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.017 | 0.058\n",
      "2023-12-30 17:43:58 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.014 | 0.060\n",
      "2023-12-30 17:43:59 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.017 | 0.063\n",
      "2023-12-30 17:43:59 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.020 | 0.066\n",
      "2023-12-30 17:43:59 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.038 | 0.060\n",
      "2023-12-30 17:44:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.012 | 0.061\n",
      "2023-12-30 17:44:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.012 | 0.065\n",
      "2023-12-30 17:44:00 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.018 | 0.061\n",
      "2023-12-30 17:44:01 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.021 | 0.063\n",
      "2023-12-30 17:44:01 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.017 | 0.061\n",
      "2023-12-30 17:44:01 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.024 | 0.061\n",
      "2023-12-30 17:44:02 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.025 | 0.061\n",
      "2023-12-30 17:44:02 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.016 | 0.062\n",
      "2023-12-30 17:44:02 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.021 | 0.062\n",
      "2023-12-30 17:44:03 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.019 | 0.060\n",
      "2023-12-30 17:44:03 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.024 | 0.059\n",
      "2023-12-30 17:44:03 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.025 | 0.059\n",
      "2023-12-30 17:44:03 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.010 | 0.060\n",
      "2023-12-30 17:44:04 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:04 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:04 - INFO     | Early stopping: no decrease (0.058 vs 0.062); counter: 3 out of 3\n",
      "2023-12-30 17:44:04 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:44:04 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 75%|███████▌  | 15/20 [02:17<00:46,  9.32s/it]2023-12-30 17:44:04 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 17:44:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.009 | 0.062\n",
      "2023-12-30 17:44:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.017 | 0.060\n",
      "2023-12-30 17:44:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.011 | 0.059\n",
      "2023-12-30 17:44:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.015 | 0.058\n",
      "2023-12-30 17:44:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.014 | 0.058\n",
      "2023-12-30 17:44:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.018 | 0.059\n",
      "2023-12-30 17:44:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.011 | 0.058\n",
      "2023-12-30 17:44:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.016 | 0.058\n",
      "2023-12-30 17:44:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.011 | 0.059\n",
      "2023-12-30 17:44:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.015 | 0.060\n",
      "2023-12-30 17:44:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.015 | 0.060\n",
      "2023-12-30 17:44:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.010 | 0.059\n",
      "2023-12-30 17:44:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.013 | 0.059\n",
      "2023-12-30 17:44:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.022 | 0.059\n",
      "2023-12-30 17:44:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.011 | 0.059\n",
      "2023-12-30 17:44:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.016 | 0.059\n",
      "2023-12-30 17:44:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.030 | 0.059\n",
      "2023-12-30 17:44:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.022 | 0.059\n",
      "2023-12-30 17:44:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.017 | 0.060\n",
      "2023-12-30 17:44:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:11 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.009 | 0.060\n",
      "2023-12-30 17:44:11 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.011 | 0.060\n",
      "2023-12-30 17:44:11 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.015 | 0.060\n",
      "2023-12-30 17:44:11 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.017 | 0.059\n",
      "2023-12-30 17:44:12 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:12 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.017 | 0.060\n",
      "2023-12-30 17:44:12 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.012 | 0.059\n",
      "2023-12-30 17:44:13 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.018 | 0.060\n",
      "2023-12-30 17:44:13 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.015 | 0.060\n",
      "2023-12-30 17:44:13 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.022 | 0.061\n",
      "2023-12-30 17:44:13 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.010 | 0.060\n",
      "2023-12-30 17:44:14 - INFO     | Early stopping: no decrease (0.058 vs 0.061); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:27<00:37,  9.33s/it]2023-12-30 17:44:14 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 17:44:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.014 | 0.061\n",
      "2023-12-30 17:44:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.015 | 0.060\n",
      "2023-12-30 17:44:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.016 | 0.060\n",
      "2023-12-30 17:44:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.010 | 0.061\n",
      "2023-12-30 17:44:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.020 | 0.061\n",
      "2023-12-30 17:44:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.025 | 0.060\n",
      "2023-12-30 17:44:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.011 | 0.061\n",
      "2023-12-30 17:44:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.025 | 0.061\n",
      "2023-12-30 17:44:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.009 | 0.061\n",
      "2023-12-30 17:44:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.021 | 0.061\n",
      "2023-12-30 17:44:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.014 | 0.061\n",
      "2023-12-30 17:44:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.013 | 0.060\n",
      "2023-12-30 17:44:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.011 | 0.060\n",
      "2023-12-30 17:44:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.012 | 0.060\n",
      "2023-12-30 17:44:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.025 | 0.060\n",
      "2023-12-30 17:44:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.017 | 0.059\n",
      "2023-12-30 17:44:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.010 | 0.060\n",
      "2023-12-30 17:44:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.011 | 0.060\n",
      "2023-12-30 17:44:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.012 | 0.061\n",
      "2023-12-30 17:44:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.014 | 0.061\n",
      "2023-12-30 17:44:21 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.009 | 0.061\n",
      "2023-12-30 17:44:21 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:21 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.019 | 0.062\n",
      "2023-12-30 17:44:21 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.007 | 0.061\n",
      "2023-12-30 17:44:22 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.011 | 0.060\n",
      "2023-12-30 17:44:22 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.009 | 0.062\n",
      "2023-12-30 17:44:22 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.014 | 0.061\n",
      "2023-12-30 17:44:22 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.021 | 0.061\n",
      "2023-12-30 17:44:23 - INFO     | Early stopping: no decrease (0.058 vs 0.061); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:36<00:27,  9.24s/it]2023-12-30 17:44:23 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 17:44:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.019 | 0.061\n",
      "2023-12-30 17:44:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.015 | 0.062\n",
      "2023-12-30 17:44:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.011 | 0.062\n",
      "2023-12-30 17:44:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.013 | 0.061\n",
      "2023-12-30 17:44:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.021 | 0.062\n",
      "2023-12-30 17:44:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.017 | 0.061\n",
      "2023-12-30 17:44:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.011 | 0.062\n",
      "2023-12-30 17:44:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.010 | 0.063\n",
      "2023-12-30 17:44:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.013 | 0.060\n",
      "2023-12-30 17:44:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.018 | 0.061\n",
      "2023-12-30 17:44:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.011 | 0.061\n",
      "2023-12-30 17:44:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.010 | 0.061\n",
      "2023-12-30 17:44:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.015 | 0.061\n",
      "2023-12-30 17:44:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.014 | 0.060\n",
      "2023-12-30 17:44:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.024 | 0.061\n",
      "2023-12-30 17:44:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.021 | 0.062\n",
      "2023-12-30 17:44:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.012 | 0.062\n",
      "2023-12-30 17:44:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.015 | 0.061\n",
      "2023-12-30 17:44:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.017 | 0.061\n",
      "2023-12-30 17:44:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.010 | 0.061\n",
      "2023-12-30 17:44:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.014 | 0.061\n",
      "2023-12-30 17:44:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.007 | 0.061\n",
      "2023-12-30 17:44:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.009 | 0.061\n",
      "2023-12-30 17:44:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.018 | 0.061\n",
      "2023-12-30 17:44:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.016 | 0.061\n",
      "2023-12-30 17:44:31 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.010 | 0.061\n",
      "2023-12-30 17:44:31 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.011 | 0.061\n",
      "2023-12-30 17:44:31 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.013 | 0.062\n",
      "2023-12-30 17:44:32 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.017 | 0.062\n",
      "2023-12-30 17:44:32 - INFO     | Early stopping: no decrease (0.058 vs 0.062); counter: 3 out of 3\n",
      "2023-12-30 17:44:32 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:44:32 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 90%|█████████ | 18/20 [02:45<00:18,  9.21s/it]2023-12-30 17:44:32 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 17:44:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.037 | 0.062\n",
      "2023-12-30 17:44:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.014 | 0.062\n",
      "2023-12-30 17:44:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.011 | 0.062\n",
      "2023-12-30 17:44:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.011 | 0.061\n",
      "2023-12-30 17:44:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.009 | 0.061\n",
      "2023-12-30 17:44:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.008 | 0.061\n",
      "2023-12-30 17:44:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.019 | 0.061\n",
      "2023-12-30 17:44:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.013 | 0.061\n",
      "2023-12-30 17:44:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.012 | 0.061\n",
      "2023-12-30 17:44:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.008 | 0.061\n",
      "2023-12-30 17:44:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.011 | 0.062\n",
      "2023-12-30 17:44:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.021 | 0.061\n",
      "2023-12-30 17:44:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.008 | 0.062\n",
      "2023-12-30 17:44:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.006 | 0.062\n",
      "2023-12-30 17:44:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.008 | 0.062\n",
      "2023-12-30 17:44:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.014 | 0.062\n",
      "2023-12-30 17:44:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.009 | 0.062\n",
      "2023-12-30 17:44:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.014 | 0.063\n",
      "2023-12-30 17:44:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.011 | 0.063\n",
      "2023-12-30 17:44:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.013 | 0.062\n",
      "2023-12-30 17:44:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.006 | 0.062\n",
      "2023-12-30 17:44:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.020 | 0.062\n",
      "2023-12-30 17:44:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.009 | 0.062\n",
      "2023-12-30 17:44:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.014 | 0.062\n",
      "2023-12-30 17:44:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.020 | 0.062\n",
      "2023-12-30 17:44:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.012 | 0.062\n",
      "2023-12-30 17:44:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.016 | 0.062\n",
      "2023-12-30 17:44:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.015 | 0.061\n",
      "2023-12-30 17:44:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.016 | 0.061\n",
      "2023-12-30 17:44:41 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.026 | 0.061\n",
      "2023-12-30 17:44:41 - INFO     | Early stopping: no decrease (0.058 vs 0.061); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [02:54<00:09,  9.20s/it]2023-12-30 17:44:41 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 17:44:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.004 | 0.061\n",
      "2023-12-30 17:44:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.012 | 0.061\n",
      "2023-12-30 17:44:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.011 | 0.062\n",
      "2023-12-30 17:44:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.015 | 0.061\n",
      "2023-12-30 17:44:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.021 | 0.062\n",
      "2023-12-30 17:44:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.010 | 0.062\n",
      "2023-12-30 17:44:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.007 | 0.062\n",
      "2023-12-30 17:44:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.010 | 0.062\n",
      "2023-12-30 17:44:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.014 | 0.062\n",
      "2023-12-30 17:44:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.015 | 0.062\n",
      "2023-12-30 17:44:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.009 | 0.063\n",
      "2023-12-30 17:44:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.017 | 0.062\n",
      "2023-12-30 17:44:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.010 | 0.062\n",
      "2023-12-30 17:44:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.010 | 0.062\n",
      "2023-12-30 17:44:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.012 | 0.062\n",
      "2023-12-30 17:44:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.016 | 0.062\n",
      "2023-12-30 17:44:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.009 | 0.062\n",
      "2023-12-30 17:44:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.013 | 0.062\n",
      "2023-12-30 17:44:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.010 | 0.062\n",
      "2023-12-30 17:44:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.016 | 0.062\n",
      "2023-12-30 17:44:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.013 | 0.062\n",
      "2023-12-30 17:44:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.009 | 0.062\n",
      "2023-12-30 17:44:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.018 | 0.062\n",
      "2023-12-30 17:44:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.010 | 0.062\n",
      "2023-12-30 17:44:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.014 | 0.062\n",
      "2023-12-30 17:44:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.011 | 0.063\n",
      "2023-12-30 17:44:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.013 | 0.063\n",
      "2023-12-30 17:44:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.012 | 0.062\n",
      "2023-12-30 17:44:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.013 | 0.063\n",
      "2023-12-30 17:44:50 - INFO     | Early stopping: no decrease (0.058 vs 0.063); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:03<00:00,  9.20s/it]\n",
      "2023-12-30 17:44:50 - INFO     | Best validation loss: 0.058\n",
      "2023-12-30 17:44:50 - INFO     | Best early stopping index/epoch: 8\n",
      "2023-12-30 17:44:51 - INFO     | Average Loss on test set: 0.063\n",
      "2023-12-30 17:44:53 - INFO     | Weighted Precision: 0.982, Recall: 0.982, F1: 0.982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>████████▄▄▄▄▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>██▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>██▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_validation_loss</td><td>0.05778</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.01287</td></tr><tr><td>step_validation_loss</td><td>0.06259</td></tr><tr><td>test_loss</td><td>0.06272</td></tr><tr><td>weighted_f1</td><td>0.98215</td></tr><tr><td>weighted_precision</td><td>0.98217</td></tr><tr><td>weighted_recall</td><td>0.98214</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-sweep-8</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i2hyee2e' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i2hyee2e</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_174146-i2hyee2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h05p313x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 32]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_174504-h05p313x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/h05p313x' target=\"_blank\">twilight-sweep-9</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/h05p313x' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/h05p313x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 32], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:45:05 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 17:45:05 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 19.998 | 71.004\n",
      "2023-12-30 17:45:05 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 8.558 | 1.014\n",
      "2023-12-30 17:45:06 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 0.666 | 0.490\n",
      "2023-12-30 17:45:06 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 0.395 | 0.349\n",
      "2023-12-30 17:45:06 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 0.329 | 0.371\n",
      "2023-12-30 17:45:06 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 0.330 | 0.274\n",
      "2023-12-30 17:45:07 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 0.270 | 0.258\n",
      "2023-12-30 17:45:07 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.288 | 0.227\n",
      "2023-12-30 17:45:07 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.249 | 0.209\n",
      "2023-12-30 17:45:08 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.190 | 0.208\n",
      "2023-12-30 17:45:08 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.213 | 0.184\n",
      "2023-12-30 17:45:08 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.196 | 0.205\n",
      "2023-12-30 17:45:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.180 | 0.193\n",
      "2023-12-30 17:45:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.173 | 0.191\n",
      "2023-12-30 17:45:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.203 | 0.187\n",
      "2023-12-30 17:45:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.165 | 0.153\n",
      "2023-12-30 17:45:10 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.137 | 0.157\n",
      "2023-12-30 17:45:10 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.162 | 0.191\n",
      "2023-12-30 17:45:10 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.155 | 0.222\n",
      "2023-12-30 17:45:11 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.170 | 0.184\n",
      "2023-12-30 17:45:11 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.163 | 0.199\n",
      "2023-12-30 17:45:11 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.144 | 0.163\n",
      "2023-12-30 17:45:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.142 | 0.159\n",
      "2023-12-30 17:45:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.194 | 0.146\n",
      "2023-12-30 17:45:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.177 | 0.182\n",
      "2023-12-30 17:45:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.138 | 0.165\n",
      "2023-12-30 17:45:13 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.137 | 0.150\n",
      "2023-12-30 17:45:13 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.155 | 0.140\n",
      "2023-12-30 17:45:13 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.117 | 0.153\n",
      "2023-12-30 17:45:14 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.126 | 0.138\n",
      "2023-12-30 17:45:14 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.146 | 0.163\n",
      "2023-12-30 17:45:14 - INFO     | Early stopping: loss decreased (inf -> 0.163; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:58,  9.38s/it]2023-12-30 17:45:14 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 17:45:14 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.033 | 0.164\n",
      "2023-12-30 17:45:15 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.150 | 0.131\n",
      "2023-12-30 17:45:15 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.123 | 0.145\n",
      "2023-12-30 17:45:15 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.132 | 0.162\n",
      "2023-12-30 17:45:16 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.137 | 0.139\n",
      "2023-12-30 17:45:16 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.143 | 0.136\n",
      "2023-12-30 17:45:16 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.161 | 0.162\n",
      "2023-12-30 17:45:16 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.131 | 0.156\n",
      "2023-12-30 17:45:17 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.139 | 0.150\n",
      "2023-12-30 17:45:17 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.118 | 0.150\n",
      "2023-12-30 17:45:17 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.121 | 0.152\n",
      "2023-12-30 17:45:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.119 | 0.131\n",
      "2023-12-30 17:45:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.140 | 0.155\n",
      "2023-12-30 17:45:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.131 | 0.140\n",
      "2023-12-30 17:45:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.161 | 0.125\n",
      "2023-12-30 17:45:19 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.084 | 0.126\n",
      "2023-12-30 17:45:19 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.135 | 0.157\n",
      "2023-12-30 17:45:19 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.113 | 0.133\n",
      "2023-12-30 17:45:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.160 | 0.204\n",
      "2023-12-30 17:45:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.120 | 0.146\n",
      "2023-12-30 17:45:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.129 | 0.182\n",
      "2023-12-30 17:45:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.156 | 0.132\n",
      "2023-12-30 17:45:21 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.141 | 0.150\n",
      "2023-12-30 17:45:21 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.125 | 0.175\n",
      "2023-12-30 17:45:21 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.183 | 0.217\n",
      "2023-12-30 17:45:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.163 | 0.144\n",
      "2023-12-30 17:45:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.138 | 0.150\n",
      "2023-12-30 17:45:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.152 | 0.142\n",
      "2023-12-30 17:45:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.124 | 0.157\n",
      "2023-12-30 17:45:23 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.133 | 0.148\n",
      "2023-12-30 17:45:23 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.113 | 0.139\n",
      "2023-12-30 17:45:23 - INFO     | Early stopping: loss decreased (0.163 -> 0.143; -12.6%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:46,  9.25s/it]2023-12-30 17:45:23 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 17:45:24 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.081 | 0.141\n",
      "2023-12-30 17:45:24 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.116 | 0.139\n",
      "2023-12-30 17:45:24 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.123 | 0.183\n",
      "2023-12-30 17:45:25 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.161 | 0.215\n",
      "2023-12-30 17:45:25 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.156 | 0.150\n",
      "2023-12-30 17:45:25 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.151 | 0.153\n",
      "2023-12-30 17:45:25 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.119 | 0.250\n",
      "2023-12-30 17:45:26 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.152 | 0.126\n",
      "2023-12-30 17:45:26 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.084 | 0.139\n",
      "2023-12-30 17:45:26 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.115 | 0.130\n",
      "2023-12-30 17:45:27 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.115 | 0.155\n",
      "2023-12-30 17:45:27 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.109 | 0.152\n",
      "2023-12-30 17:45:27 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.140 | 0.155\n",
      "2023-12-30 17:45:27 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.112 | 0.153\n",
      "2023-12-30 17:45:28 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.118 | 0.146\n",
      "2023-12-30 17:45:28 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.122 | 0.143\n",
      "2023-12-30 17:45:28 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.146 | 0.144\n",
      "2023-12-30 17:45:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.138 | 0.158\n",
      "2023-12-30 17:45:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.156 | 0.187\n",
      "2023-12-30 17:45:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.162 | 0.127\n",
      "2023-12-30 17:45:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.110 | 0.171\n",
      "2023-12-30 17:45:30 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.128 | 0.150\n",
      "2023-12-30 17:45:30 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.130 | 0.131\n",
      "2023-12-30 17:45:30 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.154 | 0.166\n",
      "2023-12-30 17:45:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.144 | 0.120\n",
      "2023-12-30 17:45:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.120 | 0.140\n",
      "2023-12-30 17:45:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.130 | 0.127\n",
      "2023-12-30 17:45:32 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.109 | 0.165\n",
      "2023-12-30 17:45:32 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.140 | 0.141\n",
      "2023-12-30 17:45:32 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.132 | 0.181\n",
      "2023-12-30 17:45:33 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.155 | 0.142\n",
      "2023-12-30 17:45:33 - INFO     | Early stopping: no decrease (0.143 vs 0.143); counter: 1 out of 3\n",
      " 15%|█▌        | 3/20 [00:28<02:39,  9.38s/it]2023-12-30 17:45:33 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 17:45:33 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.070 | 0.138\n",
      "2023-12-30 17:45:33 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.124 | 0.143\n",
      "2023-12-30 17:45:34 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.095 | 0.137\n",
      "2023-12-30 17:45:34 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.118 | 0.125\n",
      "2023-12-30 17:45:34 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.129 | 0.140\n",
      "2023-12-30 17:45:35 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.126 | 0.154\n",
      "2023-12-30 17:45:35 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.147 | 0.121\n",
      "2023-12-30 17:45:35 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.088 | 0.159\n",
      "2023-12-30 17:45:35 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.095 | 0.182\n",
      "2023-12-30 17:45:36 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.171 | 0.138\n",
      "2023-12-30 17:45:36 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.131 | 0.172\n",
      "2023-12-30 17:45:36 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.160 | 0.150\n",
      "2023-12-30 17:45:37 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.143 | 0.144\n",
      "2023-12-30 17:45:37 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.144 | 0.146\n",
      "2023-12-30 17:45:37 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.134 | 0.134\n",
      "2023-12-30 17:45:37 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.133 | 0.144\n",
      "2023-12-30 17:45:38 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.103 | 0.130\n",
      "2023-12-30 17:45:38 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.139 | 0.160\n",
      "2023-12-30 17:45:38 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.129 | 0.114\n",
      "2023-12-30 17:45:39 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.100 | 0.142\n",
      "2023-12-30 17:45:39 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.121 | 0.125\n",
      "2023-12-30 17:45:39 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.111 | 0.122\n",
      "2023-12-30 17:45:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.149 | 0.118\n",
      "2023-12-30 17:45:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.143 | 0.139\n",
      "2023-12-30 17:45:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.121 | 0.118\n",
      "2023-12-30 17:45:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.103 | 0.126\n",
      "2023-12-30 17:45:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.110 | 0.138\n",
      "2023-12-30 17:45:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.138 | 0.160\n",
      "2023-12-30 17:45:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.144 | 0.135\n",
      "2023-12-30 17:45:42 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.113 | 0.140\n",
      "2023-12-30 17:45:42 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.104 | 0.197\n",
      "2023-12-30 17:45:42 - INFO     | Early stopping: no decrease (0.143 vs 0.188); counter: 2 out of 3\n",
      " 20%|██        | 4/20 [00:37<02:30,  9.42s/it]2023-12-30 17:45:42 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 17:45:43 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.073 | 0.180\n",
      "2023-12-30 17:45:43 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.139 | 0.121\n",
      "2023-12-30 17:45:43 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.103 | 0.131\n",
      "2023-12-30 17:45:44 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.087 | 0.169\n",
      "2023-12-30 17:45:44 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.113 | 0.136\n",
      "2023-12-30 17:45:44 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.117 | 0.151\n",
      "2023-12-30 17:45:44 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.100 | 0.151\n",
      "2023-12-30 17:45:45 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.133 | 0.145\n",
      "2023-12-30 17:45:45 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.114 | 0.181\n",
      "2023-12-30 17:45:45 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.102 | 0.176\n",
      "2023-12-30 17:45:46 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.182 | 0.178\n",
      "2023-12-30 17:45:46 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.153 | 0.145\n",
      "2023-12-30 17:45:46 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.109 | 0.158\n",
      "2023-12-30 17:45:47 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.137 | 0.177\n",
      "2023-12-30 17:45:47 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.119 | 0.135\n",
      "2023-12-30 17:45:47 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.144 | 0.196\n",
      "2023-12-30 17:45:47 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.156 | 0.144\n",
      "2023-12-30 17:45:48 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.114 | 0.165\n",
      "2023-12-30 17:45:48 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.128 | 0.190\n",
      "2023-12-30 17:45:48 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.156 | 0.198\n",
      "2023-12-30 17:45:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.193 | 0.221\n",
      "2023-12-30 17:45:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.158 | 0.156\n",
      "2023-12-30 17:45:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.151 | 0.148\n",
      "2023-12-30 17:45:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.143 | 0.162\n",
      "2023-12-30 17:45:50 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.262 | 0.168\n",
      "2023-12-30 17:45:50 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.123 | 0.155\n",
      "2023-12-30 17:45:50 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.119 | 0.175\n",
      "2023-12-30 17:45:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.124 | 0.154\n",
      "2023-12-30 17:45:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.113 | 0.157\n",
      "2023-12-30 17:45:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.152 | 0.181\n",
      "2023-12-30 17:45:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.146 | 0.137\n",
      "2023-12-30 17:45:52 - INFO     | Early stopping: no decrease (0.143 vs 0.138); counter: 3 out of 3\n",
      "2023-12-30 17:45:52 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:45:52 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 25%|██▌       | 5/20 [00:46<02:21,  9.43s/it]2023-12-30 17:45:52 - INFO     | Epoch: 5 | Learning Rate: 0.003\n",
      "2023-12-30 17:45:52 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 280064 examples: 0.040 | 0.138\n",
      "2023-12-30 17:45:52 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 281920 examples: 0.107 | 0.145\n",
      "2023-12-30 17:45:53 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 283776 examples: 0.071 | 0.123\n",
      "2023-12-30 17:45:53 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 285632 examples: 0.072 | 0.134\n",
      "2023-12-30 17:45:53 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 287488 examples: 0.093 | 0.132\n",
      "2023-12-30 17:45:54 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 289344 examples: 0.074 | 0.124\n",
      "2023-12-30 17:45:54 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 291200 examples: 0.099 | 0.118\n",
      "2023-12-30 17:45:54 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 293056 examples: 0.090 | 0.121\n",
      "2023-12-30 17:45:54 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 294912 examples: 0.069 | 0.128\n",
      "2023-12-30 17:45:55 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 296768 examples: 0.125 | 0.117\n",
      "2023-12-30 17:45:55 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 298624 examples: 0.079 | 0.120\n",
      "2023-12-30 17:45:55 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 300480 examples: 0.079 | 0.113\n",
      "2023-12-30 17:45:56 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 302336 examples: 0.106 | 0.116\n",
      "2023-12-30 17:45:56 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 304192 examples: 0.096 | 0.135\n",
      "2023-12-30 17:45:56 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 306048 examples: 0.100 | 0.105\n",
      "2023-12-30 17:45:56 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 307904 examples: 0.080 | 0.108\n",
      "2023-12-30 17:45:57 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 309760 examples: 0.099 | 0.115\n",
      "2023-12-30 17:45:57 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 311616 examples: 0.086 | 0.136\n",
      "2023-12-30 17:45:57 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 313472 examples: 0.094 | 0.114\n",
      "2023-12-30 17:45:58 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 315328 examples: 0.064 | 0.102\n",
      "2023-12-30 17:45:58 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 317184 examples: 0.098 | 0.112\n",
      "2023-12-30 17:45:58 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 319040 examples: 0.064 | 0.102\n",
      "2023-12-30 17:45:58 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 320896 examples: 0.080 | 0.096\n",
      "2023-12-30 17:45:59 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 322752 examples: 0.069 | 0.101\n",
      "2023-12-30 17:45:59 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 324608 examples: 0.075 | 0.113\n",
      "2023-12-30 17:45:59 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 326464 examples: 0.079 | 0.093\n",
      "2023-12-30 17:46:00 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 328320 examples: 0.110 | 0.121\n",
      "2023-12-30 17:46:00 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 330176 examples: 0.090 | 0.123\n",
      "2023-12-30 17:46:00 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 332032 examples: 0.075 | 0.122\n",
      "2023-12-30 17:46:00 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 333888 examples: 0.105 | 0.105\n",
      "2023-12-30 17:46:01 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 335744 examples: 0.122 | 0.104\n",
      "2023-12-30 17:46:01 - INFO     | Early stopping: loss decreased (0.143 -> 0.105; -26.3%). Caching model state.\n",
      " 30%|███       | 6/20 [00:56<02:11,  9.37s/it]2023-12-30 17:46:01 - INFO     | Epoch: 6 | Learning Rate: 0.003\n",
      "2023-12-30 17:46:01 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 336064 examples: 0.025 | 0.105\n",
      "2023-12-30 17:46:02 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 337920 examples: 0.071 | 0.122\n",
      "2023-12-30 17:46:02 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 339776 examples: 0.064 | 0.095\n",
      "2023-12-30 17:46:02 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 341632 examples: 0.068 | 0.097\n",
      "2023-12-30 17:46:02 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 343488 examples: 0.066 | 0.095\n",
      "2023-12-30 17:46:03 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 345344 examples: 0.074 | 0.092\n",
      "2023-12-30 17:46:03 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 347200 examples: 0.066 | 0.096\n",
      "2023-12-30 17:46:03 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 349056 examples: 0.085 | 0.094\n",
      "2023-12-30 17:46:04 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 350912 examples: 0.078 | 0.105\n",
      "2023-12-30 17:46:04 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 352768 examples: 0.080 | 0.103\n",
      "2023-12-30 17:46:04 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 354624 examples: 0.073 | 0.093\n",
      "2023-12-30 17:46:04 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 356480 examples: 0.100 | 0.097\n",
      "2023-12-30 17:46:05 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 358336 examples: 0.052 | 0.101\n",
      "2023-12-30 17:46:05 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 360192 examples: 0.070 | 0.098\n",
      "2023-12-30 17:46:05 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 362048 examples: 0.071 | 0.143\n",
      "2023-12-30 17:46:06 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 363904 examples: 0.101 | 0.111\n",
      "2023-12-30 17:46:06 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 365760 examples: 0.075 | 0.113\n",
      "2023-12-30 17:46:06 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 367616 examples: 0.074 | 0.108\n",
      "2023-12-30 17:46:06 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 369472 examples: 0.085 | 0.120\n",
      "2023-12-30 17:46:07 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 371328 examples: 0.087 | 0.108\n",
      "2023-12-30 17:46:07 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 373184 examples: 0.075 | 0.124\n",
      "2023-12-30 17:46:07 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 375040 examples: 0.090 | 0.112\n",
      "2023-12-30 17:46:08 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 376896 examples: 0.071 | 0.108\n",
      "2023-12-30 17:46:08 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 378752 examples: 0.069 | 0.116\n",
      "2023-12-30 17:46:08 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 380608 examples: 0.066 | 0.121\n",
      "2023-12-30 17:46:08 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 382464 examples: 0.081 | 0.101\n",
      "2023-12-30 17:46:09 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 384320 examples: 0.062 | 0.109\n",
      "2023-12-30 17:46:09 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 386176 examples: 0.104 | 0.108\n",
      "2023-12-30 17:46:09 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 388032 examples: 0.063 | 0.097\n",
      "2023-12-30 17:46:10 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 389888 examples: 0.073 | 0.103\n",
      "2023-12-30 17:46:10 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 391744 examples: 0.069 | 0.110\n",
      "2023-12-30 17:46:10 - INFO     | Early stopping: no decrease (0.105 vs 0.108); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:05<02:01,  9.35s/it]2023-12-30 17:46:10 - INFO     | Epoch: 7 | Learning Rate: 0.003\n",
      "2023-12-30 17:46:11 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 392064 examples: 0.034 | 0.109\n",
      "2023-12-30 17:46:11 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 393920 examples: 0.038 | 0.104\n",
      "2023-12-30 17:46:11 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 395776 examples: 0.070 | 0.096\n",
      "2023-12-30 17:46:11 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 397632 examples: 0.068 | 0.099\n",
      "2023-12-30 17:46:12 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 399488 examples: 0.061 | 0.094\n",
      "2023-12-30 17:46:12 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 401344 examples: 0.068 | 0.091\n",
      "2023-12-30 17:46:12 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 403200 examples: 0.049 | 0.101\n",
      "2023-12-30 17:46:13 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 405056 examples: 0.054 | 0.099\n",
      "2023-12-30 17:46:13 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 406912 examples: 0.063 | 0.114\n",
      "2023-12-30 17:46:13 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 408768 examples: 0.075 | 0.116\n",
      "2023-12-30 17:46:14 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 410624 examples: 0.115 | 0.116\n",
      "2023-12-30 17:46:14 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 412480 examples: 0.064 | 0.112\n",
      "2023-12-30 17:46:14 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 414336 examples: 0.058 | 0.107\n",
      "2023-12-30 17:46:14 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 416192 examples: 0.081 | 0.098\n",
      "2023-12-30 17:46:15 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 418048 examples: 0.086 | 0.110\n",
      "2023-12-30 17:46:15 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 419904 examples: 0.070 | 0.117\n",
      "2023-12-30 17:46:15 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 421760 examples: 0.111 | 0.107\n",
      "2023-12-30 17:46:16 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 423616 examples: 0.074 | 0.108\n",
      "2023-12-30 17:46:16 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 425472 examples: 0.071 | 0.102\n",
      "2023-12-30 17:46:16 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 427328 examples: 0.079 | 0.094\n",
      "2023-12-30 17:46:17 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 429184 examples: 0.075 | 0.097\n",
      "2023-12-30 17:46:17 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 431040 examples: 0.090 | 0.109\n",
      "2023-12-30 17:46:17 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 432896 examples: 0.090 | 0.109\n",
      "2023-12-30 17:46:17 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 434752 examples: 0.072 | 0.106\n",
      "2023-12-30 17:46:18 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 436608 examples: 0.060 | 0.110\n",
      "2023-12-30 17:46:18 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 438464 examples: 0.080 | 0.089\n",
      "2023-12-30 17:46:18 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 440320 examples: 0.052 | 0.090\n",
      "2023-12-30 17:46:19 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 442176 examples: 0.082 | 0.098\n",
      "2023-12-30 17:46:19 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 444032 examples: 0.098 | 0.091\n",
      "2023-12-30 17:46:19 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 445888 examples: 0.082 | 0.096\n",
      "2023-12-30 17:46:19 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 447744 examples: 0.072 | 0.107\n",
      "2023-12-30 17:46:20 - INFO     | Early stopping: no decrease (0.105 vs 0.100); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:15<01:52,  9.38s/it]2023-12-30 17:46:20 - INFO     | Epoch: 8 | Learning Rate: 0.003\n",
      "2023-12-30 17:46:20 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 448064 examples: 0.025 | 0.097\n",
      "2023-12-30 17:46:20 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 449920 examples: 0.047 | 0.112\n",
      "2023-12-30 17:46:21 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 451776 examples: 0.056 | 0.094\n",
      "2023-12-30 17:46:21 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 453632 examples: 0.051 | 0.093\n",
      "2023-12-30 17:46:21 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 455488 examples: 0.064 | 0.095\n",
      "2023-12-30 17:46:21 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 457344 examples: 0.061 | 0.099\n",
      "2023-12-30 17:46:22 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 459200 examples: 0.063 | 0.100\n",
      "2023-12-30 17:46:22 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 461056 examples: 0.062 | 0.092\n",
      "2023-12-30 17:46:22 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 462912 examples: 0.050 | 0.093\n",
      "2023-12-30 17:46:23 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 464768 examples: 0.068 | 0.092\n",
      "2023-12-30 17:46:23 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 466624 examples: 0.061 | 0.084\n",
      "2023-12-30 17:46:23 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 468480 examples: 0.049 | 0.101\n",
      "2023-12-30 17:46:23 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 470336 examples: 0.074 | 0.099\n",
      "2023-12-30 17:46:24 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 472192 examples: 0.080 | 0.099\n",
      "2023-12-30 17:46:24 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 474048 examples: 0.065 | 0.102\n",
      "2023-12-30 17:46:24 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 475904 examples: 0.070 | 0.112\n",
      "2023-12-30 17:46:25 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 477760 examples: 0.069 | 0.113\n",
      "2023-12-30 17:46:25 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 479616 examples: 0.091 | 0.129\n",
      "2023-12-30 17:46:25 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 481472 examples: 0.071 | 0.130\n",
      "2023-12-30 17:46:26 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 483328 examples: 0.069 | 0.121\n",
      "2023-12-30 17:46:26 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 485184 examples: 0.064 | 0.117\n",
      "2023-12-30 17:46:26 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 487040 examples: 0.067 | 0.105\n",
      "2023-12-30 17:46:26 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 488896 examples: 0.062 | 0.116\n",
      "2023-12-30 17:46:27 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 490752 examples: 0.070 | 0.114\n",
      "2023-12-30 17:46:27 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 492608 examples: 0.101 | 0.163\n",
      "2023-12-30 17:46:27 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 494464 examples: 0.098 | 0.108\n",
      "2023-12-30 17:46:28 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 496320 examples: 0.071 | 0.113\n",
      "2023-12-30 17:46:28 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 498176 examples: 0.062 | 0.105\n",
      "2023-12-30 17:46:28 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 500032 examples: 0.065 | 0.100\n",
      "2023-12-30 17:46:29 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 501888 examples: 0.076 | 0.100\n",
      "2023-12-30 17:46:29 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 503744 examples: 0.073 | 0.108\n",
      "2023-12-30 17:46:29 - INFO     | Early stopping: no decrease (0.105 vs 0.107); counter: 3 out of 3\n",
      "2023-12-30 17:46:29 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:46:29 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 45%|████▌     | 9/20 [01:24<01:43,  9.39s/it]2023-12-30 17:46:29 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 17:46:29 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.027 | 0.106\n",
      "2023-12-30 17:46:30 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.061 | 0.116\n",
      "2023-12-30 17:46:30 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.052 | 0.100\n",
      "2023-12-30 17:46:30 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.030 | 0.101\n",
      "2023-12-30 17:46:31 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.042 | 0.101\n",
      "2023-12-30 17:46:31 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.062 | 0.098\n",
      "2023-12-30 17:46:31 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.049 | 0.093\n",
      "2023-12-30 17:46:31 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.050 | 0.093\n",
      "2023-12-30 17:46:32 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.033 | 0.093\n",
      "2023-12-30 17:46:32 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.045 | 0.102\n",
      "2023-12-30 17:46:32 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.039 | 0.088\n",
      "2023-12-30 17:46:33 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.037 | 0.084\n",
      "2023-12-30 17:46:33 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.030 | 0.082\n",
      "2023-12-30 17:46:33 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.036 | 0.083\n",
      "2023-12-30 17:46:34 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.049 | 0.082\n",
      "2023-12-30 17:46:34 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.046 | 0.085\n",
      "2023-12-30 17:46:34 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.044 | 0.086\n",
      "2023-12-30 17:46:34 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.033 | 0.089\n",
      "2023-12-30 17:46:35 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.064 | 0.087\n",
      "2023-12-30 17:46:35 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.032 | 0.085\n",
      "2023-12-30 17:46:35 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.040 | 0.091\n",
      "2023-12-30 17:46:36 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.034 | 0.086\n",
      "2023-12-30 17:46:36 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.029 | 0.090\n",
      "2023-12-30 17:46:36 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.044 | 0.091\n",
      "2023-12-30 17:46:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.031 | 0.085\n",
      "2023-12-30 17:46:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.022 | 0.086\n",
      "2023-12-30 17:46:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.042 | 0.088\n",
      "2023-12-30 17:46:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.036 | 0.094\n",
      "2023-12-30 17:46:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.037 | 0.084\n",
      "2023-12-30 17:46:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.039 | 0.090\n",
      "2023-12-30 17:46:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.052 | 0.112\n",
      "2023-12-30 17:46:39 - INFO     | Early stopping: no decrease (0.105 vs 0.111); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:33<01:34,  9.40s/it]2023-12-30 17:46:39 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 17:46:39 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.072 | 0.110\n",
      "2023-12-30 17:46:39 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.020 | 0.084\n",
      "2023-12-30 17:46:39 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.038 | 0.093\n",
      "2023-12-30 17:46:40 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.033 | 0.100\n",
      "2023-12-30 17:46:40 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.033 | 0.098\n",
      "2023-12-30 17:46:40 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.020 | 0.088\n",
      "2023-12-30 17:46:41 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.045 | 0.094\n",
      "2023-12-30 17:46:41 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.019 | 0.091\n",
      "2023-12-30 17:46:41 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.018 | 0.094\n",
      "2023-12-30 17:46:41 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.025 | 0.095\n",
      "2023-12-30 17:46:42 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.036 | 0.092\n",
      "2023-12-30 17:46:42 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.031 | 0.089\n",
      "2023-12-30 17:46:42 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.034 | 0.091\n",
      "2023-12-30 17:46:43 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.018 | 0.098\n",
      "2023-12-30 17:46:43 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.041 | 0.102\n",
      "2023-12-30 17:46:43 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.041 | 0.109\n",
      "2023-12-30 17:46:44 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.056 | 0.094\n",
      "2023-12-30 17:46:44 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.044 | 0.092\n",
      "2023-12-30 17:46:44 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.027 | 0.090\n",
      "2023-12-30 17:46:44 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.037 | 0.097\n",
      "2023-12-30 17:46:45 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.038 | 0.098\n",
      "2023-12-30 17:46:45 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.029 | 0.094\n",
      "2023-12-30 17:46:45 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.033 | 0.097\n",
      "2023-12-30 17:46:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.033 | 0.099\n",
      "2023-12-30 17:46:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.021 | 0.092\n",
      "2023-12-30 17:46:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.032 | 0.097\n",
      "2023-12-30 17:46:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.032 | 0.102\n",
      "2023-12-30 17:46:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.029 | 0.107\n",
      "2023-12-30 17:46:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.040 | 0.105\n",
      "2023-12-30 17:46:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.024 | 0.102\n",
      "2023-12-30 17:46:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.035 | 0.096\n",
      "2023-12-30 17:46:48 - INFO     | Early stopping: loss decreased (0.105 -> 0.095; -9.1%). Caching model state.\n",
      " 55%|█████▌    | 11/20 [01:43<01:24,  9.41s/it]2023-12-30 17:46:48 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 17:46:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.030 | 0.095\n",
      "2023-12-30 17:46:49 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.030 | 0.091\n",
      "2023-12-30 17:46:49 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.017 | 0.091\n",
      "2023-12-30 17:46:49 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.021 | 0.096\n",
      "2023-12-30 17:46:50 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.029 | 0.096\n",
      "2023-12-30 17:46:50 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.019 | 0.099\n",
      "2023-12-30 17:46:50 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.014 | 0.099\n",
      "2023-12-30 17:46:50 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.025 | 0.098\n",
      "2023-12-30 17:46:51 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.016 | 0.096\n",
      "2023-12-30 17:46:51 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.020 | 0.097\n",
      "2023-12-30 17:46:51 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.029 | 0.102\n",
      "2023-12-30 17:46:52 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.029 | 0.111\n",
      "2023-12-30 17:46:52 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.012 | 0.096\n",
      "2023-12-30 17:46:52 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.013 | 0.094\n",
      "2023-12-30 17:46:52 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.015 | 0.096\n",
      "2023-12-30 17:46:53 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.015 | 0.098\n",
      "2023-12-30 17:46:53 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.025 | 0.102\n",
      "2023-12-30 17:46:53 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.016 | 0.104\n",
      "2023-12-30 17:46:54 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.012 | 0.101\n",
      "2023-12-30 17:46:54 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.026 | 0.107\n",
      "2023-12-30 17:46:54 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.027 | 0.114\n",
      "2023-12-30 17:46:54 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.033 | 0.099\n",
      "2023-12-30 17:46:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.027 | 0.103\n",
      "2023-12-30 17:46:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.016 | 0.100\n",
      "2023-12-30 17:46:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.048 | 0.101\n",
      "2023-12-30 17:46:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.025 | 0.093\n",
      "2023-12-30 17:46:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.022 | 0.097\n",
      "2023-12-30 17:46:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.039 | 0.109\n",
      "2023-12-30 17:46:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.033 | 0.098\n",
      "2023-12-30 17:46:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.038 | 0.102\n",
      "2023-12-30 17:46:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.043 | 0.102\n",
      "2023-12-30 17:46:57 - INFO     | Early stopping: no decrease (0.095 vs 0.100); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [01:52<01:15,  9.41s/it]2023-12-30 17:46:57 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 17:46:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.008 | 0.100\n",
      "2023-12-30 17:46:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.024 | 0.102\n",
      "2023-12-30 17:46:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.012 | 0.110\n",
      "2023-12-30 17:46:59 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.024 | 0.141\n",
      "2023-12-30 17:46:59 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.030 | 0.117\n",
      "2023-12-30 17:46:59 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.033 | 0.110\n",
      "2023-12-30 17:47:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.022 | 0.114\n",
      "2023-12-30 17:47:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.022 | 0.111\n",
      "2023-12-30 17:47:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.024 | 0.109\n",
      "2023-12-30 17:47:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.015 | 0.110\n",
      "2023-12-30 17:47:01 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.013 | 0.112\n",
      "2023-12-30 17:47:01 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.015 | 0.117\n",
      "2023-12-30 17:47:01 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.023 | 0.117\n",
      "2023-12-30 17:47:02 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.016 | 0.109\n",
      "2023-12-30 17:47:02 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.018 | 0.109\n",
      "2023-12-30 17:47:02 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.016 | 0.102\n",
      "2023-12-30 17:47:02 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.015 | 0.101\n",
      "2023-12-30 17:47:03 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.022 | 0.105\n",
      "2023-12-30 17:47:03 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.025 | 0.115\n",
      "2023-12-30 17:47:03 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.022 | 0.110\n",
      "2023-12-30 17:47:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.026 | 0.113\n",
      "2023-12-30 17:47:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.025 | 0.108\n",
      "2023-12-30 17:47:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.010 | 0.102\n",
      "2023-12-30 17:47:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.024 | 0.110\n",
      "2023-12-30 17:47:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.032 | 0.114\n",
      "2023-12-30 17:47:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.020 | 0.106\n",
      "2023-12-30 17:47:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.037 | 0.112\n",
      "2023-12-30 17:47:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.023 | 0.117\n",
      "2023-12-30 17:47:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.028 | 0.133\n",
      "2023-12-30 17:47:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.022 | 0.127\n",
      "2023-12-30 17:47:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.025 | 0.118\n",
      "2023-12-30 17:47:07 - INFO     | Early stopping: no decrease (0.095 vs 0.117); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:02<01:05,  9.43s/it]2023-12-30 17:47:07 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 17:47:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.058 | 0.116\n",
      "2023-12-30 17:47:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.020 | 0.117\n",
      "2023-12-30 17:47:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.015 | 0.132\n",
      "2023-12-30 17:47:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.009 | 0.117\n",
      "2023-12-30 17:47:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.009 | 0.120\n",
      "2023-12-30 17:47:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.012 | 0.115\n",
      "2023-12-30 17:47:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.028 | 0.112\n",
      "2023-12-30 17:47:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.007 | 0.116\n",
      "2023-12-30 17:47:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.022 | 0.119\n",
      "2023-12-30 17:47:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.022 | 0.115\n",
      "2023-12-30 17:47:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.016 | 0.129\n",
      "2023-12-30 17:47:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.023 | 0.120\n",
      "2023-12-30 17:47:11 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.023 | 0.118\n",
      "2023-12-30 17:47:11 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.018 | 0.139\n",
      "2023-12-30 17:47:11 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.027 | 0.135\n",
      "2023-12-30 17:47:12 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.025 | 0.143\n",
      "2023-12-30 17:47:12 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.020 | 0.121\n",
      "2023-12-30 17:47:12 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.028 | 0.119\n",
      "2023-12-30 17:47:12 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.044 | 0.119\n",
      "2023-12-30 17:47:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.018 | 0.115\n",
      "2023-12-30 17:47:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.023 | 0.120\n",
      "2023-12-30 17:47:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.018 | 0.118\n",
      "2023-12-30 17:47:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.021 | 0.116\n",
      "2023-12-30 17:47:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.024 | 0.111\n",
      "2023-12-30 17:47:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.015 | 0.113\n",
      "2023-12-30 17:47:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.013 | 0.116\n",
      "2023-12-30 17:47:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.020 | 0.118\n",
      "2023-12-30 17:47:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.021 | 0.125\n",
      "2023-12-30 17:47:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.033 | 0.135\n",
      "2023-12-30 17:47:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.020 | 0.131\n",
      "2023-12-30 17:47:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.016 | 0.134\n",
      "2023-12-30 17:47:16 - INFO     | Early stopping: no decrease (0.095 vs 0.141); counter: 3 out of 3\n",
      "2023-12-30 17:47:16 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:47:16 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 70%|███████   | 14/20 [02:11<00:56,  9.42s/it]2023-12-30 17:47:16 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 17:47:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.004 | 0.140\n",
      "2023-12-30 17:47:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.016 | 0.132\n",
      "2023-12-30 17:47:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.006 | 0.131\n",
      "2023-12-30 17:47:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.003 | 0.127\n",
      "2023-12-30 17:47:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.017 | 0.127\n",
      "2023-12-30 17:47:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.005 | 0.125\n",
      "2023-12-30 17:47:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.011 | 0.128\n",
      "2023-12-30 17:47:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.011 | 0.130\n",
      "2023-12-30 17:47:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.005 | 0.127\n",
      "2023-12-30 17:47:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.011 | 0.126\n",
      "2023-12-30 17:47:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.010 | 0.125\n",
      "2023-12-30 17:47:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.008 | 0.126\n",
      "2023-12-30 17:47:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.005 | 0.120\n",
      "2023-12-30 17:47:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.017 | 0.123\n",
      "2023-12-30 17:47:21 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.009 | 0.120\n",
      "2023-12-30 17:47:21 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.009 | 0.122\n",
      "2023-12-30 17:47:21 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.007 | 0.122\n",
      "2023-12-30 17:47:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.009 | 0.128\n",
      "2023-12-30 17:47:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.014 | 0.124\n",
      "2023-12-30 17:47:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.012 | 0.121\n",
      "2023-12-30 17:47:22 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.009 | 0.123\n",
      "2023-12-30 17:47:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.007 | 0.122\n",
      "2023-12-30 17:47:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.015 | 0.121\n",
      "2023-12-30 17:47:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.003 | 0.121\n",
      "2023-12-30 17:47:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.006 | 0.123\n",
      "2023-12-30 17:47:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.015 | 0.119\n",
      "2023-12-30 17:47:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.006 | 0.120\n",
      "2023-12-30 17:47:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.014 | 0.119\n",
      "2023-12-30 17:47:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.008 | 0.124\n",
      "2023-12-30 17:47:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.006 | 0.133\n",
      "2023-12-30 17:47:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.009 | 0.124\n",
      "2023-12-30 17:47:26 - INFO     | Early stopping: no decrease (0.095 vs 0.124); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:20<00:46,  9.39s/it]2023-12-30 17:47:26 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 17:47:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.007 | 0.123\n",
      "2023-12-30 17:47:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.003 | 0.121\n",
      "2023-12-30 17:47:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.010 | 0.124\n",
      "2023-12-30 17:47:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.004 | 0.121\n",
      "2023-12-30 17:47:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.002 | 0.121\n",
      "2023-12-30 17:47:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.008 | 0.126\n",
      "2023-12-30 17:47:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.003 | 0.124\n",
      "2023-12-30 17:47:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.004 | 0.127\n",
      "2023-12-30 17:47:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.009 | 0.128\n",
      "2023-12-30 17:47:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.004 | 0.130\n",
      "2023-12-30 17:47:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.006 | 0.131\n",
      "2023-12-30 17:47:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.004 | 0.125\n",
      "2023-12-30 17:47:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.003 | 0.126\n",
      "2023-12-30 17:47:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.004 | 0.122\n",
      "2023-12-30 17:47:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.007 | 0.122\n",
      "2023-12-30 17:47:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.003 | 0.129\n",
      "2023-12-30 17:47:31 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.006 | 0.128\n",
      "2023-12-30 17:47:31 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.009 | 0.128\n",
      "2023-12-30 17:47:31 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.002 | 0.126\n",
      "2023-12-30 17:47:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.002 | 0.121\n",
      "2023-12-30 17:47:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.003 | 0.122\n",
      "2023-12-30 17:47:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.003 | 0.121\n",
      "2023-12-30 17:47:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.002 | 0.120\n",
      "2023-12-30 17:47:33 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.004 | 0.119\n",
      "2023-12-30 17:47:33 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.007 | 0.117\n",
      "2023-12-30 17:47:33 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.004 | 0.125\n",
      "2023-12-30 17:47:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.006 | 0.132\n",
      "2023-12-30 17:47:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.005 | 0.131\n",
      "2023-12-30 17:47:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.003 | 0.133\n",
      "2023-12-30 17:47:35 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.001 | 0.135\n",
      "2023-12-30 17:47:35 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.004 | 0.129\n",
      "2023-12-30 17:47:35 - INFO     | Early stopping: no decrease (0.095 vs 0.129); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:30<00:37,  9.44s/it]2023-12-30 17:47:35 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 17:47:35 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.000 | 0.129\n",
      "2023-12-30 17:47:36 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.009 | 0.129\n",
      "2023-12-30 17:47:36 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.002 | 0.130\n",
      "2023-12-30 17:47:36 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.002 | 0.132\n",
      "2023-12-30 17:47:37 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.004 | 0.128\n",
      "2023-12-30 17:47:37 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.002 | 0.126\n",
      "2023-12-30 17:47:37 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.005 | 0.133\n",
      "2023-12-30 17:47:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.004 | 0.132\n",
      "2023-12-30 17:47:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.002 | 0.129\n",
      "2023-12-30 17:47:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.005 | 0.129\n",
      "2023-12-30 17:47:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.002 | 0.131\n",
      "2023-12-30 17:47:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.001 | 0.132\n",
      "2023-12-30 17:47:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.002 | 0.132\n",
      "2023-12-30 17:47:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.004 | 0.136\n",
      "2023-12-30 17:47:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.003 | 0.134\n",
      "2023-12-30 17:47:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.004 | 0.135\n",
      "2023-12-30 17:47:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.008 | 0.133\n",
      "2023-12-30 17:47:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.005 | 0.134\n",
      "2023-12-30 17:47:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.002 | 0.132\n",
      "2023-12-30 17:47:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.004 | 0.133\n",
      "2023-12-30 17:47:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.003 | 0.133\n",
      "2023-12-30 17:47:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.002 | 0.133\n",
      "2023-12-30 17:47:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.002 | 0.137\n",
      "2023-12-30 17:47:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.003 | 0.139\n",
      "2023-12-30 17:47:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.002 | 0.141\n",
      "2023-12-30 17:47:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.005 | 0.139\n",
      "2023-12-30 17:47:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.005 | 0.149\n",
      "2023-12-30 17:47:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.005 | 0.144\n",
      "2023-12-30 17:47:44 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.002 | 0.138\n",
      "2023-12-30 17:47:44 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.002 | 0.138\n",
      "2023-12-30 17:47:44 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.001 | 0.134\n",
      "2023-12-30 17:47:45 - INFO     | Early stopping: no decrease (0.095 vs 0.134); counter: 3 out of 3\n",
      "2023-12-30 17:47:45 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:47:45 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      " 85%|████████▌ | 17/20 [02:39<00:28,  9.42s/it]2023-12-30 17:47:45 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 17:47:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.134\n",
      "2023-12-30 17:47:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.002 | 0.131\n",
      "2023-12-30 17:47:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.001 | 0.132\n",
      "2023-12-30 17:47:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.001 | 0.137\n",
      "2023-12-30 17:47:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.003 | 0.131\n",
      "2023-12-30 17:47:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.001 | 0.129\n",
      "2023-12-30 17:47:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.001 | 0.130\n",
      "2023-12-30 17:47:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.002 | 0.135\n",
      "2023-12-30 17:47:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.001 | 0.135\n",
      "2023-12-30 17:47:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.002 | 0.135\n",
      "2023-12-30 17:47:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.001 | 0.136\n",
      "2023-12-30 17:47:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.001 | 0.136\n",
      "2023-12-30 17:47:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.001 | 0.135\n",
      "2023-12-30 17:47:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.001 | 0.138\n",
      "2023-12-30 17:47:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.002 | 0.136\n",
      "2023-12-30 17:47:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.001 | 0.140\n",
      "2023-12-30 17:47:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.001 | 0.138\n",
      "2023-12-30 17:47:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.001 | 0.137\n",
      "2023-12-30 17:47:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.001 | 0.137\n",
      "2023-12-30 17:47:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.000 | 0.137\n",
      "2023-12-30 17:47:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.000 | 0.139\n",
      "2023-12-30 17:47:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.001 | 0.140\n",
      "2023-12-30 17:47:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.002 | 0.141\n",
      "2023-12-30 17:47:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.001 | 0.137\n",
      "2023-12-30 17:47:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.009 | 0.140\n",
      "2023-12-30 17:47:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.003 | 0.140\n",
      "2023-12-30 17:47:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.001 | 0.142\n",
      "2023-12-30 17:47:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.002 | 0.141\n",
      "2023-12-30 17:47:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.003 | 0.143\n",
      "2023-12-30 17:47:54 - INFO     | Early stopping: no decrease (0.095 vs 0.142); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [02:49<00:18,  9.45s/it]2023-12-30 17:47:54 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 17:47:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.001 | 0.142\n",
      "2023-12-30 17:47:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.002 | 0.140\n",
      "2023-12-30 17:47:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.001 | 0.141\n",
      "2023-12-30 17:47:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.141\n",
      "2023-12-30 17:47:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.001 | 0.140\n",
      "2023-12-30 17:47:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.139\n",
      "2023-12-30 17:47:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.139\n",
      "2023-12-30 17:47:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.000 | 0.138\n",
      "2023-12-30 17:47:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.001 | 0.139\n",
      "2023-12-30 17:47:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.139\n",
      "2023-12-30 17:47:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.001 | 0.140\n",
      "2023-12-30 17:47:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.000 | 0.140\n",
      "2023-12-30 17:47:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.001 | 0.142\n",
      "2023-12-30 17:47:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.142\n",
      "2023-12-30 17:48:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.142\n",
      "2023-12-30 17:48:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.142\n",
      "2023-12-30 17:48:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.007 | 0.143\n",
      "2023-12-30 17:48:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.144\n",
      "2023-12-30 17:48:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.002 | 0.143\n",
      "2023-12-30 17:48:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.001 | 0.143\n",
      "2023-12-30 17:48:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.001 | 0.139\n",
      "2023-12-30 17:48:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.002 | 0.143\n",
      "2023-12-30 17:48:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.001 | 0.140\n",
      "2023-12-30 17:48:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.139\n",
      "2023-12-30 17:48:04 - INFO     | Early stopping: no decrease (0.095 vs 0.139); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [02:58<00:09,  9.45s/it]2023-12-30 17:48:04 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 17:48:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.139\n",
      "2023-12-30 17:48:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.004 | 0.139\n",
      "2023-12-30 17:48:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.000 | 0.139\n",
      "2023-12-30 17:48:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.001 | 0.142\n",
      "2023-12-30 17:48:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.141\n",
      "2023-12-30 17:48:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.001 | 0.140\n",
      "2023-12-30 17:48:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.139\n",
      "2023-12-30 17:48:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.139\n",
      "2023-12-30 17:48:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.140\n",
      "2023-12-30 17:48:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.142\n",
      "2023-12-30 17:48:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.142\n",
      "2023-12-30 17:48:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.000 | 0.142\n",
      "2023-12-30 17:48:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.001 | 0.151\n",
      "2023-12-30 17:48:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.001 | 0.152\n",
      "2023-12-30 17:48:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.149\n",
      "2023-12-30 17:48:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.149\n",
      "2023-12-30 17:48:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.147\n",
      "2023-12-30 17:48:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.147\n",
      "2023-12-30 17:48:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.146\n",
      "2023-12-30 17:48:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.144\n",
      "2023-12-30 17:48:13 - INFO     | Early stopping: no decrease (0.095 vs 0.144); counter: 3 out of 3\n",
      "2023-12-30 17:48:13 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:48:13 - INFO     | Reducing learning rate: 0.0003125 -> 0.00015625\n",
      "100%|██████████| 20/20 [03:08<00:00,  9.40s/it]\n",
      "2023-12-30 17:48:13 - INFO     | Best validation loss: 0.095\n",
      "2023-12-30 17:48:13 - INFO     | Best early stopping index/epoch: 10\n",
      "2023-12-30 17:48:13 - INFO     | Average Loss on test set: 0.164\n",
      "2023-12-30 17:48:15 - INFO     | Weighted Precision: 0.983, Recall: 0.983, F1: 0.983\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>█████▄▄▄▄▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>██████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▂▄▄▄▃▄▃▅▃▃▃▃▂▃▃▃▂▂▁▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▄▅▅▄▅▄▂▅▇▃▂▁▂▂▂▁▂▁▁▁▁▁▂▄▂▃▃▃▃▃▃▃▄▃▄▄▄▄▄</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_validation_loss</td><td>0.09549</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00031</td></tr><tr><td>step_learning_rate</td><td>0.00031</td></tr><tr><td>step_training_loss</td><td>7e-05</td></tr><tr><td>step_validation_loss</td><td>0.14438</td></tr><tr><td>test_loss</td><td>0.16442</td></tr><tr><td>weighted_f1</td><td>0.98285</td></tr><tr><td>weighted_precision</td><td>0.98287</td></tr><tr><td>weighted_recall</td><td>0.98286</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sweep-9</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/h05p313x' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/h05p313x</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_174504-h05p313x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 33boavoo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 32]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_174828-33boavoo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/33boavoo' target=\"_blank\">polar-sweep-10</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/33boavoo' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/33boavoo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 32], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:48:29 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 17:48:29 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 23.549 | 413.470\n",
      "2023-12-30 17:48:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 18.533 | 2.305\n",
      "2023-12-30 17:48:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 2.295 | 2.282\n",
      "2023-12-30 17:48:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 2.264 | 2.236\n",
      "2023-12-30 17:48:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 2.119 | 1.710\n",
      "2023-12-30 17:48:31 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 1.294 | 0.874\n",
      "2023-12-30 17:48:31 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 0.806 | 0.575\n",
      "2023-12-30 17:48:31 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.539 | 0.476\n",
      "2023-12-30 17:48:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.447 | 0.634\n",
      "2023-12-30 17:48:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.383 | 0.326\n",
      "2023-12-30 17:48:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.333 | 0.297\n",
      "2023-12-30 17:48:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.285 | 0.298\n",
      "2023-12-30 17:48:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.255 | 0.252\n",
      "2023-12-30 17:48:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.262 | 0.246\n",
      "2023-12-30 17:48:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.256 | 0.232\n",
      "2023-12-30 17:48:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.252 | 0.218\n",
      "2023-12-30 17:48:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.204 | 0.221\n",
      "2023-12-30 17:48:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.221 | 0.196\n",
      "2023-12-30 17:48:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.209 | 0.189\n",
      "2023-12-30 17:48:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.197 | 0.184\n",
      "2023-12-30 17:48:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.215 | 0.204\n",
      "2023-12-30 17:48:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.175 | 0.182\n",
      "2023-12-30 17:48:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.167 | 0.162\n",
      "2023-12-30 17:48:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.178 | 0.159\n",
      "2023-12-30 17:48:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.161 | 0.158\n",
      "2023-12-30 17:48:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.158 | 0.152\n",
      "2023-12-30 17:48:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.161 | 0.165\n",
      "2023-12-30 17:48:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.163 | 0.147\n",
      "2023-12-30 17:48:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.158 | 0.148\n",
      "2023-12-30 17:48:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.160 | 0.150\n",
      "2023-12-30 17:48:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.164 | 0.137\n",
      "2023-12-30 17:48:38 - INFO     | Early stopping: loss decreased (inf -> 0.133; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:55,  9.25s/it]2023-12-30 17:48:38 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 17:48:38 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.088 | 0.134\n",
      "2023-12-30 17:48:39 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.140 | 0.134\n",
      "2023-12-30 17:48:39 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.135 | 0.132\n",
      "2023-12-30 17:48:39 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.140 | 0.154\n",
      "2023-12-30 17:48:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.114 | 0.125\n",
      "2023-12-30 17:48:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.107 | 0.120\n",
      "2023-12-30 17:48:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.136 | 0.129\n",
      "2023-12-30 17:48:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.092 | 0.126\n",
      "2023-12-30 17:48:41 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.120 | 0.146\n",
      "2023-12-30 17:48:41 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.105 | 0.126\n",
      "2023-12-30 17:48:41 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.108 | 0.123\n",
      "2023-12-30 17:48:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.095 | 0.123\n",
      "2023-12-30 17:48:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.086 | 0.119\n",
      "2023-12-30 17:48:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.111 | 0.129\n",
      "2023-12-30 17:48:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.112 | 0.122\n",
      "2023-12-30 17:48:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.107 | 0.115\n",
      "2023-12-30 17:48:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.103 | 0.119\n",
      "2023-12-30 17:48:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.127 | 0.122\n",
      "2023-12-30 17:48:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.112 | 0.127\n",
      "2023-12-30 17:48:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.099 | 0.106\n",
      "2023-12-30 17:48:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.114 | 0.115\n",
      "2023-12-30 17:48:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.089 | 0.105\n",
      "2023-12-30 17:48:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.109 | 0.100\n",
      "2023-12-30 17:48:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.128 | 0.101\n",
      "2023-12-30 17:48:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.080 | 0.094\n",
      "2023-12-30 17:48:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.079 | 0.113\n",
      "2023-12-30 17:48:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.118 | 0.096\n",
      "2023-12-30 17:48:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.112 | 0.111\n",
      "2023-12-30 17:48:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.111 | 0.097\n",
      "2023-12-30 17:48:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.096 | 0.099\n",
      "2023-12-30 17:48:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.104 | 0.098\n",
      "2023-12-30 17:48:48 - INFO     | Early stopping: loss decreased (0.133 -> 0.104; -22.0%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:48,  9.39s/it]2023-12-30 17:48:48 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 17:48:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.019 | 0.104\n",
      "2023-12-30 17:48:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.084 | 0.096\n",
      "2023-12-30 17:48:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.084 | 0.090\n",
      "2023-12-30 17:48:49 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.073 | 0.095\n",
      "2023-12-30 17:48:49 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.093 | 0.089\n",
      "2023-12-30 17:48:49 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.106 | 0.088\n",
      "2023-12-30 17:48:50 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.085 | 0.095\n",
      "2023-12-30 17:48:50 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.095 | 0.092\n",
      "2023-12-30 17:48:50 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.064 | 0.094\n",
      "2023-12-30 17:48:51 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.095 | 0.089\n",
      "2023-12-30 17:48:51 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.075 | 0.088\n",
      "2023-12-30 17:48:51 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.092 | 0.090\n",
      "2023-12-30 17:48:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.090 | 0.091\n",
      "2023-12-30 17:48:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.072 | 0.088\n",
      "2023-12-30 17:48:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.077 | 0.091\n",
      "2023-12-30 17:48:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.089 | 0.083\n",
      "2023-12-30 17:48:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.075 | 0.087\n",
      "2023-12-30 17:48:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.085 | 0.081\n",
      "2023-12-30 17:48:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.085 | 0.088\n",
      "2023-12-30 17:48:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.083 | 0.091\n",
      "2023-12-30 17:48:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.080 | 0.091\n",
      "2023-12-30 17:48:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.079 | 0.085\n",
      "2023-12-30 17:48:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.073 | 0.090\n",
      "2023-12-30 17:48:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.076 | 0.098\n",
      "2023-12-30 17:48:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.059 | 0.083\n",
      "2023-12-30 17:48:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.069 | 0.084\n",
      "2023-12-30 17:48:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.069 | 0.081\n",
      "2023-12-30 17:48:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.082 | 0.081\n",
      "2023-12-30 17:48:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.088 | 0.082\n",
      "2023-12-30 17:48:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.063 | 0.083\n",
      "2023-12-30 17:48:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.082 | 0.081\n",
      "2023-12-30 17:48:57 - INFO     | Early stopping: loss decreased (0.104 -> 0.097; -6.8%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:28<02:40,  9.44s/it]2023-12-30 17:48:57 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 17:48:57 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.108 | 0.087\n",
      "2023-12-30 17:48:58 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.076 | 0.086\n",
      "2023-12-30 17:48:58 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.075 | 0.075\n",
      "2023-12-30 17:48:58 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.066 | 0.077\n",
      "2023-12-30 17:48:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.064 | 0.074\n",
      "2023-12-30 17:48:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.060 | 0.079\n",
      "2023-12-30 17:48:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.057 | 0.077\n",
      "2023-12-30 17:48:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.070 | 0.074\n",
      "2023-12-30 17:49:00 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.042 | 0.075\n",
      "2023-12-30 17:49:00 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.061 | 0.074\n",
      "2023-12-30 17:49:00 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.082 | 0.082\n",
      "2023-12-30 17:49:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.073 | 0.085\n",
      "2023-12-30 17:49:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.052 | 0.074\n",
      "2023-12-30 17:49:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.060 | 0.078\n",
      "2023-12-30 17:49:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.073 | 0.086\n",
      "2023-12-30 17:49:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.054 | 0.075\n",
      "2023-12-30 17:49:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.082 | 0.079\n",
      "2023-12-30 17:49:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.067 | 0.072\n",
      "2023-12-30 17:49:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.056 | 0.074\n",
      "2023-12-30 17:49:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.076 | 0.074\n",
      "2023-12-30 17:49:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.054 | 0.075\n",
      "2023-12-30 17:49:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.073 | 0.084\n",
      "2023-12-30 17:49:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.068 | 0.070\n",
      "2023-12-30 17:49:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.068 | 0.076\n",
      "2023-12-30 17:49:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.080 | 0.073\n",
      "2023-12-30 17:49:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.069 | 0.072\n",
      "2023-12-30 17:49:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.059 | 0.071\n",
      "2023-12-30 17:49:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.070 | 0.081\n",
      "2023-12-30 17:49:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.057 | 0.073\n",
      "2023-12-30 17:49:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.058 | 0.083\n",
      "2023-12-30 17:49:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.066 | 0.069\n",
      "2023-12-30 17:49:06 - INFO     | Early stopping: loss decreased (0.097 -> 0.068; -29.8%). Caching model state.\n",
      " 20%|██        | 4/20 [00:37<02:29,  9.36s/it]2023-12-30 17:49:06 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 17:49:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.051 | 0.067\n",
      "2023-12-30 17:49:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.058 | 0.074\n",
      "2023-12-30 17:49:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.045 | 0.076\n",
      "2023-12-30 17:49:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.059 | 0.075\n",
      "2023-12-30 17:49:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.059 | 0.072\n",
      "2023-12-30 17:49:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.052 | 0.068\n",
      "2023-12-30 17:49:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.058 | 0.067\n",
      "2023-12-30 17:49:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.042 | 0.073\n",
      "2023-12-30 17:49:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.054 | 0.067\n",
      "2023-12-30 17:49:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.055 | 0.083\n",
      "2023-12-30 17:49:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.069 | 0.075\n",
      "2023-12-30 17:49:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.044 | 0.079\n",
      "2023-12-30 17:49:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.061 | 0.067\n",
      "2023-12-30 17:49:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.061 | 0.068\n",
      "2023-12-30 17:49:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.046 | 0.071\n",
      "2023-12-30 17:49:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.068 | 0.072\n",
      "2023-12-30 17:49:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.066 | 0.063\n",
      "2023-12-30 17:49:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.065 | 0.065\n",
      "2023-12-30 17:49:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.052 | 0.068\n",
      "2023-12-30 17:49:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.055 | 0.072\n",
      "2023-12-30 17:49:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.042 | 0.069\n",
      "2023-12-30 17:49:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.050 | 0.073\n",
      "2023-12-30 17:49:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.044 | 0.070\n",
      "2023-12-30 17:49:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.048 | 0.079\n",
      "2023-12-30 17:49:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.058 | 0.069\n",
      "2023-12-30 17:49:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.052 | 0.071\n",
      "2023-12-30 17:49:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.065 | 0.064\n",
      "2023-12-30 17:49:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.060 | 0.084\n",
      "2023-12-30 17:49:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.054 | 0.064\n",
      "2023-12-30 17:49:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.058 | 0.069\n",
      "2023-12-30 17:49:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.071 | 0.066\n",
      "2023-12-30 17:49:16 - INFO     | Early stopping: no decrease (0.068 vs 0.066); counter: 1 out of 3\n",
      " 25%|██▌       | 5/20 [00:46<02:20,  9.34s/it]2023-12-30 17:49:16 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 17:49:16 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.113 | 0.066\n",
      "2023-12-30 17:49:16 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.046 | 0.068\n",
      "2023-12-30 17:49:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.037 | 0.064\n",
      "2023-12-30 17:49:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.047 | 0.068\n",
      "2023-12-30 17:49:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.027 | 0.063\n",
      "2023-12-30 17:49:17 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.037 | 0.068\n",
      "2023-12-30 17:49:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.044 | 0.069\n",
      "2023-12-30 17:49:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.043 | 0.070\n",
      "2023-12-30 17:49:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.050 | 0.064\n",
      "2023-12-30 17:49:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.066 | 0.066\n",
      "2023-12-30 17:49:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.055 | 0.065\n",
      "2023-12-30 17:49:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.040 | 0.068\n",
      "2023-12-30 17:49:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.048 | 0.072\n",
      "2023-12-30 17:49:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.046 | 0.073\n",
      "2023-12-30 17:49:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.053 | 0.061\n",
      "2023-12-30 17:49:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.043 | 0.068\n",
      "2023-12-30 17:49:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.043 | 0.061\n",
      "2023-12-30 17:49:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.057 | 0.067\n",
      "2023-12-30 17:49:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.058 | 0.073\n",
      "2023-12-30 17:49:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.053 | 0.069\n",
      "2023-12-30 17:49:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.053 | 0.063\n",
      "2023-12-30 17:49:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.037 | 0.066\n",
      "2023-12-30 17:49:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.051 | 0.068\n",
      "2023-12-30 17:49:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.053 | 0.068\n",
      "2023-12-30 17:49:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.058 | 0.066\n",
      "2023-12-30 17:49:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.049 | 0.067\n",
      "2023-12-30 17:49:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.038 | 0.063\n",
      "2023-12-30 17:49:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.045 | 0.064\n",
      "2023-12-30 17:49:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.055 | 0.060\n",
      "2023-12-30 17:49:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.044 | 0.060\n",
      "2023-12-30 17:49:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.050 | 0.065\n",
      "2023-12-30 17:49:25 - INFO     | Early stopping: loss decreased (0.068 -> 0.064; -5.5%). Caching model state.\n",
      " 30%|███       | 6/20 [00:55<02:09,  9.24s/it]2023-12-30 17:49:25 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 17:49:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.017 | 0.063\n",
      "2023-12-30 17:49:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.037 | 0.069\n",
      "2023-12-30 17:49:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.034 | 0.067\n",
      "2023-12-30 17:49:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.028 | 0.067\n",
      "2023-12-30 17:49:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.050 | 0.065\n",
      "2023-12-30 17:49:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.038 | 0.069\n",
      "2023-12-30 17:49:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.037 | 0.066\n",
      "2023-12-30 17:49:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.039 | 0.068\n",
      "2023-12-30 17:49:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.055 | 0.076\n",
      "2023-12-30 17:49:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.066 | 0.066\n",
      "2023-12-30 17:49:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.032 | 0.064\n",
      "2023-12-30 17:49:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.046 | 0.066\n",
      "2023-12-30 17:49:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.048 | 0.062\n",
      "2023-12-30 17:49:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.040 | 0.063\n",
      "2023-12-30 17:49:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.035 | 0.064\n",
      "2023-12-30 17:49:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.041 | 0.062\n",
      "2023-12-30 17:49:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.037 | 0.062\n",
      "2023-12-30 17:49:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.032 | 0.068\n",
      "2023-12-30 17:49:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.033 | 0.062\n",
      "2023-12-30 17:49:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.031 | 0.059\n",
      "2023-12-30 17:49:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.055 | 0.062\n",
      "2023-12-30 17:49:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.041 | 0.067\n",
      "2023-12-30 17:49:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.042 | 0.059\n",
      "2023-12-30 17:49:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.049 | 0.062\n",
      "2023-12-30 17:49:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.027 | 0.072\n",
      "2023-12-30 17:49:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.037 | 0.066\n",
      "2023-12-30 17:49:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.033 | 0.074\n",
      "2023-12-30 17:49:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.043 | 0.065\n",
      "2023-12-30 17:49:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.054 | 0.069\n",
      "2023-12-30 17:49:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.050 | 0.069\n",
      "2023-12-30 17:49:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.048 | 0.061\n",
      "2023-12-30 17:49:34 - INFO     | Early stopping: no decrease (0.064 vs 0.065); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:05<02:00,  9.24s/it]2023-12-30 17:49:34 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 17:49:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.016 | 0.064\n",
      "2023-12-30 17:49:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.040 | 0.064\n",
      "2023-12-30 17:49:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.035 | 0.063\n",
      "2023-12-30 17:49:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.031 | 0.061\n",
      "2023-12-30 17:49:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.034 | 0.066\n",
      "2023-12-30 17:49:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.038 | 0.069\n",
      "2023-12-30 17:49:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.046 | 0.076\n",
      "2023-12-30 17:49:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.045 | 0.067\n",
      "2023-12-30 17:49:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.049 | 0.064\n",
      "2023-12-30 17:49:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.028 | 0.061\n",
      "2023-12-30 17:49:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.029 | 0.065\n",
      "2023-12-30 17:49:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.031 | 0.070\n",
      "2023-12-30 17:49:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.027 | 0.059\n",
      "2023-12-30 17:49:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.036 | 0.059\n",
      "2023-12-30 17:49:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.034 | 0.061\n",
      "2023-12-30 17:49:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.026 | 0.063\n",
      "2023-12-30 17:49:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.046 | 0.067\n",
      "2023-12-30 17:49:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.048 | 0.063\n",
      "2023-12-30 17:49:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.045 | 0.063\n",
      "2023-12-30 17:49:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.035 | 0.067\n",
      "2023-12-30 17:49:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.056 | 0.063\n",
      "2023-12-30 17:49:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.038 | 0.080\n",
      "2023-12-30 17:49:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.030 | 0.063\n",
      "2023-12-30 17:49:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.037 | 0.072\n",
      "2023-12-30 17:49:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.042 | 0.064\n",
      "2023-12-30 17:49:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.031 | 0.068\n",
      "2023-12-30 17:49:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.048 | 0.072\n",
      "2023-12-30 17:49:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.036 | 0.065\n",
      "2023-12-30 17:49:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.028 | 0.068\n",
      "2023-12-30 17:49:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.032 | 0.062\n",
      "2023-12-30 17:49:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.046 | 0.067\n",
      "2023-12-30 17:49:43 - INFO     | Early stopping: no decrease (0.064 vs 0.069); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:14<01:51,  9.29s/it]2023-12-30 17:49:43 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 17:49:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.008 | 0.069\n",
      "2023-12-30 17:49:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.034 | 0.066\n",
      "2023-12-30 17:49:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.026 | 0.064\n",
      "2023-12-30 17:49:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.039 | 0.061\n",
      "2023-12-30 17:49:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.031 | 0.063\n",
      "2023-12-30 17:49:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.035 | 0.062\n",
      "2023-12-30 17:49:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.028 | 0.061\n",
      "2023-12-30 17:49:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.035 | 0.062\n",
      "2023-12-30 17:49:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.027 | 0.064\n",
      "2023-12-30 17:49:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.036 | 0.061\n",
      "2023-12-30 17:49:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.026 | 0.059\n",
      "2023-12-30 17:49:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.043 | 0.060\n",
      "2023-12-30 17:49:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.026 | 0.059\n",
      "2023-12-30 17:49:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.034 | 0.072\n",
      "2023-12-30 17:49:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.026 | 0.062\n",
      "2023-12-30 17:49:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.038 | 0.062\n",
      "2023-12-30 17:49:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.022 | 0.061\n",
      "2023-12-30 17:49:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.028 | 0.065\n",
      "2023-12-30 17:49:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.038 | 0.065\n",
      "2023-12-30 17:49:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.038 | 0.068\n",
      "2023-12-30 17:49:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.044 | 0.063\n",
      "2023-12-30 17:49:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.025 | 0.066\n",
      "2023-12-30 17:49:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.035 | 0.062\n",
      "2023-12-30 17:49:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.037 | 0.060\n",
      "2023-12-30 17:49:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.033 | 0.067\n",
      "2023-12-30 17:49:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.046 | 0.065\n",
      "2023-12-30 17:49:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.045 | 0.064\n",
      "2023-12-30 17:49:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.023 | 0.062\n",
      "2023-12-30 17:49:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.024 | 0.064\n",
      "2023-12-30 17:49:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.030 | 0.063\n",
      "2023-12-30 17:49:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.044 | 0.067\n",
      "2023-12-30 17:49:53 - INFO     | Early stopping: no decrease (0.064 vs 0.066); counter: 3 out of 3\n",
      "2023-12-30 17:49:53 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:49:53 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 45%|████▌     | 9/20 [01:23<01:42,  9.30s/it]2023-12-30 17:49:53 - INFO     | Epoch: 9 | Learning Rate: 0.003\n",
      "2023-12-30 17:49:53 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 504064 examples: 0.147 | 0.062\n",
      "2023-12-30 17:49:53 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 505920 examples: 0.017 | 0.062\n",
      "2023-12-30 17:49:53 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 507776 examples: 0.024 | 0.065\n",
      "2023-12-30 17:49:54 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 509632 examples: 0.018 | 0.062\n",
      "2023-12-30 17:49:54 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 511488 examples: 0.017 | 0.064\n",
      "2023-12-30 17:49:54 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 513344 examples: 0.018 | 0.060\n",
      "2023-12-30 17:49:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 515200 examples: 0.016 | 0.060\n",
      "2023-12-30 17:49:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 517056 examples: 0.021 | 0.065\n",
      "2023-12-30 17:49:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 518912 examples: 0.021 | 0.062\n",
      "2023-12-30 17:49:55 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 520768 examples: 0.025 | 0.060\n",
      "2023-12-30 17:49:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 522624 examples: 0.031 | 0.060\n",
      "2023-12-30 17:49:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 524480 examples: 0.016 | 0.060\n",
      "2023-12-30 17:49:56 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 526336 examples: 0.027 | 0.059\n",
      "2023-12-30 17:49:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 528192 examples: 0.015 | 0.061\n",
      "2023-12-30 17:49:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 530048 examples: 0.020 | 0.072\n",
      "2023-12-30 17:49:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 531904 examples: 0.012 | 0.061\n",
      "2023-12-30 17:49:57 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 533760 examples: 0.021 | 0.061\n",
      "2023-12-30 17:49:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 535616 examples: 0.036 | 0.062\n",
      "2023-12-30 17:49:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 537472 examples: 0.028 | 0.061\n",
      "2023-12-30 17:49:58 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 539328 examples: 0.030 | 0.061\n",
      "2023-12-30 17:49:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 541184 examples: 0.022 | 0.059\n",
      "2023-12-30 17:49:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 543040 examples: 0.027 | 0.062\n",
      "2023-12-30 17:49:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 544896 examples: 0.039 | 0.058\n",
      "2023-12-30 17:49:59 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 546752 examples: 0.025 | 0.059\n",
      "2023-12-30 17:50:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 548608 examples: 0.028 | 0.060\n",
      "2023-12-30 17:50:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 550464 examples: 0.020 | 0.062\n",
      "2023-12-30 17:50:00 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 552320 examples: 0.032 | 0.058\n",
      "2023-12-30 17:50:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 554176 examples: 0.017 | 0.059\n",
      "2023-12-30 17:50:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 556032 examples: 0.016 | 0.057\n",
      "2023-12-30 17:50:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 557888 examples: 0.042 | 0.057\n",
      "2023-12-30 17:50:01 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 559744 examples: 0.027 | 0.062\n",
      "2023-12-30 17:50:02 - INFO     | Early stopping: loss decreased (0.064 -> 0.060; -5.8%). Caching model state.\n",
      " 50%|█████     | 10/20 [01:32<01:32,  9.23s/it]2023-12-30 17:50:02 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 17:50:02 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.122 | 0.061\n",
      "2023-12-30 17:50:02 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.017 | 0.059\n",
      "2023-12-30 17:50:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.016 | 0.059\n",
      "2023-12-30 17:50:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.020 | 0.060\n",
      "2023-12-30 17:50:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.019 | 0.058\n",
      "2023-12-30 17:50:03 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.018 | 0.058\n",
      "2023-12-30 17:50:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.018 | 0.058\n",
      "2023-12-30 17:50:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.013 | 0.056\n",
      "2023-12-30 17:50:04 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.023 | 0.059\n",
      "2023-12-30 17:50:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.021 | 0.063\n",
      "2023-12-30 17:50:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.043 | 0.061\n",
      "2023-12-30 17:50:05 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.019 | 0.057\n",
      "2023-12-30 17:50:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.015 | 0.057\n",
      "2023-12-30 17:50:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.020 | 0.059\n",
      "2023-12-30 17:50:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.014 | 0.059\n",
      "2023-12-30 17:50:06 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.026 | 0.059\n",
      "2023-12-30 17:50:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.018 | 0.060\n",
      "2023-12-30 17:50:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.020 | 0.058\n",
      "2023-12-30 17:50:07 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.016 | 0.059\n",
      "2023-12-30 17:50:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.020 | 0.061\n",
      "2023-12-30 17:50:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.029 | 0.060\n",
      "2023-12-30 17:50:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.023 | 0.060\n",
      "2023-12-30 17:50:08 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.017 | 0.057\n",
      "2023-12-30 17:50:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.024 | 0.059\n",
      "2023-12-30 17:50:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.014 | 0.057\n",
      "2023-12-30 17:50:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.032 | 0.058\n",
      "2023-12-30 17:50:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.030 | 0.059\n",
      "2023-12-30 17:50:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.021 | 0.064\n",
      "2023-12-30 17:50:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.022 | 0.060\n",
      "2023-12-30 17:50:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.014 | 0.061\n",
      "2023-12-30 17:50:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.026 | 0.060\n",
      "2023-12-30 17:50:11 - INFO     | Early stopping: no decrease (0.060 vs 0.058); counter: 1 out of 3\n",
      " 55%|█████▌    | 11/20 [01:42<01:23,  9.28s/it]2023-12-30 17:50:11 - INFO     | Epoch: 11 | Learning Rate: 0.003\n",
      "2023-12-30 17:50:11 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 616064 examples: 0.006 | 0.059\n",
      "2023-12-30 17:50:12 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 617920 examples: 0.012 | 0.058\n",
      "2023-12-30 17:50:12 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 619776 examples: 0.018 | 0.057\n",
      "2023-12-30 17:50:12 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 621632 examples: 0.025 | 0.060\n",
      "2023-12-30 17:50:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 623488 examples: 0.011 | 0.059\n",
      "2023-12-30 17:50:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 625344 examples: 0.018 | 0.060\n",
      "2023-12-30 17:50:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 627200 examples: 0.018 | 0.059\n",
      "2023-12-30 17:50:13 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 629056 examples: 0.019 | 0.059\n",
      "2023-12-30 17:50:14 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 630912 examples: 0.021 | 0.059\n",
      "2023-12-30 17:50:14 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 632768 examples: 0.019 | 0.057\n",
      "2023-12-30 17:50:14 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 634624 examples: 0.016 | 0.061\n",
      "2023-12-30 17:50:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 636480 examples: 0.029 | 0.059\n",
      "2023-12-30 17:50:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 638336 examples: 0.021 | 0.059\n",
      "2023-12-30 17:50:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 640192 examples: 0.018 | 0.057\n",
      "2023-12-30 17:50:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 642048 examples: 0.017 | 0.057\n",
      "2023-12-30 17:50:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 643904 examples: 0.015 | 0.060\n",
      "2023-12-30 17:50:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 645760 examples: 0.013 | 0.059\n",
      "2023-12-30 17:50:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 647616 examples: 0.020 | 0.059\n",
      "2023-12-30 17:50:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 649472 examples: 0.011 | 0.060\n",
      "2023-12-30 17:50:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 651328 examples: 0.026 | 0.059\n",
      "2023-12-30 17:50:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 653184 examples: 0.023 | 0.064\n",
      "2023-12-30 17:50:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 655040 examples: 0.027 | 0.059\n",
      "2023-12-30 17:50:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 656896 examples: 0.016 | 0.059\n",
      "2023-12-30 17:50:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 658752 examples: 0.017 | 0.058\n",
      "2023-12-30 17:50:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 660608 examples: 0.026 | 0.064\n",
      "2023-12-30 17:50:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 662464 examples: 0.022 | 0.059\n",
      "2023-12-30 17:50:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 664320 examples: 0.019 | 0.059\n",
      "2023-12-30 17:50:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 666176 examples: 0.017 | 0.059\n",
      "2023-12-30 17:50:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 668032 examples: 0.039 | 0.062\n",
      "2023-12-30 17:50:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 669888 examples: 0.025 | 0.061\n",
      "2023-12-30 17:50:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 671744 examples: 0.021 | 0.058\n",
      "2023-12-30 17:50:20 - INFO     | Early stopping: loss decreased (0.060 -> 0.057; -5.7%). Caching model state.\n",
      " 60%|██████    | 12/20 [01:51<01:13,  9.25s/it]2023-12-30 17:50:20 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 17:50:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.013 | 0.057\n",
      "2023-12-30 17:50:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.023 | 0.060\n",
      "2023-12-30 17:50:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.015 | 0.059\n",
      "2023-12-30 17:50:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.014 | 0.058\n",
      "2023-12-30 17:50:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.013 | 0.060\n",
      "2023-12-30 17:50:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.016 | 0.060\n",
      "2023-12-30 17:50:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.019 | 0.058\n",
      "2023-12-30 17:50:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.015 | 0.060\n",
      "2023-12-30 17:50:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.014 | 0.061\n",
      "2023-12-30 17:50:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.015 | 0.061\n",
      "2023-12-30 17:50:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.018 | 0.059\n",
      "2023-12-30 17:50:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.014 | 0.058\n",
      "2023-12-30 17:50:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.027 | 0.058\n",
      "2023-12-30 17:50:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.013 | 0.058\n",
      "2023-12-30 17:50:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.022 | 0.059\n",
      "2023-12-30 17:50:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.016 | 0.058\n",
      "2023-12-30 17:50:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.027 | 0.064\n",
      "2023-12-30 17:50:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.011 | 0.059\n",
      "2023-12-30 17:50:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.013 | 0.058\n",
      "2023-12-30 17:50:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.021 | 0.063\n",
      "2023-12-30 17:50:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.019 | 0.060\n",
      "2023-12-30 17:50:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.025 | 0.064\n",
      "2023-12-30 17:50:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.020 | 0.058\n",
      "2023-12-30 17:50:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.018 | 0.062\n",
      "2023-12-30 17:50:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.015 | 0.058\n",
      "2023-12-30 17:50:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.020 | 0.057\n",
      "2023-12-30 17:50:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.020 | 0.060\n",
      "2023-12-30 17:50:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.022 | 0.058\n",
      "2023-12-30 17:50:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.025 | 0.059\n",
      "2023-12-30 17:50:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.020 | 0.059\n",
      "2023-12-30 17:50:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.017 | 0.063\n",
      "2023-12-30 17:50:29 - INFO     | Early stopping: no decrease (0.057 vs 0.060); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:00<01:04,  9.19s/it]2023-12-30 17:50:29 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 17:50:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.011 | 0.060\n",
      "2023-12-30 17:50:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.011 | 0.062\n",
      "2023-12-30 17:50:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.021 | 0.057\n",
      "2023-12-30 17:50:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.011 | 0.058\n",
      "2023-12-30 17:50:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.012 | 0.058\n",
      "2023-12-30 17:50:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.017 | 0.058\n",
      "2023-12-30 17:50:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.010 | 0.059\n",
      "2023-12-30 17:50:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.016 | 0.066\n",
      "2023-12-30 17:50:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.018 | 0.059\n",
      "2023-12-30 17:50:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.020 | 0.060\n",
      "2023-12-30 17:50:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.014 | 0.059\n",
      "2023-12-30 17:50:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.014 | 0.068\n",
      "2023-12-30 17:50:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.015 | 0.062\n",
      "2023-12-30 17:50:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.015 | 0.061\n",
      "2023-12-30 17:50:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.013 | 0.061\n",
      "2023-12-30 17:50:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.016 | 0.061\n",
      "2023-12-30 17:50:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.028 | 0.060\n",
      "2023-12-30 17:50:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.020 | 0.060\n",
      "2023-12-30 17:50:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.011 | 0.064\n",
      "2023-12-30 17:50:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.019 | 0.062\n",
      "2023-12-30 17:50:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.021 | 0.063\n",
      "2023-12-30 17:50:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.023 | 0.064\n",
      "2023-12-30 17:50:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.021 | 0.061\n",
      "2023-12-30 17:50:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.022 | 0.061\n",
      "2023-12-30 17:50:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.021 | 0.061\n",
      "2023-12-30 17:50:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.014 | 0.059\n",
      "2023-12-30 17:50:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.013 | 0.065\n",
      "2023-12-30 17:50:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.017 | 0.059\n",
      "2023-12-30 17:50:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.016 | 0.060\n",
      "2023-12-30 17:50:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.019 | 0.058\n",
      "2023-12-30 17:50:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.011 | 0.058\n",
      "2023-12-30 17:50:39 - INFO     | Early stopping: no decrease (0.057 vs 0.057); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:09<00:55,  9.23s/it]2023-12-30 17:50:39 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 17:50:39 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.004 | 0.057\n",
      "2023-12-30 17:50:39 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.010 | 0.057\n",
      "2023-12-30 17:50:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.021 | 0.062\n",
      "2023-12-30 17:50:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.017 | 0.060\n",
      "2023-12-30 17:50:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.014 | 0.066\n",
      "2023-12-30 17:50:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.016 | 0.060\n",
      "2023-12-30 17:50:41 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.012 | 0.062\n",
      "2023-12-30 17:50:41 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.012 | 0.061\n",
      "2023-12-30 17:50:41 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.014 | 0.062\n",
      "2023-12-30 17:50:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.012 | 0.061\n",
      "2023-12-30 17:50:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.021 | 0.065\n",
      "2023-12-30 17:50:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.015 | 0.061\n",
      "2023-12-30 17:50:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.013 | 0.062\n",
      "2023-12-30 17:50:43 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.025 | 0.067\n",
      "2023-12-30 17:50:43 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.010 | 0.060\n",
      "2023-12-30 17:50:43 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.013 | 0.059\n",
      "2023-12-30 17:50:44 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.012 | 0.062\n",
      "2023-12-30 17:50:44 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.017 | 0.061\n",
      "2023-12-30 17:50:44 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.020 | 0.059\n",
      "2023-12-30 17:50:44 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.015 | 0.064\n",
      "2023-12-30 17:50:45 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.024 | 0.063\n",
      "2023-12-30 17:50:45 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.017 | 0.061\n",
      "2023-12-30 17:50:45 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.009 | 0.061\n",
      "2023-12-30 17:50:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.014 | 0.061\n",
      "2023-12-30 17:50:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.012 | 0.060\n",
      "2023-12-30 17:50:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.015 | 0.061\n",
      "2023-12-30 17:50:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.016 | 0.058\n",
      "2023-12-30 17:50:47 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.022 | 0.061\n",
      "2023-12-30 17:50:47 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.012 | 0.062\n",
      "2023-12-30 17:50:47 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.014 | 0.062\n",
      "2023-12-30 17:50:48 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.013 | 0.062\n",
      "2023-12-30 17:50:48 - INFO     | Early stopping: no decrease (0.057 vs 0.060); counter: 3 out of 3\n",
      "2023-12-30 17:50:48 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:50:48 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 75%|███████▌  | 15/20 [02:18<00:46,  9.21s/it]2023-12-30 17:50:48 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 17:50:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.001 | 0.060\n",
      "2023-12-30 17:50:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.008 | 0.060\n",
      "2023-12-30 17:50:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.009 | 0.060\n",
      "2023-12-30 17:50:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.013 | 0.059\n",
      "2023-12-30 17:50:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.011 | 0.059\n",
      "2023-12-30 17:50:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.012 | 0.058\n",
      "2023-12-30 17:50:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.014 | 0.061\n",
      "2023-12-30 17:50:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.008 | 0.060\n",
      "2023-12-30 17:50:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.008 | 0.059\n",
      "2023-12-30 17:50:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.015 | 0.058\n",
      "2023-12-30 17:50:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.009 | 0.059\n",
      "2023-12-30 17:50:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.015 | 0.059\n",
      "2023-12-30 17:50:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.015 | 0.060\n",
      "2023-12-30 17:50:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.013 | 0.058\n",
      "2023-12-30 17:50:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.014 | 0.059\n",
      "2023-12-30 17:50:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.012 | 0.060\n",
      "2023-12-30 17:50:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.013 | 0.058\n",
      "2023-12-30 17:50:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.009 | 0.059\n",
      "2023-12-30 17:50:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.011 | 0.059\n",
      "2023-12-30 17:50:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.010 | 0.061\n",
      "2023-12-30 17:50:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.019 | 0.059\n",
      "2023-12-30 17:50:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.011 | 0.062\n",
      "2023-12-30 17:50:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.010 | 0.059\n",
      "2023-12-30 17:50:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.017 | 0.060\n",
      "2023-12-30 17:50:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.014 | 0.060\n",
      "2023-12-30 17:50:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.014 | 0.059\n",
      "2023-12-30 17:50:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.016 | 0.059\n",
      "2023-12-30 17:50:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.011 | 0.061\n",
      "2023-12-30 17:50:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.011 | 0.060\n",
      "2023-12-30 17:50:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.015 | 0.061\n",
      "2023-12-30 17:50:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.007 | 0.060\n",
      "2023-12-30 17:50:57 - INFO     | Early stopping: no decrease (0.057 vs 0.060); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:28<00:36,  9.21s/it]2023-12-30 17:50:57 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 17:50:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.004 | 0.060\n",
      "2023-12-30 17:50:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.023 | 0.060\n",
      "2023-12-30 17:50:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.010 | 0.061\n",
      "2023-12-30 17:50:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.010 | 0.062\n",
      "2023-12-30 17:50:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.011 | 0.061\n",
      "2023-12-30 17:50:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.012 | 0.060\n",
      "2023-12-30 17:50:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.009 | 0.061\n",
      "2023-12-30 17:50:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.014 | 0.060\n",
      "2023-12-30 17:51:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.008 | 0.059\n",
      "2023-12-30 17:51:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.011 | 0.060\n",
      "2023-12-30 17:51:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.010 | 0.059\n",
      "2023-12-30 17:51:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.008 | 0.059\n",
      "2023-12-30 17:51:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.013 | 0.060\n",
      "2023-12-30 17:51:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.011 | 0.060\n",
      "2023-12-30 17:51:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.019 | 0.061\n",
      "2023-12-30 17:51:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.012 | 0.061\n",
      "2023-12-30 17:51:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.011 | 0.061\n",
      "2023-12-30 17:51:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.014 | 0.062\n",
      "2023-12-30 17:51:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.018 | 0.061\n",
      "2023-12-30 17:51:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.008 | 0.060\n",
      "2023-12-30 17:51:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.011 | 0.060\n",
      "2023-12-30 17:51:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.013 | 0.062\n",
      "2023-12-30 17:51:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.011 | 0.062\n",
      "2023-12-30 17:51:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.006 | 0.060\n",
      "2023-12-30 17:51:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.014 | 0.062\n",
      "2023-12-30 17:51:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.009 | 0.060\n",
      "2023-12-30 17:51:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.010 | 0.059\n",
      "2023-12-30 17:51:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.013 | 0.059\n",
      "2023-12-30 17:51:06 - INFO     | Early stopping: no decrease (0.057 vs 0.059); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:37<00:27,  9.20s/it]2023-12-30 17:51:06 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 17:51:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.009 | 0.060\n",
      "2023-12-30 17:51:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.010 | 0.061\n",
      "2023-12-30 17:51:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.010 | 0.059\n",
      "2023-12-30 17:51:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.009 | 0.060\n",
      "2023-12-30 17:51:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.008 | 0.062\n",
      "2023-12-30 17:51:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.012 | 0.062\n",
      "2023-12-30 17:51:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.021 | 0.064\n",
      "2023-12-30 17:51:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.008 | 0.064\n",
      "2023-12-30 17:51:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.006 | 0.062\n",
      "2023-12-30 17:51:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.017 | 0.063\n",
      "2023-12-30 17:51:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.010 | 0.062\n",
      "2023-12-30 17:51:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.018 | 0.060\n",
      "2023-12-30 17:51:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.013 | 0.061\n",
      "2023-12-30 17:51:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.014 | 0.061\n",
      "2023-12-30 17:51:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.018 | 0.062\n",
      "2023-12-30 17:51:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.015 | 0.061\n",
      "2023-12-30 17:51:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.013 | 0.062\n",
      "2023-12-30 17:51:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.006 | 0.060\n",
      "2023-12-30 17:51:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.014 | 0.061\n",
      "2023-12-30 17:51:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.018 | 0.061\n",
      "2023-12-30 17:51:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.007 | 0.061\n",
      "2023-12-30 17:51:15 - INFO     | Early stopping: no decrease (0.057 vs 0.061); counter: 3 out of 3\n",
      "2023-12-30 17:51:15 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:51:15 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 90%|█████████ | 18/20 [02:46<00:18,  9.21s/it]2023-12-30 17:51:15 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 17:51:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.011 | 0.061\n",
      "2023-12-30 17:51:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.010 | 0.061\n",
      "2023-12-30 17:51:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.010 | 0.061\n",
      "2023-12-30 17:51:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.006 | 0.061\n",
      "2023-12-30 17:51:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.006 | 0.061\n",
      "2023-12-30 17:51:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.014 | 0.061\n",
      "2023-12-30 17:51:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.007 | 0.061\n",
      "2023-12-30 17:51:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.013 | 0.060\n",
      "2023-12-30 17:51:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.006 | 0.060\n",
      "2023-12-30 17:51:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.011 | 0.061\n",
      "2023-12-30 17:51:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.020 | 0.060\n",
      "2023-12-30 17:51:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.010 | 0.061\n",
      "2023-12-30 17:51:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.006 | 0.061\n",
      "2023-12-30 17:51:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.014 | 0.061\n",
      "2023-12-30 17:51:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.017 | 0.060\n",
      "2023-12-30 17:51:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.006 | 0.060\n",
      "2023-12-30 17:51:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.009 | 0.060\n",
      "2023-12-30 17:51:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.008 | 0.062\n",
      "2023-12-30 17:51:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.008 | 0.060\n",
      "2023-12-30 17:51:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.012 | 0.061\n",
      "2023-12-30 17:51:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.007 | 0.061\n",
      "2023-12-30 17:51:25 - INFO     | Early stopping: no decrease (0.057 vs 0.061); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [02:55<00:09,  9.21s/it]2023-12-30 17:51:25 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 17:51:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.029 | 0.061\n",
      "2023-12-30 17:51:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.009 | 0.060\n",
      "2023-12-30 17:51:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.014 | 0.060\n",
      "2023-12-30 17:51:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.008 | 0.060\n",
      "2023-12-30 17:51:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.008 | 0.060\n",
      "2023-12-30 17:51:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.006 | 0.060\n",
      "2023-12-30 17:51:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.010 | 0.061\n",
      "2023-12-30 17:51:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.010 | 0.060\n",
      "2023-12-30 17:51:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.007 | 0.060\n",
      "2023-12-30 17:51:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.010 | 0.061\n",
      "2023-12-30 17:51:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.011 | 0.063\n",
      "2023-12-30 17:51:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.005 | 0.061\n",
      "2023-12-30 17:51:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.013 | 0.061\n",
      "2023-12-30 17:51:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.020 | 0.061\n",
      "2023-12-30 17:51:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.007 | 0.061\n",
      "2023-12-30 17:51:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.011 | 0.061\n",
      "2023-12-30 17:51:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.007 | 0.061\n",
      "2023-12-30 17:51:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.009 | 0.061\n",
      "2023-12-30 17:51:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.012 | 0.060\n",
      "2023-12-30 17:51:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.008 | 0.060\n",
      "2023-12-30 17:51:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.010 | 0.060\n",
      "2023-12-30 17:51:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.009 | 0.060\n",
      "2023-12-30 17:51:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.015 | 0.061\n",
      "2023-12-30 17:51:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.007 | 0.061\n",
      "2023-12-30 17:51:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.006 | 0.061\n",
      "2023-12-30 17:51:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.008 | 0.061\n",
      "2023-12-30 17:51:34 - INFO     | Early stopping: no decrease (0.057 vs 0.061); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:04<00:00,  9.24s/it]\n",
      "2023-12-30 17:51:34 - INFO     | Best validation loss: 0.057\n",
      "2023-12-30 17:51:34 - INFO     | Best early stopping index/epoch: 11\n",
      "2023-12-30 17:51:34 - INFO     | Average Loss on test set: 0.074\n",
      "2023-12-30 17:51:36 - INFO     | Weighted Precision: 0.980, Recall: 0.980, F1: 0.980\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>█████████▄▄▄▄▄▄▂▂▂▁▁</td></tr><tr><td>step_learning_rate</td><td>██████████████████▄▄▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▄▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_validation_loss</td><td>0.05702</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.00798</td></tr><tr><td>step_validation_loss</td><td>0.06117</td></tr><tr><td>test_loss</td><td>0.07356</td></tr><tr><td>weighted_f1</td><td>0.98043</td></tr><tr><td>weighted_precision</td><td>0.98044</td></tr><tr><td>weighted_recall</td><td>0.98043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-10</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/33boavoo' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/33boavoo</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_174828-33boavoo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vegyjp2m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 32]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_175147-vegyjp2m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/vegyjp2m' target=\"_blank\">lyric-sweep-11</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/vegyjp2m' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/vegyjp2m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 32], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:51:48 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 17:51:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 20.469 | 14.757\n",
      "2023-12-30 17:51:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 4.297 | 0.669\n",
      "2023-12-30 17:51:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.445 | 0.302\n",
      "2023-12-30 17:51:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.250 | 0.222\n",
      "2023-12-30 17:51:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.197 | 0.173\n",
      "2023-12-30 17:51:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.162 | 0.162\n",
      "2023-12-30 17:51:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.161 | 0.155\n",
      "2023-12-30 17:51:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.151 | 0.122\n",
      "2023-12-30 17:51:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.137 | 0.128\n",
      "2023-12-30 17:51:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.117 | 0.132\n",
      "2023-12-30 17:51:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.121 | 0.117\n",
      "2023-12-30 17:51:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.114 | 0.167\n",
      "2023-12-30 17:51:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.122 | 0.106\n",
      "2023-12-30 17:51:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.112 | 0.097\n",
      "2023-12-30 17:51:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.110 | 0.098\n",
      "2023-12-30 17:51:53 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.102 | 0.107\n",
      "2023-12-30 17:51:53 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.103 | 0.099\n",
      "2023-12-30 17:51:53 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.092 | 0.097\n",
      "2023-12-30 17:51:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.089 | 0.078\n",
      "2023-12-30 17:51:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.092 | 0.086\n",
      "2023-12-30 17:51:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.071 | 0.097\n",
      "2023-12-30 17:51:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.070 | 0.090\n",
      "2023-12-30 17:51:55 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.106 | 0.080\n",
      "2023-12-30 17:51:55 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.113 | 0.082\n",
      "2023-12-30 17:51:55 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.085 | 0.084\n",
      "2023-12-30 17:51:56 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.071 | 0.074\n",
      "2023-12-30 17:51:56 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.078 | 0.079\n",
      "2023-12-30 17:51:56 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.101 | 0.069\n",
      "2023-12-30 17:51:57 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.086 | 0.061\n",
      "2023-12-30 17:51:57 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.058 | 0.072\n",
      "2023-12-30 17:51:57 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.084 | 0.084\n",
      "2023-12-30 17:51:57 - INFO     | Early stopping: loss decreased (inf -> 0.088; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:58,  9.39s/it]2023-12-30 17:51:57 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 17:51:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.080 | 0.095\n",
      "2023-12-30 17:51:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.046 | 0.064\n",
      "2023-12-30 17:51:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.061 | 0.068\n",
      "2023-12-30 17:51:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.056 | 0.071\n",
      "2023-12-30 17:51:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.068 | 0.075\n",
      "2023-12-30 17:51:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.041 | 0.070\n",
      "2023-12-30 17:52:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.050 | 0.087\n",
      "2023-12-30 17:52:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.054 | 0.079\n",
      "2023-12-30 17:52:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.112 | 0.090\n",
      "2023-12-30 17:52:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.065 | 0.111\n",
      "2023-12-30 17:52:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.077 | 0.083\n",
      "2023-12-30 17:52:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.069 | 0.068\n",
      "2023-12-30 17:52:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.091 | 0.096\n",
      "2023-12-30 17:52:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.072 | 0.063\n",
      "2023-12-30 17:52:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.073 | 0.073\n",
      "2023-12-30 17:52:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.044 | 0.080\n",
      "2023-12-30 17:52:03 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.068 | 0.083\n",
      "2023-12-30 17:52:03 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.073 | 0.071\n",
      "2023-12-30 17:52:03 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.049 | 0.071\n",
      "2023-12-30 17:52:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.070 | 0.068\n",
      "2023-12-30 17:52:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.042 | 0.064\n",
      "2023-12-30 17:52:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.052 | 0.055\n",
      "2023-12-30 17:52:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.081 | 0.080\n",
      "2023-12-30 17:52:05 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.064 | 0.067\n",
      "2023-12-30 17:52:05 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.093 | 0.068\n",
      "2023-12-30 17:52:05 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.046 | 0.069\n",
      "2023-12-30 17:52:06 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.049 | 0.080\n",
      "2023-12-30 17:52:06 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.085 | 0.061\n",
      "2023-12-30 17:52:06 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.066 | 0.067\n",
      "2023-12-30 17:52:06 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.040 | 0.059\n",
      "2023-12-30 17:52:07 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.073 | 0.065\n",
      "2023-12-30 17:52:07 - INFO     | Early stopping: loss decreased (0.088 -> 0.071; -20.0%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:50,  9.46s/it]2023-12-30 17:52:07 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 17:52:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.024 | 0.072\n",
      "2023-12-30 17:52:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.026 | 0.086\n",
      "2023-12-30 17:52:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.061 | 0.077\n",
      "2023-12-30 17:52:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.031 | 0.054\n",
      "2023-12-30 17:52:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.038 | 0.070\n",
      "2023-12-30 17:52:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.052 | 0.068\n",
      "2023-12-30 17:52:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.036 | 0.066\n",
      "2023-12-30 17:52:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.049 | 0.058\n",
      "2023-12-30 17:52:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.058 | 0.064\n",
      "2023-12-30 17:52:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.046 | 0.065\n",
      "2023-12-30 17:52:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.050 | 0.069\n",
      "2023-12-30 17:52:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.046 | 0.068\n",
      "2023-12-30 17:52:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.035 | 0.077\n",
      "2023-12-30 17:52:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.053 | 0.073\n",
      "2023-12-30 17:52:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.057 | 0.111\n",
      "2023-12-30 17:52:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.060 | 0.065\n",
      "2023-12-30 17:52:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.028 | 0.062\n",
      "2023-12-30 17:52:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.051 | 0.072\n",
      "2023-12-30 17:52:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.065 | 0.070\n",
      "2023-12-30 17:52:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.069 | 0.089\n",
      "2023-12-30 17:52:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.065 | 0.074\n",
      "2023-12-30 17:52:14 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.041 | 0.062\n",
      "2023-12-30 17:52:14 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.049 | 0.073\n",
      "2023-12-30 17:52:14 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.065 | 0.054\n",
      "2023-12-30 17:52:15 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.063 | 0.062\n",
      "2023-12-30 17:52:15 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.036 | 0.068\n",
      "2023-12-30 17:52:15 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.072 | 0.085\n",
      "2023-12-30 17:52:15 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.068 | 0.074\n",
      "2023-12-30 17:52:16 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.064 | 0.077\n",
      "2023-12-30 17:52:16 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.056 | 0.074\n",
      "2023-12-30 17:52:16 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.067 | 0.054\n",
      "2023-12-30 17:52:17 - INFO     | Early stopping: loss decreased (0.071 -> 0.055; -22.7%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:28<02:43,  9.59s/it]2023-12-30 17:52:17 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 17:52:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.053 | 0.055\n",
      "2023-12-30 17:52:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.043 | 0.050\n",
      "2023-12-30 17:52:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.025 | 0.056\n",
      "2023-12-30 17:52:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.035 | 0.068\n",
      "2023-12-30 17:52:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.028 | 0.065\n",
      "2023-12-30 17:52:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.028 | 0.065\n",
      "2023-12-30 17:52:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.048 | 0.060\n",
      "2023-12-30 17:52:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.036 | 0.060\n",
      "2023-12-30 17:52:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.035 | 0.055\n",
      "2023-12-30 17:52:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.035 | 0.065\n",
      "2023-12-30 17:52:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.042 | 0.057\n",
      "2023-12-30 17:52:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.030 | 0.057\n",
      "2023-12-30 17:52:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.036 | 0.065\n",
      "2023-12-30 17:52:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.045 | 0.068\n",
      "2023-12-30 17:52:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.036 | 0.051\n",
      "2023-12-30 17:52:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.026 | 0.065\n",
      "2023-12-30 17:52:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.036 | 0.056\n",
      "2023-12-30 17:52:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.047 | 0.082\n",
      "2023-12-30 17:52:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.042 | 0.056\n",
      "2023-12-30 17:52:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.046 | 0.069\n",
      "2023-12-30 17:52:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.045 | 0.065\n",
      "2023-12-30 17:52:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.041 | 0.060\n",
      "2023-12-30 17:52:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.036 | 0.059\n",
      "2023-12-30 17:52:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.041 | 0.077\n",
      "2023-12-30 17:52:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.052 | 0.071\n",
      "2023-12-30 17:52:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.052 | 0.075\n",
      "2023-12-30 17:52:25 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.066 | 0.082\n",
      "2023-12-30 17:52:25 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.034 | 0.066\n",
      "2023-12-30 17:52:25 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.040 | 0.083\n",
      "2023-12-30 17:52:26 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.064 | 0.086\n",
      "2023-12-30 17:52:26 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.061 | 0.078\n",
      "2023-12-30 17:52:26 - INFO     | Early stopping: no decrease (0.055 vs 0.077); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:38<02:32,  9.54s/it]2023-12-30 17:52:26 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 17:52:26 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.007 | 0.079\n",
      "2023-12-30 17:52:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.046 | 0.076\n",
      "2023-12-30 17:52:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.030 | 0.073\n",
      "2023-12-30 17:52:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.028 | 0.078\n",
      "2023-12-30 17:52:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.040 | 0.074\n",
      "2023-12-30 17:52:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.029 | 0.063\n",
      "2023-12-30 17:52:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.023 | 0.059\n",
      "2023-12-30 17:52:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.030 | 0.064\n",
      "2023-12-30 17:52:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.035 | 0.070\n",
      "2023-12-30 17:52:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.045 | 0.096\n",
      "2023-12-30 17:52:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.045 | 0.064\n",
      "2023-12-30 17:52:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.061 | 0.086\n",
      "2023-12-30 17:52:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.047 | 0.068\n",
      "2023-12-30 17:52:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.031 | 0.069\n",
      "2023-12-30 17:52:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.028 | 0.072\n",
      "2023-12-30 17:52:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.053 | 0.065\n",
      "2023-12-30 17:52:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.032 | 0.067\n",
      "2023-12-30 17:52:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.043 | 0.071\n",
      "2023-12-30 17:52:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.035 | 0.060\n",
      "2023-12-30 17:52:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.045 | 0.065\n",
      "2023-12-30 17:52:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.022 | 0.071\n",
      "2023-12-30 17:52:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.036 | 0.076\n",
      "2023-12-30 17:52:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.030 | 0.071\n",
      "2023-12-30 17:52:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.037 | 0.078\n",
      "2023-12-30 17:52:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.067 | 0.075\n",
      "2023-12-30 17:52:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.043 | 0.067\n",
      "2023-12-30 17:52:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.064 | 0.072\n",
      "2023-12-30 17:52:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.044 | 0.073\n",
      "2023-12-30 17:52:35 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.042 | 0.068\n",
      "2023-12-30 17:52:35 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.079 | 0.080\n",
      "2023-12-30 17:52:35 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.051 | 0.054\n",
      "2023-12-30 17:52:36 - INFO     | Early stopping: no decrease (0.055 vs 0.055); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:47<02:22,  9.50s/it]2023-12-30 17:52:36 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 17:52:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.029 | 0.055\n",
      "2023-12-30 17:52:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.033 | 0.058\n",
      "2023-12-30 17:52:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.027 | 0.049\n",
      "2023-12-30 17:52:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.024 | 0.056\n",
      "2023-12-30 17:52:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.027 | 0.061\n",
      "2023-12-30 17:52:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.023 | 0.057\n",
      "2023-12-30 17:52:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.022 | 0.066\n",
      "2023-12-30 17:52:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.029 | 0.068\n",
      "2023-12-30 17:52:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.014 | 0.055\n",
      "2023-12-30 17:52:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.040 | 0.057\n",
      "2023-12-30 17:52:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.026 | 0.064\n",
      "2023-12-30 17:52:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.021 | 0.061\n",
      "2023-12-30 17:52:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.033 | 0.066\n",
      "2023-12-30 17:52:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.043 | 0.063\n",
      "2023-12-30 17:52:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.035 | 0.091\n",
      "2023-12-30 17:52:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.057 | 0.085\n",
      "2023-12-30 17:52:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.038 | 0.081\n",
      "2023-12-30 17:52:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.062 | 0.083\n",
      "2023-12-30 17:52:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.064 | 0.076\n",
      "2023-12-30 17:52:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.037 | 0.076\n",
      "2023-12-30 17:52:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.041 | 0.072\n",
      "2023-12-30 17:52:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.037 | 0.071\n",
      "2023-12-30 17:52:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.037 | 0.069\n",
      "2023-12-30 17:52:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.055 | 0.064\n",
      "2023-12-30 17:52:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.020 | 0.056\n",
      "2023-12-30 17:52:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.032 | 0.079\n",
      "2023-12-30 17:52:44 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.049 | 0.064\n",
      "2023-12-30 17:52:44 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.034 | 0.065\n",
      "2023-12-30 17:52:44 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.054 | 0.069\n",
      "2023-12-30 17:52:45 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.042 | 0.058\n",
      "2023-12-30 17:52:45 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.037 | 0.084\n",
      "2023-12-30 17:52:45 - INFO     | Early stopping: no decrease (0.055 vs 0.072); counter: 3 out of 3\n",
      "2023-12-30 17:52:45 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:52:45 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 30%|███       | 6/20 [00:56<02:12,  9.48s/it]2023-12-30 17:52:45 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 17:52:45 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.022 | 0.068\n",
      "2023-12-30 17:52:46 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.038 | 0.059\n",
      "2023-12-30 17:52:46 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.028 | 0.060\n",
      "2023-12-30 17:52:46 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.021 | 0.056\n",
      "2023-12-30 17:52:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.015 | 0.053\n",
      "2023-12-30 17:52:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.020 | 0.061\n",
      "2023-12-30 17:52:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.021 | 0.052\n",
      "2023-12-30 17:52:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.019 | 0.058\n",
      "2023-12-30 17:52:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.012 | 0.053\n",
      "2023-12-30 17:52:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.019 | 0.053\n",
      "2023-12-30 17:52:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.019 | 0.053\n",
      "2023-12-30 17:52:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.022 | 0.059\n",
      "2023-12-30 17:52:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.016 | 0.059\n",
      "2023-12-30 17:52:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.019 | 0.059\n",
      "2023-12-30 17:52:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.014 | 0.057\n",
      "2023-12-30 17:52:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.022 | 0.056\n",
      "2023-12-30 17:52:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.026 | 0.057\n",
      "2023-12-30 17:52:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.012 | 0.052\n",
      "2023-12-30 17:52:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.021 | 0.060\n",
      "2023-12-30 17:52:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.013 | 0.057\n",
      "2023-12-30 17:52:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.018 | 0.058\n",
      "2023-12-30 17:52:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.029 | 0.060\n",
      "2023-12-30 17:52:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.023 | 0.061\n",
      "2023-12-30 17:52:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.014 | 0.061\n",
      "2023-12-30 17:52:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.023 | 0.058\n",
      "2023-12-30 17:52:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.026 | 0.057\n",
      "2023-12-30 17:52:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.018 | 0.062\n",
      "2023-12-30 17:52:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.018 | 0.057\n",
      "2023-12-30 17:52:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.016 | 0.053\n",
      "2023-12-30 17:52:54 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.016 | 0.061\n",
      "2023-12-30 17:52:54 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.022 | 0.055\n",
      "2023-12-30 17:52:54 - INFO     | Early stopping: no decrease (0.055 vs 0.053); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:06<02:02,  9.43s/it]2023-12-30 17:52:54 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 17:52:55 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.004 | 0.053\n",
      "2023-12-30 17:52:55 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.006 | 0.053\n",
      "2023-12-30 17:52:55 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.009 | 0.055\n",
      "2023-12-30 17:52:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.006 | 0.057\n",
      "2023-12-30 17:52:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.009 | 0.060\n",
      "2023-12-30 17:52:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.005 | 0.057\n",
      "2023-12-30 17:52:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.004 | 0.060\n",
      "2023-12-30 17:52:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.008 | 0.058\n",
      "2023-12-30 17:52:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.009 | 0.063\n",
      "2023-12-30 17:52:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.011 | 0.060\n",
      "2023-12-30 17:52:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.006 | 0.058\n",
      "2023-12-30 17:52:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.009 | 0.061\n",
      "2023-12-30 17:52:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.009 | 0.050\n",
      "2023-12-30 17:52:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.005 | 0.057\n",
      "2023-12-30 17:52:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.008 | 0.062\n",
      "2023-12-30 17:52:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.012 | 0.066\n",
      "2023-12-30 17:53:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.005 | 0.064\n",
      "2023-12-30 17:53:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.015 | 0.073\n",
      "2023-12-30 17:53:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.012 | 0.061\n",
      "2023-12-30 17:53:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.023 | 0.063\n",
      "2023-12-30 17:53:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.016 | 0.061\n",
      "2023-12-30 17:53:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.025 | 0.065\n",
      "2023-12-30 17:53:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.017 | 0.060\n",
      "2023-12-30 17:53:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.011 | 0.066\n",
      "2023-12-30 17:53:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.018 | 0.060\n",
      "2023-12-30 17:53:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.016 | 0.062\n",
      "2023-12-30 17:53:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.017 | 0.067\n",
      "2023-12-30 17:53:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.022 | 0.056\n",
      "2023-12-30 17:53:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.009 | 0.061\n",
      "2023-12-30 17:53:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.014 | 0.061\n",
      "2023-12-30 17:53:04 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.011 | 0.062\n",
      "2023-12-30 17:53:04 - INFO     | Early stopping: no decrease (0.055 vs 0.062); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:15<01:53,  9.44s/it]2023-12-30 17:53:04 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 17:53:04 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.004 | 0.062\n",
      "2023-12-30 17:53:04 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.012 | 0.057\n",
      "2023-12-30 17:53:05 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.004 | 0.061\n",
      "2023-12-30 17:53:05 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.007 | 0.060\n",
      "2023-12-30 17:53:05 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.007 | 0.056\n",
      "2023-12-30 17:53:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.009 | 0.057\n",
      "2023-12-30 17:53:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.009 | 0.054\n",
      "2023-12-30 17:53:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.006 | 0.052\n",
      "2023-12-30 17:53:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.007 | 0.059\n",
      "2023-12-30 17:53:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.005 | 0.058\n",
      "2023-12-30 17:53:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.005 | 0.061\n",
      "2023-12-30 17:53:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.005 | 0.067\n",
      "2023-12-30 17:53:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.006 | 0.057\n",
      "2023-12-30 17:53:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.007 | 0.063\n",
      "2023-12-30 17:53:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.007 | 0.064\n",
      "2023-12-30 17:53:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.009 | 0.063\n",
      "2023-12-30 17:53:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.012 | 0.065\n",
      "2023-12-30 17:53:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.007 | 0.056\n",
      "2023-12-30 17:53:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.020 | 0.056\n",
      "2023-12-30 17:53:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.020 | 0.067\n",
      "2023-12-30 17:53:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.019 | 0.067\n",
      "2023-12-30 17:53:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.024 | 0.079\n",
      "2023-12-30 17:53:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.023 | 0.067\n",
      "2023-12-30 17:53:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.015 | 0.078\n",
      "2023-12-30 17:53:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.010 | 0.072\n",
      "2023-12-30 17:53:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.016 | 0.073\n",
      "2023-12-30 17:53:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.017 | 0.070\n",
      "2023-12-30 17:53:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.012 | 0.075\n",
      "2023-12-30 17:53:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.015 | 0.069\n",
      "2023-12-30 17:53:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.018 | 0.072\n",
      "2023-12-30 17:53:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.010 | 0.062\n",
      "2023-12-30 17:53:13 - INFO     | Early stopping: no decrease (0.055 vs 0.061); counter: 3 out of 3\n",
      "2023-12-30 17:53:13 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:53:13 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 45%|████▌     | 9/20 [01:25<01:43,  9.45s/it]2023-12-30 17:53:13 - INFO     | Epoch: 9 | Learning Rate: 0.000\n",
      "2023-12-30 17:53:14 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 504064 examples: 0.003 | 0.060\n",
      "2023-12-30 17:53:14 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 505920 examples: 0.004 | 0.063\n",
      "2023-12-30 17:53:14 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 507776 examples: 0.005 | 0.060\n",
      "2023-12-30 17:53:14 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 509632 examples: 0.007 | 0.060\n",
      "2023-12-30 17:53:15 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 511488 examples: 0.004 | 0.062\n",
      "2023-12-30 17:53:15 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 513344 examples: 0.004 | 0.061\n",
      "2023-12-30 17:53:15 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 515200 examples: 0.003 | 0.058\n",
      "2023-12-30 17:53:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 517056 examples: 0.003 | 0.062\n",
      "2023-12-30 17:53:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 518912 examples: 0.005 | 0.059\n",
      "2023-12-30 17:53:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 520768 examples: 0.001 | 0.060\n",
      "2023-12-30 17:53:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 522624 examples: 0.002 | 0.061\n",
      "2023-12-30 17:53:17 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 524480 examples: 0.002 | 0.060\n",
      "2023-12-30 17:53:17 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 526336 examples: 0.002 | 0.061\n",
      "2023-12-30 17:53:17 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 528192 examples: 0.003 | 0.063\n",
      "2023-12-30 17:53:18 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 530048 examples: 0.003 | 0.061\n",
      "2023-12-30 17:53:18 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 531904 examples: 0.003 | 0.067\n",
      "2023-12-30 17:53:18 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 533760 examples: 0.006 | 0.066\n",
      "2023-12-30 17:53:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 535616 examples: 0.002 | 0.064\n",
      "2023-12-30 17:53:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 537472 examples: 0.006 | 0.063\n",
      "2023-12-30 17:53:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 539328 examples: 0.006 | 0.068\n",
      "2023-12-30 17:53:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 541184 examples: 0.010 | 0.073\n",
      "2023-12-30 17:53:20 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 543040 examples: 0.005 | 0.074\n",
      "2023-12-30 17:53:20 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 544896 examples: 0.006 | 0.070\n",
      "2023-12-30 17:53:20 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 546752 examples: 0.002 | 0.067\n",
      "2023-12-30 17:53:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 548608 examples: 0.002 | 0.065\n",
      "2023-12-30 17:53:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 550464 examples: 0.005 | 0.066\n",
      "2023-12-30 17:53:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 552320 examples: 0.005 | 0.071\n",
      "2023-12-30 17:53:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 554176 examples: 0.008 | 0.064\n",
      "2023-12-30 17:53:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 556032 examples: 0.006 | 0.065\n",
      "2023-12-30 17:53:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 557888 examples: 0.004 | 0.063\n",
      "2023-12-30 17:53:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 559744 examples: 0.003 | 0.068\n",
      "2023-12-30 17:53:23 - INFO     | Early stopping: no decrease (0.055 vs 0.069); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:34<01:34,  9.41s/it]2023-12-30 17:53:23 - INFO     | Epoch: 10 | Learning Rate: 0.000\n",
      "2023-12-30 17:53:23 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 560064 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:23 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 561920 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:24 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 563776 examples: 0.001 | 0.067\n",
      "2023-12-30 17:53:24 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 565632 examples: 0.002 | 0.064\n",
      "2023-12-30 17:53:24 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 567488 examples: 0.000 | 0.064\n",
      "2023-12-30 17:53:24 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 569344 examples: 0.003 | 0.069\n",
      "2023-12-30 17:53:25 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 571200 examples: 0.001 | 0.066\n",
      "2023-12-30 17:53:25 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 573056 examples: 0.001 | 0.063\n",
      "2023-12-30 17:53:25 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 574912 examples: 0.001 | 0.062\n",
      "2023-12-30 17:53:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 576768 examples: 0.000 | 0.062\n",
      "2023-12-30 17:53:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 578624 examples: 0.000 | 0.061\n",
      "2023-12-30 17:53:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 580480 examples: 0.000 | 0.060\n",
      "2023-12-30 17:53:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 582336 examples: 0.001 | 0.060\n",
      "2023-12-30 17:53:27 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 584192 examples: 0.000 | 0.059\n",
      "2023-12-30 17:53:27 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 586048 examples: 0.001 | 0.058\n",
      "2023-12-30 17:53:27 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 587904 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 589760 examples: 0.000 | 0.059\n",
      "2023-12-30 17:53:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 591616 examples: 0.001 | 0.060\n",
      "2023-12-30 17:53:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 593472 examples: 0.000 | 0.061\n",
      "2023-12-30 17:53:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 595328 examples: 0.001 | 0.058\n",
      "2023-12-30 17:53:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 597184 examples: 0.001 | 0.056\n",
      "2023-12-30 17:53:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 599040 examples: 0.003 | 0.055\n",
      "2023-12-30 17:53:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 600896 examples: 0.000 | 0.055\n",
      "2023-12-30 17:53:30 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 602752 examples: 0.000 | 0.055\n",
      "2023-12-30 17:53:30 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 604608 examples: 0.000 | 0.055\n",
      "2023-12-30 17:53:30 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 606464 examples: 0.000 | 0.055\n",
      "2023-12-30 17:53:31 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 608320 examples: 0.001 | 0.058\n",
      "2023-12-30 17:53:31 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 610176 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:31 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 612032 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:32 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 613888 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:32 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 615744 examples: 0.006 | 0.062\n",
      "2023-12-30 17:53:32 - INFO     | Early stopping: no decrease (0.055 vs 0.062); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:44<01:24,  9.43s/it]2023-12-30 17:53:32 - INFO     | Epoch: 11 | Learning Rate: 0.000\n",
      "2023-12-30 17:53:32 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 616064 examples: 0.000 | 0.063\n",
      "2023-12-30 17:53:33 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 617920 examples: 0.002 | 0.058\n",
      "2023-12-30 17:53:33 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 619776 examples: 0.001 | 0.060\n",
      "2023-12-30 17:53:33 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 621632 examples: 0.001 | 0.061\n",
      "2023-12-30 17:53:34 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 623488 examples: 0.001 | 0.058\n",
      "2023-12-30 17:53:34 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 625344 examples: 0.001 | 0.059\n",
      "2023-12-30 17:53:34 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 627200 examples: 0.001 | 0.061\n",
      "2023-12-30 17:53:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 629056 examples: 0.000 | 0.060\n",
      "2023-12-30 17:53:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 630912 examples: 0.000 | 0.060\n",
      "2023-12-30 17:53:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 632768 examples: 0.000 | 0.060\n",
      "2023-12-30 17:53:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 634624 examples: 0.001 | 0.059\n",
      "2023-12-30 17:53:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 636480 examples: 0.000 | 0.059\n",
      "2023-12-30 17:53:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 638336 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 640192 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 642048 examples: 0.000 | 0.057\n",
      "2023-12-30 17:53:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 643904 examples: 0.000 | 0.058\n",
      "2023-12-30 17:53:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 645760 examples: 0.000 | 0.059\n",
      "2023-12-30 17:53:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 647616 examples: 0.001 | 0.061\n",
      "2023-12-30 17:53:38 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 649472 examples: 0.004 | 0.058\n",
      "2023-12-30 17:53:38 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 651328 examples: 0.002 | 0.060\n",
      "2023-12-30 17:53:38 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 653184 examples: 0.002 | 0.060\n",
      "2023-12-30 17:53:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 655040 examples: 0.000 | 0.060\n",
      "2023-12-30 17:53:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 656896 examples: 0.001 | 0.062\n",
      "2023-12-30 17:53:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 658752 examples: 0.001 | 0.062\n",
      "2023-12-30 17:53:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 660608 examples: 0.002 | 0.072\n",
      "2023-12-30 17:53:40 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 662464 examples: 0.006 | 0.064\n",
      "2023-12-30 17:53:40 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 664320 examples: 0.000 | 0.067\n",
      "2023-12-30 17:53:40 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 666176 examples: 0.002 | 0.071\n",
      "2023-12-30 17:53:41 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 668032 examples: 0.004 | 0.070\n",
      "2023-12-30 17:53:41 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 669888 examples: 0.005 | 0.078\n",
      "2023-12-30 17:53:41 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 671744 examples: 0.004 | 0.075\n",
      "2023-12-30 17:53:41 - INFO     | Early stopping: no decrease (0.055 vs 0.076); counter: 3 out of 3\n",
      "2023-12-30 17:53:41 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:53:41 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      " 60%|██████    | 12/20 [01:53<01:15,  9.41s/it]2023-12-30 17:53:41 - INFO     | Epoch: 12 | Learning Rate: 0.000\n",
      "2023-12-30 17:53:42 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 672064 examples: 0.000 | 0.076\n",
      "2023-12-30 17:53:42 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 673920 examples: 0.001 | 0.069\n",
      "2023-12-30 17:53:42 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 675776 examples: 0.001 | 0.066\n",
      "2023-12-30 17:53:43 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 677632 examples: 0.000 | 0.067\n",
      "2023-12-30 17:53:43 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 679488 examples: 0.000 | 0.067\n",
      "2023-12-30 17:53:43 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 681344 examples: 0.001 | 0.066\n",
      "2023-12-30 17:53:44 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 683200 examples: 0.000 | 0.066\n",
      "2023-12-30 17:53:44 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 685056 examples: 0.000 | 0.066\n",
      "2023-12-30 17:53:44 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 686912 examples: 0.000 | 0.065\n",
      "2023-12-30 17:53:44 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 688768 examples: 0.000 | 0.065\n",
      "2023-12-30 17:53:45 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 690624 examples: 0.000 | 0.065\n",
      "2023-12-30 17:53:45 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 692480 examples: 0.000 | 0.064\n",
      "2023-12-30 17:53:45 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 694336 examples: 0.000 | 0.063\n",
      "2023-12-30 17:53:46 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 696192 examples: 0.001 | 0.064\n",
      "2023-12-30 17:53:46 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 698048 examples: 0.001 | 0.064\n",
      "2023-12-30 17:53:46 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 699904 examples: 0.001 | 0.064\n",
      "2023-12-30 17:53:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 701760 examples: 0.000 | 0.066\n",
      "2023-12-30 17:53:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 703616 examples: 0.000 | 0.067\n",
      "2023-12-30 17:53:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 705472 examples: 0.001 | 0.069\n",
      "2023-12-30 17:53:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 707328 examples: 0.000 | 0.067\n",
      "2023-12-30 17:53:48 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 709184 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:48 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 711040 examples: 0.002 | 0.068\n",
      "2023-12-30 17:53:48 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 712896 examples: 0.001 | 0.070\n",
      "2023-12-30 17:53:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 714752 examples: 0.000 | 0.071\n",
      "2023-12-30 17:53:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 716608 examples: 0.000 | 0.071\n",
      "2023-12-30 17:53:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 718464 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 720320 examples: 0.000 | 0.070\n",
      "2023-12-30 17:53:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 722176 examples: 0.000 | 0.070\n",
      "2023-12-30 17:53:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 724032 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 725888 examples: 0.001 | 0.074\n",
      "2023-12-30 17:53:51 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 727744 examples: 0.000 | 0.076\n",
      "2023-12-30 17:53:51 - INFO     | Early stopping: no decrease (0.055 vs 0.076); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:02<01:05,  9.42s/it]2023-12-30 17:53:51 - INFO     | Epoch: 13 | Learning Rate: 0.000\n",
      "2023-12-30 17:53:51 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 728064 examples: 0.000 | 0.076\n",
      "2023-12-30 17:53:52 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 729920 examples: 0.001 | 0.072\n",
      "2023-12-30 17:53:52 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 731776 examples: 0.000 | 0.071\n",
      "2023-12-30 17:53:52 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 733632 examples: 0.000 | 0.071\n",
      "2023-12-30 17:53:52 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 735488 examples: 0.000 | 0.070\n",
      "2023-12-30 17:53:53 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 737344 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:53 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 739200 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:53 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 741056 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:54 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 742912 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:54 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 744768 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:54 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 746624 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:54 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 748480 examples: 0.000 | 0.070\n",
      "2023-12-30 17:53:55 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 750336 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:55 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 752192 examples: 0.000 | 0.072\n",
      "2023-12-30 17:53:56 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 754048 examples: 0.000 | 0.074\n",
      "2023-12-30 17:53:56 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 755904 examples: 0.000 | 0.074\n",
      "2023-12-30 17:53:56 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 757760 examples: 0.000 | 0.074\n",
      "2023-12-30 17:53:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 759616 examples: 0.000 | 0.074\n",
      "2023-12-30 17:53:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 761472 examples: 0.000 | 0.073\n",
      "2023-12-30 17:53:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 763328 examples: 0.000 | 0.071\n",
      "2023-12-30 17:53:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 765184 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:58 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 767040 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:58 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 768896 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:58 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 770752 examples: 0.000 | 0.068\n",
      "2023-12-30 17:53:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 772608 examples: 0.000 | 0.070\n",
      "2023-12-30 17:53:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 774464 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 776320 examples: 0.000 | 0.069\n",
      "2023-12-30 17:53:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 778176 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:00 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 780032 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:00 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 781888 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:00 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 783744 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:01 - INFO     | Early stopping: no decrease (0.055 vs 0.071); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:12<00:57,  9.51s/it]2023-12-30 17:54:01 - INFO     | Epoch: 14 | Learning Rate: 0.000\n",
      "2023-12-30 17:54:01 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 784064 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:01 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 785920 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:01 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 787776 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:02 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 789632 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:02 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 791488 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:02 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 793344 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:03 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 795200 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:03 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 797056 examples: 0.000 | 0.066\n",
      "2023-12-30 17:54:03 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 798912 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:04 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 800768 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:04 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 802624 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:04 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 804480 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:04 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 806336 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:05 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 808192 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:05 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 810048 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:05 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 811904 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 813760 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 815616 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 817472 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 819328 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:07 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 821184 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:07 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 823040 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:07 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 824896 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:08 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 826752 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:08 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 828608 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:08 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 830464 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 832320 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 834176 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 836032 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 837888 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 839744 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:10 - INFO     | Early stopping: no decrease (0.055 vs 0.068); counter: 3 out of 3\n",
      "2023-12-30 17:54:10 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:54:10 - INFO     | Reducing learning rate: 0.000125 -> 6.25e-05\n",
      " 75%|███████▌  | 15/20 [02:21<00:47,  9.46s/it]2023-12-30 17:54:10 - INFO     | Epoch: 15 | Learning Rate: 0.000\n",
      "2023-12-30 17:54:10 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 840064 examples: 0.000 | 0.068\n",
      "2023-12-30 17:54:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 841920 examples: 0.000 | 0.067\n",
      "2023-12-30 17:54:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 843776 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 845632 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 847488 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:12 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 849344 examples: 0.000 | 0.069\n",
      "2023-12-30 17:54:12 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 851200 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:12 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 853056 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:13 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 854912 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:13 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 856768 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:13 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 858624 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:14 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 860480 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:14 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 862336 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:14 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 864192 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:14 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 866048 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:15 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 867904 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:15 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 869760 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:15 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 871616 examples: 0.000 | 0.070\n",
      "2023-12-30 17:54:16 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 873472 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:16 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 875328 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:16 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 877184 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 879040 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 880896 examples: 0.000 | 0.071\n",
      "2023-12-30 17:54:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 882752 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 884608 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 886464 examples: 0.000 | 0.075\n",
      "2023-12-30 17:54:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 888320 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 890176 examples: 0.000 | 0.076\n",
      "2023-12-30 17:54:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 892032 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 893888 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 895744 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:19 - INFO     | Early stopping: no decrease (0.055 vs 0.072); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:31<00:37,  9.47s/it]2023-12-30 17:54:19 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 17:54:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:21 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:21 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:21 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:23 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:23 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:23 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:24 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:24 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:24 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:25 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:25 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:25 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:25 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:26 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:26 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:26 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:27 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:27 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:27 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:29 - INFO     | Early stopping: no decrease (0.055 vs 0.073); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:40<00:28,  9.51s/it]2023-12-30 17:54:29 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 17:54:29 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:30 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:30 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.000 | 0.072\n",
      "2023-12-30 17:54:30 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:33 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:33 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:33 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:34 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:34 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:34 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:35 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:35 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:35 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:35 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:36 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:36 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:36 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:39 - INFO     | Early stopping: no decrease (0.055 vs 0.074); counter: 3 out of 3\n",
      "2023-12-30 17:54:39 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:54:39 - INFO     | Reducing learning rate: 6.25e-05 -> 3.125e-05\n",
      " 90%|█████████ | 18/20 [02:50<00:19,  9.53s/it]2023-12-30 17:54:39 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 17:54:39 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:39 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.000 | 0.075\n",
      "2023-12-30 17:54:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:41 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:41 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.000 | 0.073\n",
      "2023-12-30 17:54:41 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:42 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:42 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.075\n",
      "2023-12-30 17:54:42 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:43 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.000 | 0.074\n",
      "2023-12-30 17:54:43 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.075\n",
      "2023-12-30 17:54:43 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.000 | 0.075\n",
      "2023-12-30 17:54:44 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.000 | 0.076\n",
      "2023-12-30 17:54:44 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.000 | 0.075\n",
      "2023-12-30 17:54:44 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.076\n",
      "2023-12-30 17:54:44 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:45 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:45 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.000 | 0.076\n",
      "2023-12-30 17:54:45 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.076\n",
      "2023-12-30 17:54:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:48 - INFO     | Early stopping: no decrease (0.055 vs 0.078); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [03:00<00:09,  9.54s/it]2023-12-30 17:54:48 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 17:54:48 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:49 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:49 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:49 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:50 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:50 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:50 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:52 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:52 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:52 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:54 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:54 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:54 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:55 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:55 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.000 | 0.077\n",
      "2023-12-30 17:54:55 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:55 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.078\n",
      "2023-12-30 17:54:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.079\n",
      "2023-12-30 17:54:57 - INFO     | Early stopping: no decrease (0.055 vs 0.079); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:09<00:00,  9.47s/it]\n",
      "2023-12-30 17:54:57 - INFO     | Best validation loss: 0.055\n",
      "2023-12-30 17:54:57 - INFO     | Best early stopping index/epoch: 2\n",
      "2023-12-30 17:54:58 - INFO     | Average Loss on test set: 0.073\n",
      "2023-12-30 17:55:01 - INFO     | Weighted Precision: 0.990, Recall: 0.990, F1: 0.990\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▅▄▂▃▂▃▂▃▂▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▇▅▄▄▃▁▁▄▂▂▃▂▂▂▂▁▄▂▂▃▂▂▂▃▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_validation_loss</td><td>0.0546</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>step_learning_rate</td><td>3e-05</td></tr><tr><td>step_training_loss</td><td>0.0</td></tr><tr><td>step_validation_loss</td><td>0.07879</td></tr><tr><td>test_loss</td><td>0.07342</td></tr><tr><td>weighted_f1</td><td>0.99043</td></tr><tr><td>weighted_precision</td><td>0.99047</td></tr><tr><td>weighted_recall</td><td>0.99043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-sweep-11</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/vegyjp2m' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/vegyjp2m</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_175147-vegyjp2m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eppo596o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 32]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_175512-eppo596o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/eppo596o' target=\"_blank\">true-sweep-12</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/eppo596o' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/eppo596o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 32], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:55:13 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 17:55:13 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 18.458 | 147.088\n",
      "2023-12-30 17:55:13 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 6.331 | 0.823\n",
      "2023-12-30 17:55:13 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.658 | 0.532\n",
      "2023-12-30 17:55:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.490 | 0.409\n",
      "2023-12-30 17:55:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.337 | 0.346\n",
      "2023-12-30 17:55:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.353 | 0.294\n",
      "2023-12-30 17:55:15 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.334 | 0.259\n",
      "2023-12-30 17:55:15 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.246 | 0.238\n",
      "2023-12-30 17:55:15 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.254 | 0.224\n",
      "2023-12-30 17:55:16 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.203 | 0.210\n",
      "2023-12-30 17:55:16 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.198 | 0.192\n",
      "2023-12-30 17:55:16 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.207 | 0.183\n",
      "2023-12-30 17:55:16 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.184 | 0.183\n",
      "2023-12-30 17:55:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.175 | 0.179\n",
      "2023-12-30 17:55:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.166 | 0.162\n",
      "2023-12-30 17:55:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.167 | 0.162\n",
      "2023-12-30 17:55:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.182 | 0.152\n",
      "2023-12-30 17:55:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.188 | 0.150\n",
      "2023-12-30 17:55:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.151 | 0.140\n",
      "2023-12-30 17:55:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.122 | 0.141\n",
      "2023-12-30 17:55:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.135 | 0.135\n",
      "2023-12-30 17:55:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.142 | 0.132\n",
      "2023-12-30 17:55:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.139 | 0.128\n",
      "2023-12-30 17:55:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.145 | 0.133\n",
      "2023-12-30 17:55:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.126 | 0.132\n",
      "2023-12-30 17:55:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.117 | 0.120\n",
      "2023-12-30 17:55:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.115 | 0.119\n",
      "2023-12-30 17:55:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.132 | 0.122\n",
      "2023-12-30 17:55:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.128 | 0.115\n",
      "2023-12-30 17:55:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.113 | 0.112\n",
      "2023-12-30 17:55:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.120 | 0.111\n",
      "2023-12-30 17:55:22 - INFO     | Early stopping: loss decreased (inf -> 0.109; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<02:57,  9.34s/it]2023-12-30 17:55:22 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 17:55:22 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.214 | 0.111\n",
      "2023-12-30 17:55:22 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.108 | 0.108\n",
      "2023-12-30 17:55:23 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.104 | 0.112\n",
      "2023-12-30 17:55:23 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.108 | 0.104\n",
      "2023-12-30 17:55:23 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.100 | 0.108\n",
      "2023-12-30 17:55:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.077 | 0.104\n",
      "2023-12-30 17:55:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.093 | 0.103\n",
      "2023-12-30 17:55:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.089 | 0.102\n",
      "2023-12-30 17:55:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.098 | 0.103\n",
      "2023-12-30 17:55:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.090 | 0.100\n",
      "2023-12-30 17:55:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.092 | 0.104\n",
      "2023-12-30 17:55:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.096 | 0.095\n",
      "2023-12-30 17:55:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.098 | 0.102\n",
      "2023-12-30 17:55:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.084 | 0.101\n",
      "2023-12-30 17:55:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.090 | 0.095\n",
      "2023-12-30 17:55:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.087 | 0.091\n",
      "2023-12-30 17:55:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.099 | 0.096\n",
      "2023-12-30 17:55:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.084 | 0.093\n",
      "2023-12-30 17:55:27 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.083 | 0.089\n",
      "2023-12-30 17:55:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.088 | 0.091\n",
      "2023-12-30 17:55:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.063 | 0.096\n",
      "2023-12-30 17:55:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.080 | 0.094\n",
      "2023-12-30 17:55:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.073 | 0.090\n",
      "2023-12-30 17:55:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.079 | 0.088\n",
      "2023-12-30 17:55:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.088 | 0.087\n",
      "2023-12-30 17:55:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.076 | 0.090\n",
      "2023-12-30 17:55:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.076 | 0.084\n",
      "2023-12-30 17:55:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.070 | 0.091\n",
      "2023-12-30 17:55:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.083 | 0.088\n",
      "2023-12-30 17:55:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.076 | 0.088\n",
      "2023-12-30 17:55:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.084 | 0.086\n",
      "2023-12-30 17:55:31 - INFO     | Early stopping: loss decreased (0.109 -> 0.094; -14.1%). Caching model state.\n",
      " 10%|█         | 2/20 [00:18<02:47,  9.28s/it]2023-12-30 17:55:31 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 17:55:31 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.060 | 0.090\n",
      "2023-12-30 17:55:32 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.066 | 0.083\n",
      "2023-12-30 17:55:32 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.094 | 0.087\n",
      "2023-12-30 17:55:32 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.061 | 0.082\n",
      "2023-12-30 17:55:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.071 | 0.084\n",
      "2023-12-30 17:55:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.069 | 0.082\n",
      "2023-12-30 17:55:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.057 | 0.083\n",
      "2023-12-30 17:55:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.075 | 0.085\n",
      "2023-12-30 17:55:34 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.047 | 0.083\n",
      "2023-12-30 17:55:34 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.070 | 0.078\n",
      "2023-12-30 17:55:34 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.071 | 0.081\n",
      "2023-12-30 17:55:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.066 | 0.083\n",
      "2023-12-30 17:55:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.054 | 0.086\n",
      "2023-12-30 17:55:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.061 | 0.082\n",
      "2023-12-30 17:55:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.073 | 0.086\n",
      "2023-12-30 17:55:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.066 | 0.078\n",
      "2023-12-30 17:55:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.056 | 0.078\n",
      "2023-12-30 17:55:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.059 | 0.077\n",
      "2023-12-30 17:55:37 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.059 | 0.078\n",
      "2023-12-30 17:55:37 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.062 | 0.076\n",
      "2023-12-30 17:55:37 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.067 | 0.076\n",
      "2023-12-30 17:55:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.063 | 0.080\n",
      "2023-12-30 17:55:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.064 | 0.080\n",
      "2023-12-30 17:55:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.073 | 0.083\n",
      "2023-12-30 17:55:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.067 | 0.078\n",
      "2023-12-30 17:55:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.077 | 0.078\n",
      "2023-12-30 17:55:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.056 | 0.080\n",
      "2023-12-30 17:55:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.077 | 0.076\n",
      "2023-12-30 17:55:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.076 | 0.074\n",
      "2023-12-30 17:55:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.071 | 0.073\n",
      "2023-12-30 17:55:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.047 | 0.084\n",
      "2023-12-30 17:55:40 - INFO     | Early stopping: loss decreased (0.094 -> 0.076; -18.9%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:27<02:37,  9.29s/it]2023-12-30 17:55:40 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 17:55:41 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.032 | 0.076\n",
      "2023-12-30 17:55:41 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.056 | 0.072\n",
      "2023-12-30 17:55:41 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.071 | 0.075\n",
      "2023-12-30 17:55:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.057 | 0.075\n",
      "2023-12-30 17:55:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.051 | 0.074\n",
      "2023-12-30 17:55:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.052 | 0.076\n",
      "2023-12-30 17:55:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.061 | 0.077\n",
      "2023-12-30 17:55:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.050 | 0.068\n",
      "2023-12-30 17:55:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.075 | 0.073\n",
      "2023-12-30 17:55:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.066 | 0.073\n",
      "2023-12-30 17:55:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.057 | 0.078\n",
      "2023-12-30 17:55:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.044 | 0.070\n",
      "2023-12-30 17:55:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.064 | 0.069\n",
      "2023-12-30 17:55:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.044 | 0.067\n",
      "2023-12-30 17:55:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.066 | 0.075\n",
      "2023-12-30 17:55:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.060 | 0.073\n",
      "2023-12-30 17:55:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.054 | 0.069\n",
      "2023-12-30 17:55:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.041 | 0.076\n",
      "2023-12-30 17:55:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.067 | 0.071\n",
      "2023-12-30 17:55:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.057 | 0.068\n",
      "2023-12-30 17:55:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.046 | 0.069\n",
      "2023-12-30 17:55:47 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.053 | 0.067\n",
      "2023-12-30 17:55:47 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.049 | 0.069\n",
      "2023-12-30 17:55:47 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.066 | 0.071\n",
      "2023-12-30 17:55:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.041 | 0.066\n",
      "2023-12-30 17:55:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.057 | 0.075\n",
      "2023-12-30 17:55:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.056 | 0.070\n",
      "2023-12-30 17:55:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.044 | 0.069\n",
      "2023-12-30 17:55:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.044 | 0.073\n",
      "2023-12-30 17:55:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.055 | 0.070\n",
      "2023-12-30 17:55:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.041 | 0.070\n",
      "2023-12-30 17:55:50 - INFO     | Early stopping: no decrease (0.076 vs 0.075); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:37<02:27,  9.24s/it]2023-12-30 17:55:50 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 17:55:50 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.051 | 0.072\n",
      "2023-12-30 17:55:50 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.043 | 0.069\n",
      "2023-12-30 17:55:50 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.046 | 0.071\n",
      "2023-12-30 17:55:51 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.046 | 0.069\n",
      "2023-12-30 17:55:51 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.058 | 0.072\n",
      "2023-12-30 17:55:51 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.036 | 0.068\n",
      "2023-12-30 17:55:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.041 | 0.069\n",
      "2023-12-30 17:55:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.040 | 0.072\n",
      "2023-12-30 17:55:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.060 | 0.069\n",
      "2023-12-30 17:55:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.042 | 0.069\n",
      "2023-12-30 17:55:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.046 | 0.066\n",
      "2023-12-30 17:55:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.057 | 0.072\n",
      "2023-12-30 17:55:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.042 | 0.072\n",
      "2023-12-30 17:55:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.044 | 0.071\n",
      "2023-12-30 17:55:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.034 | 0.066\n",
      "2023-12-30 17:55:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.040 | 0.067\n",
      "2023-12-30 17:55:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.047 | 0.070\n",
      "2023-12-30 17:55:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.043 | 0.068\n",
      "2023-12-30 17:55:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.040 | 0.065\n",
      "2023-12-30 17:55:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.056 | 0.063\n",
      "2023-12-30 17:55:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.057 | 0.069\n",
      "2023-12-30 17:55:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.047 | 0.066\n",
      "2023-12-30 17:55:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.046 | 0.067\n",
      "2023-12-30 17:55:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.054 | 0.068\n",
      "2023-12-30 17:55:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.044 | 0.068\n",
      "2023-12-30 17:55:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.042 | 0.065\n",
      "2023-12-30 17:55:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.058 | 0.062\n",
      "2023-12-30 17:55:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.042 | 0.067\n",
      "2023-12-30 17:55:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.044 | 0.068\n",
      "2023-12-30 17:55:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.043 | 0.062\n",
      "2023-12-30 17:55:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.057 | 0.064\n",
      "2023-12-30 17:55:59 - INFO     | Early stopping: loss decreased (0.076 -> 0.065; -14.2%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:46<02:18,  9.26s/it]2023-12-30 17:55:59 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 17:55:59 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.015 | 0.065\n",
      "2023-12-30 17:55:59 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.035 | 0.064\n",
      "2023-12-30 17:56:00 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.028 | 0.065\n",
      "2023-12-30 17:56:00 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.046 | 0.065\n",
      "2023-12-30 17:56:00 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.033 | 0.060\n",
      "2023-12-30 17:56:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.043 | 0.060\n",
      "2023-12-30 17:56:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.042 | 0.060\n",
      "2023-12-30 17:56:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.034 | 0.064\n",
      "2023-12-30 17:56:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.041 | 0.065\n",
      "2023-12-30 17:56:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.035 | 0.068\n",
      "2023-12-30 17:56:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.036 | 0.065\n",
      "2023-12-30 17:56:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.052 | 0.064\n",
      "2023-12-30 17:56:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.045 | 0.064\n",
      "2023-12-30 17:56:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.031 | 0.064\n",
      "2023-12-30 17:56:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.046 | 0.069\n",
      "2023-12-30 17:56:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.038 | 0.066\n",
      "2023-12-30 17:56:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.045 | 0.067\n",
      "2023-12-30 17:56:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.057 | 0.065\n",
      "2023-12-30 17:56:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.042 | 0.065\n",
      "2023-12-30 17:56:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.038 | 0.065\n",
      "2023-12-30 17:56:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.039 | 0.066\n",
      "2023-12-30 17:56:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.047 | 0.066\n",
      "2023-12-30 17:56:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.043 | 0.066\n",
      "2023-12-30 17:56:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.055 | 0.070\n",
      "2023-12-30 17:56:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.058 | 0.061\n",
      "2023-12-30 17:56:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.027 | 0.068\n",
      "2023-12-30 17:56:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.049 | 0.064\n",
      "2023-12-30 17:56:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.029 | 0.061\n",
      "2023-12-30 17:56:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.042 | 0.064\n",
      "2023-12-30 17:56:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.036 | 0.062\n",
      "2023-12-30 17:56:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.050 | 0.061\n",
      "2023-12-30 17:56:08 - INFO     | Early stopping: loss decreased (0.065 -> 0.060; -7.7%). Caching model state.\n",
      " 30%|███       | 6/20 [00:55<02:10,  9.31s/it]2023-12-30 17:56:08 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 17:56:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.017 | 0.060\n",
      "2023-12-30 17:56:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.030 | 0.062\n",
      "2023-12-30 17:56:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.033 | 0.061\n",
      "2023-12-30 17:56:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.044 | 0.064\n",
      "2023-12-30 17:56:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.031 | 0.061\n",
      "2023-12-30 17:56:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.041 | 0.064\n",
      "2023-12-30 17:56:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.028 | 0.067\n",
      "2023-12-30 17:56:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.046 | 0.064\n",
      "2023-12-30 17:56:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.033 | 0.065\n",
      "2023-12-30 17:56:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.041 | 0.066\n",
      "2023-12-30 17:56:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.046 | 0.065\n",
      "2023-12-30 17:56:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.041 | 0.062\n",
      "2023-12-30 17:56:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.041 | 0.063\n",
      "2023-12-30 17:56:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.032 | 0.060\n",
      "2023-12-30 17:56:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.039 | 0.060\n",
      "2023-12-30 17:56:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.034 | 0.059\n",
      "2023-12-30 17:56:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.038 | 0.064\n",
      "2023-12-30 17:56:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.033 | 0.060\n",
      "2023-12-30 17:56:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.043 | 0.061\n",
      "2023-12-30 17:56:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.033 | 0.060\n",
      "2023-12-30 17:56:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.039 | 0.068\n",
      "2023-12-30 17:56:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.031 | 0.065\n",
      "2023-12-30 17:56:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.047 | 0.061\n",
      "2023-12-30 17:56:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.034 | 0.063\n",
      "2023-12-30 17:56:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.043 | 0.059\n",
      "2023-12-30 17:56:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.033 | 0.060\n",
      "2023-12-30 17:56:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.036 | 0.061\n",
      "2023-12-30 17:56:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.035 | 0.058\n",
      "2023-12-30 17:56:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.027 | 0.059\n",
      "2023-12-30 17:56:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.034 | 0.062\n",
      "2023-12-30 17:56:18 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.031 | 0.060\n",
      "2023-12-30 17:56:18 - INFO     | Early stopping: no decrease (0.060 vs 0.065); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:05<02:01,  9.38s/it]2023-12-30 17:56:18 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 17:56:18 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.046 | 0.063\n",
      "2023-12-30 17:56:18 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.025 | 0.057\n",
      "2023-12-30 17:56:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.029 | 0.061\n",
      "2023-12-30 17:56:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.033 | 0.060\n",
      "2023-12-30 17:56:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.023 | 0.059\n",
      "2023-12-30 17:56:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.030 | 0.059\n",
      "2023-12-30 17:56:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.024 | 0.060\n",
      "2023-12-30 17:56:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.031 | 0.064\n",
      "2023-12-30 17:56:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.042 | 0.055\n",
      "2023-12-30 17:56:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.028 | 0.058\n",
      "2023-12-30 17:56:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.029 | 0.059\n",
      "2023-12-30 17:56:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.039 | 0.062\n",
      "2023-12-30 17:56:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.044 | 0.058\n",
      "2023-12-30 17:56:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.039 | 0.059\n",
      "2023-12-30 17:56:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.029 | 0.057\n",
      "2023-12-30 17:56:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.033 | 0.057\n",
      "2023-12-30 17:56:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.025 | 0.061\n",
      "2023-12-30 17:56:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.041 | 0.057\n",
      "2023-12-30 17:56:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.033 | 0.057\n",
      "2023-12-30 17:56:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.031 | 0.059\n",
      "2023-12-30 17:56:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.041 | 0.059\n",
      "2023-12-30 17:56:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.031 | 0.056\n",
      "2023-12-30 17:56:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.033 | 0.059\n",
      "2023-12-30 17:56:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.034 | 0.058\n",
      "2023-12-30 17:56:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.033 | 0.056\n",
      "2023-12-30 17:56:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.028 | 0.057\n",
      "2023-12-30 17:56:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.056 | 0.058\n",
      "2023-12-30 17:56:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.024 | 0.059\n",
      "2023-12-30 17:56:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.032 | 0.059\n",
      "2023-12-30 17:56:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.047 | 0.060\n",
      "2023-12-30 17:56:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.029 | 0.058\n",
      "2023-12-30 17:56:27 - INFO     | Early stopping: no decrease (0.060 vs 0.063); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:14<01:51,  9.31s/it]2023-12-30 17:56:27 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 17:56:27 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.021 | 0.061\n",
      "2023-12-30 17:56:27 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.032 | 0.057\n",
      "2023-12-30 17:56:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.030 | 0.058\n",
      "2023-12-30 17:56:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.028 | 0.059\n",
      "2023-12-30 17:56:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.030 | 0.056\n",
      "2023-12-30 17:56:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.033 | 0.063\n",
      "2023-12-30 17:56:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.038 | 0.056\n",
      "2023-12-30 17:56:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.039 | 0.057\n",
      "2023-12-30 17:56:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.028 | 0.055\n",
      "2023-12-30 17:56:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.037 | 0.061\n",
      "2023-12-30 17:56:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.031 | 0.059\n",
      "2023-12-30 17:56:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.022 | 0.060\n",
      "2023-12-30 17:56:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.019 | 0.060\n",
      "2023-12-30 17:56:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.027 | 0.059\n",
      "2023-12-30 17:56:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.030 | 0.057\n",
      "2023-12-30 17:56:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.028 | 0.059\n",
      "2023-12-30 17:56:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.035 | 0.056\n",
      "2023-12-30 17:56:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.031 | 0.060\n",
      "2023-12-30 17:56:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.022 | 0.054\n",
      "2023-12-30 17:56:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.022 | 0.058\n",
      "2023-12-30 17:56:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.025 | 0.057\n",
      "2023-12-30 17:56:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.034 | 0.056\n",
      "2023-12-30 17:56:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.036 | 0.056\n",
      "2023-12-30 17:56:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.030 | 0.058\n",
      "2023-12-30 17:56:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.025 | 0.057\n",
      "2023-12-30 17:56:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.024 | 0.057\n",
      "2023-12-30 17:56:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.033 | 0.056\n",
      "2023-12-30 17:56:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.035 | 0.059\n",
      "2023-12-30 17:56:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.024 | 0.054\n",
      "2023-12-30 17:56:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.031 | 0.057\n",
      "2023-12-30 17:56:36 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.041 | 0.054\n",
      "2023-12-30 17:56:36 - INFO     | Early stopping: loss decreased (0.060 -> 0.053; -11.4%). Caching model state.\n",
      " 45%|████▌     | 9/20 [01:23<01:41,  9.23s/it]2023-12-30 17:56:36 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 17:56:36 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.012 | 0.053\n",
      "2023-12-30 17:56:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.028 | 0.057\n",
      "2023-12-30 17:56:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.023 | 0.059\n",
      "2023-12-30 17:56:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.021 | 0.063\n",
      "2023-12-30 17:56:37 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.036 | 0.057\n",
      "2023-12-30 17:56:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.024 | 0.054\n",
      "2023-12-30 17:56:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.051 | 0.059\n",
      "2023-12-30 17:56:38 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.015 | 0.055\n",
      "2023-12-30 17:56:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.017 | 0.057\n",
      "2023-12-30 17:56:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.032 | 0.056\n",
      "2023-12-30 17:56:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.026 | 0.053\n",
      "2023-12-30 17:56:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.024 | 0.056\n",
      "2023-12-30 17:56:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.024 | 0.053\n",
      "2023-12-30 17:56:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.018 | 0.056\n",
      "2023-12-30 17:56:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.031 | 0.055\n",
      "2023-12-30 17:56:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.025 | 0.055\n",
      "2023-12-30 17:56:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.023 | 0.057\n",
      "2023-12-30 17:56:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.024 | 0.056\n",
      "2023-12-30 17:56:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.024 | 0.054\n",
      "2023-12-30 17:56:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.029 | 0.053\n",
      "2023-12-30 17:56:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.033 | 0.054\n",
      "2023-12-30 17:56:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.023 | 0.055\n",
      "2023-12-30 17:56:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.027 | 0.059\n",
      "2023-12-30 17:56:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.030 | 0.054\n",
      "2023-12-30 17:56:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.033 | 0.055\n",
      "2023-12-30 17:56:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.030 | 0.054\n",
      "2023-12-30 17:56:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.030 | 0.059\n",
      "2023-12-30 17:56:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.026 | 0.059\n",
      "2023-12-30 17:56:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.027 | 0.056\n",
      "2023-12-30 17:56:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.034 | 0.060\n",
      "2023-12-30 17:56:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.029 | 0.056\n",
      "2023-12-30 17:56:45 - INFO     | Early stopping: no decrease (0.053 vs 0.056); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:32<01:32,  9.23s/it]2023-12-30 17:56:45 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 17:56:45 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.001 | 0.056\n",
      "2023-12-30 17:56:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.018 | 0.054\n",
      "2023-12-30 17:56:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.028 | 0.054\n",
      "2023-12-30 17:56:46 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.024 | 0.053\n",
      "2023-12-30 17:56:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.024 | 0.063\n",
      "2023-12-30 17:56:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.026 | 0.056\n",
      "2023-12-30 17:56:47 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.020 | 0.053\n",
      "2023-12-30 17:56:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.025 | 0.061\n",
      "2023-12-30 17:56:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.019 | 0.054\n",
      "2023-12-30 17:56:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.021 | 0.057\n",
      "2023-12-30 17:56:48 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.024 | 0.056\n",
      "2023-12-30 17:56:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.029 | 0.058\n",
      "2023-12-30 17:56:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.032 | 0.055\n",
      "2023-12-30 17:56:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.020 | 0.056\n",
      "2023-12-30 17:56:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.027 | 0.055\n",
      "2023-12-30 17:56:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.017 | 0.056\n",
      "2023-12-30 17:56:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.020 | 0.058\n",
      "2023-12-30 17:56:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.020 | 0.056\n",
      "2023-12-30 17:56:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.028 | 0.054\n",
      "2023-12-30 17:56:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.024 | 0.055\n",
      "2023-12-30 17:56:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.025 | 0.056\n",
      "2023-12-30 17:56:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.019 | 0.053\n",
      "2023-12-30 17:56:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.026 | 0.056\n",
      "2023-12-30 17:56:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.037 | 0.055\n",
      "2023-12-30 17:56:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.023 | 0.052\n",
      "2023-12-30 17:56:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.032 | 0.055\n",
      "2023-12-30 17:56:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.039 | 0.061\n",
      "2023-12-30 17:56:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.025 | 0.054\n",
      "2023-12-30 17:56:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.025 | 0.054\n",
      "2023-12-30 17:56:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.025 | 0.054\n",
      "2023-12-30 17:56:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.024 | 0.053\n",
      "2023-12-30 17:56:55 - INFO     | Early stopping: no decrease (0.053 vs 0.053); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:42<01:23,  9.25s/it]2023-12-30 17:56:55 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 17:56:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.052 | 0.053\n",
      "2023-12-30 17:56:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.020 | 0.057\n",
      "2023-12-30 17:56:55 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.017 | 0.054\n",
      "2023-12-30 17:56:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.023 | 0.055\n",
      "2023-12-30 17:56:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.015 | 0.055\n",
      "2023-12-30 17:56:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.042 | 0.056\n",
      "2023-12-30 17:56:56 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.023 | 0.055\n",
      "2023-12-30 17:56:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.019 | 0.054\n",
      "2023-12-30 17:56:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.017 | 0.056\n",
      "2023-12-30 17:56:57 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.019 | 0.058\n",
      "2023-12-30 17:56:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.018 | 0.053\n",
      "2023-12-30 17:56:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.028 | 0.053\n",
      "2023-12-30 17:56:58 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.027 | 0.054\n",
      "2023-12-30 17:56:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.019 | 0.054\n",
      "2023-12-30 17:56:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.037 | 0.055\n",
      "2023-12-30 17:56:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.028 | 0.058\n",
      "2023-12-30 17:56:59 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.025 | 0.055\n",
      "2023-12-30 17:57:00 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.021 | 0.061\n",
      "2023-12-30 17:57:00 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.023 | 0.055\n",
      "2023-12-30 17:57:00 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.022 | 0.056\n",
      "2023-12-30 17:57:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.018 | 0.053\n",
      "2023-12-30 17:57:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.023 | 0.051\n",
      "2023-12-30 17:57:01 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.025 | 0.053\n",
      "2023-12-30 17:57:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.017 | 0.055\n",
      "2023-12-30 17:57:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.021 | 0.056\n",
      "2023-12-30 17:57:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.023 | 0.055\n",
      "2023-12-30 17:57:02 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.019 | 0.052\n",
      "2023-12-30 17:57:03 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.016 | 0.055\n",
      "2023-12-30 17:57:03 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.033 | 0.056\n",
      "2023-12-30 17:57:03 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.025 | 0.053\n",
      "2023-12-30 17:57:04 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.023 | 0.057\n",
      "2023-12-30 17:57:04 - INFO     | Early stopping: no decrease (0.053 vs 0.057); counter: 3 out of 3\n",
      "2023-12-30 17:57:04 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:57:04 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 60%|██████    | 12/20 [01:51<01:14,  9.28s/it]2023-12-30 17:57:04 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 17:57:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.014 | 0.056\n",
      "2023-12-30 17:57:04 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.020 | 0.053\n",
      "2023-12-30 17:57:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.017 | 0.052\n",
      "2023-12-30 17:57:05 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.028 | 0.052\n",
      "2023-12-30 17:57:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.015 | 0.054\n",
      "2023-12-30 17:57:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.012 | 0.053\n",
      "2023-12-30 17:57:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.011 | 0.051\n",
      "2023-12-30 17:57:06 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.010 | 0.053\n",
      "2023-12-30 17:57:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.015 | 0.053\n",
      "2023-12-30 17:57:07 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.019 | 0.053\n",
      "2023-12-30 17:57:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.021 | 0.053\n",
      "2023-12-30 17:57:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.025 | 0.052\n",
      "2023-12-30 17:57:08 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.017 | 0.052\n",
      "2023-12-30 17:57:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.026 | 0.053\n",
      "2023-12-30 17:57:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.018 | 0.051\n",
      "2023-12-30 17:57:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.019 | 0.053\n",
      "2023-12-30 17:57:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.022 | 0.052\n",
      "2023-12-30 17:57:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.024 | 0.057\n",
      "2023-12-30 17:57:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.021 | 0.053\n",
      "2023-12-30 17:57:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.011 | 0.052\n",
      "2023-12-30 17:57:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.025 | 0.053\n",
      "2023-12-30 17:57:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.023 | 0.052\n",
      "2023-12-30 17:57:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.021 | 0.051\n",
      "2023-12-30 17:57:13 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.023 | 0.053\n",
      "2023-12-30 17:57:13 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.016 | 0.053\n",
      "2023-12-30 17:57:13 - INFO     | Early stopping: no decrease (0.053 vs 0.053); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:00<01:04,  9.27s/it]2023-12-30 17:57:13 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 17:57:13 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.005 | 0.053\n",
      "2023-12-30 17:57:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.012 | 0.052\n",
      "2023-12-30 17:57:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.014 | 0.053\n",
      "2023-12-30 17:57:14 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.016 | 0.053\n",
      "2023-12-30 17:57:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.020 | 0.052\n",
      "2023-12-30 17:57:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.017 | 0.053\n",
      "2023-12-30 17:57:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.018 | 0.052\n",
      "2023-12-30 17:57:15 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.024 | 0.052\n",
      "2023-12-30 17:57:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.020 | 0.055\n",
      "2023-12-30 17:57:16 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.016 | 0.053\n",
      "2023-12-30 17:57:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.018 | 0.052\n",
      "2023-12-30 17:57:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.019 | 0.051\n",
      "2023-12-30 17:57:17 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.023 | 0.052\n",
      "2023-12-30 17:57:18 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:18 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:18 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.022 | 0.052\n",
      "2023-12-30 17:57:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.015 | 0.051\n",
      "2023-12-30 17:57:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.015 | 0.053\n",
      "2023-12-30 17:57:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.019 | 0.052\n",
      "2023-12-30 17:57:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.022 | 0.052\n",
      "2023-12-30 17:57:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.019 | 0.052\n",
      "2023-12-30 17:57:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.017 | 0.051\n",
      "2023-12-30 17:57:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.023 | 0.053\n",
      "2023-12-30 17:57:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.018 | 0.051\n",
      "2023-12-30 17:57:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.012 | 0.052\n",
      "2023-12-30 17:57:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:22 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.022 | 0.053\n",
      "2023-12-30 17:57:22 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.016 | 0.050\n",
      "2023-12-30 17:57:22 - INFO     | Early stopping: loss decreased (0.053 -> 0.050; -5.8%). Caching model state.\n",
      " 70%|███████   | 14/20 [02:09<00:55,  9.27s/it]2023-12-30 17:57:22 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 17:57:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.008 | 0.050\n",
      "2023-12-30 17:57:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.017 | 0.053\n",
      "2023-12-30 17:57:23 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.019 | 0.052\n",
      "2023-12-30 17:57:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.018 | 0.052\n",
      "2023-12-30 17:57:24 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.017 | 0.051\n",
      "2023-12-30 17:57:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.011 | 0.052\n",
      "2023-12-30 17:57:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.011 | 0.051\n",
      "2023-12-30 17:57:25 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.020 | 0.051\n",
      "2023-12-30 17:57:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.026 | 0.051\n",
      "2023-12-30 17:57:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.018 | 0.053\n",
      "2023-12-30 17:57:26 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.017 | 0.052\n",
      "2023-12-30 17:57:27 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:27 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:27 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.022 | 0.052\n",
      "2023-12-30 17:57:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.023 | 0.050\n",
      "2023-12-30 17:57:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.018 | 0.051\n",
      "2023-12-30 17:57:28 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.017 | 0.050\n",
      "2023-12-30 17:57:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.015 | 0.051\n",
      "2023-12-30 17:57:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.017 | 0.052\n",
      "2023-12-30 17:57:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.014 | 0.053\n",
      "2023-12-30 17:57:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.017 | 0.050\n",
      "2023-12-30 17:57:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.018 | 0.052\n",
      "2023-12-30 17:57:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.017 | 0.053\n",
      "2023-12-30 17:57:31 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.011 | 0.051\n",
      "2023-12-30 17:57:31 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.013 | 0.054\n",
      "2023-12-30 17:57:31 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.021 | 0.053\n",
      "2023-12-30 17:57:32 - INFO     | Early stopping: no decrease (0.050 vs 0.053); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:19<00:46,  9.24s/it]2023-12-30 17:57:32 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 17:57:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.001 | 0.053\n",
      "2023-12-30 17:57:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.019 | 0.052\n",
      "2023-12-30 17:57:32 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.017 | 0.053\n",
      "2023-12-30 17:57:33 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:33 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:33 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.018 | 0.052\n",
      "2023-12-30 17:57:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.013 | 0.052\n",
      "2023-12-30 17:57:34 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.011 | 0.054\n",
      "2023-12-30 17:57:35 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.020 | 0.054\n",
      "2023-12-30 17:57:35 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.017 | 0.053\n",
      "2023-12-30 17:57:35 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.026 | 0.051\n",
      "2023-12-30 17:57:36 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.009 | 0.051\n",
      "2023-12-30 17:57:36 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.017 | 0.052\n",
      "2023-12-30 17:57:36 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.011 | 0.052\n",
      "2023-12-30 17:57:36 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.019 | 0.055\n",
      "2023-12-30 17:57:37 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.015 | 0.051\n",
      "2023-12-30 17:57:37 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.017 | 0.053\n",
      "2023-12-30 17:57:37 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:38 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:38 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:38 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.024 | 0.051\n",
      "2023-12-30 17:57:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.012 | 0.052\n",
      "2023-12-30 17:57:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.024 | 0.052\n",
      "2023-12-30 17:57:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.012 | 0.052\n",
      "2023-12-30 17:57:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.018 | 0.053\n",
      "2023-12-30 17:57:40 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:40 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.015 | 0.052\n",
      "2023-12-30 17:57:40 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.015 | 0.051\n",
      "2023-12-30 17:57:41 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:41 - INFO     | Early stopping: no decrease (0.050 vs 0.051); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:28<00:36,  9.25s/it]2023-12-30 17:57:41 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 17:57:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.009 | 0.051\n",
      "2023-12-30 17:57:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.009 | 0.052\n",
      "2023-12-30 17:57:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.017 | 0.050\n",
      "2023-12-30 17:57:42 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.016 | 0.052\n",
      "2023-12-30 17:57:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.018 | 0.054\n",
      "2023-12-30 17:57:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.019 | 0.053\n",
      "2023-12-30 17:57:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.012 | 0.051\n",
      "2023-12-30 17:57:43 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:44 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:44 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.010 | 0.053\n",
      "2023-12-30 17:57:44 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.024 | 0.052\n",
      "2023-12-30 17:57:45 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.022 | 0.052\n",
      "2023-12-30 17:57:45 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.014 | 0.053\n",
      "2023-12-30 17:57:45 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.017 | 0.051\n",
      "2023-12-30 17:57:45 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:46 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:46 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.014 | 0.050\n",
      "2023-12-30 17:57:46 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.012 | 0.050\n",
      "2023-12-30 17:57:47 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:47 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.017 | 0.052\n",
      "2023-12-30 17:57:47 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.011 | 0.055\n",
      "2023-12-30 17:57:48 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.018 | 0.052\n",
      "2023-12-30 17:57:48 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.022 | 0.052\n",
      "2023-12-30 17:57:48 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:48 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.013 | 0.052\n",
      "2023-12-30 17:57:49 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.011 | 0.051\n",
      "2023-12-30 17:57:49 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:49 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.012 | 0.051\n",
      "2023-12-30 17:57:50 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.026 | 0.050\n",
      "2023-12-30 17:57:50 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.011 | 0.052\n",
      "2023-12-30 17:57:50 - INFO     | Early stopping: no decrease (0.050 vs 0.053); counter: 3 out of 3\n",
      "2023-12-30 17:57:50 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:57:50 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 85%|████████▌ | 17/20 [02:37<00:27,  9.26s/it]2023-12-30 17:57:50 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 17:57:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.005 | 0.052\n",
      "2023-12-30 17:57:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.013 | 0.052\n",
      "2023-12-30 17:57:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.012 | 0.051\n",
      "2023-12-30 17:57:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.011 | 0.051\n",
      "2023-12-30 17:57:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:52 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.012 | 0.051\n",
      "2023-12-30 17:57:53 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.012 | 0.051\n",
      "2023-12-30 17:57:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.017 | 0.051\n",
      "2023-12-30 17:57:54 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.014 | 0.051\n",
      "2023-12-30 17:57:55 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:55 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.019 | 0.051\n",
      "2023-12-30 17:57:55 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.022 | 0.051\n",
      "2023-12-30 17:57:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.016 | 0.051\n",
      "2023-12-30 17:57:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.010 | 0.051\n",
      "2023-12-30 17:57:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.008 | 0.051\n",
      "2023-12-30 17:57:56 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:57 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.010 | 0.052\n",
      "2023-12-30 17:57:57 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.013 | 0.052\n",
      "2023-12-30 17:57:57 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:58 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:58 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.011 | 0.051\n",
      "2023-12-30 17:57:58 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.013 | 0.051\n",
      "2023-12-30 17:57:58 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.014 | 0.052\n",
      "2023-12-30 17:57:59 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.013 | 0.050\n",
      "2023-12-30 17:57:59 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.025 | 0.052\n",
      "2023-12-30 17:57:59 - INFO     | Early stopping: no decrease (0.050 vs 0.052); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [02:46<00:18,  9.25s/it]2023-12-30 17:57:59 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 17:58:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.008 | 0.052\n",
      "2023-12-30 17:58:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.008 | 0.052\n",
      "2023-12-30 17:58:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.017 | 0.051\n",
      "2023-12-30 17:58:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.013 | 0.051\n",
      "2023-12-30 17:58:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.010 | 0.051\n",
      "2023-12-30 17:58:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.013 | 0.051\n",
      "2023-12-30 17:58:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.009 | 0.052\n",
      "2023-12-30 17:58:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.022 | 0.052\n",
      "2023-12-30 17:58:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.014 | 0.052\n",
      "2023-12-30 17:58:02 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.009 | 0.050\n",
      "2023-12-30 17:58:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.015 | 0.050\n",
      "2023-12-30 17:58:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:03 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.016 | 0.050\n",
      "2023-12-30 17:58:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.013 | 0.050\n",
      "2023-12-30 17:58:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.018 | 0.051\n",
      "2023-12-30 17:58:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.014 | 0.052\n",
      "2023-12-30 17:58:04 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.010 | 0.051\n",
      "2023-12-30 17:58:05 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.010 | 0.051\n",
      "2023-12-30 17:58:05 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.017 | 0.051\n",
      "2023-12-30 17:58:05 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.013 | 0.051\n",
      "2023-12-30 17:58:06 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.012 | 0.050\n",
      "2023-12-30 17:58:06 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:06 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.014 | 0.052\n",
      "2023-12-30 17:58:06 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:07 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.012 | 0.052\n",
      "2023-12-30 17:58:07 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.016 | 0.052\n",
      "2023-12-30 17:58:07 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.009 | 0.052\n",
      "2023-12-30 17:58:08 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.010 | 0.051\n",
      "2023-12-30 17:58:08 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.012 | 0.051\n",
      "2023-12-30 17:58:08 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.012 | 0.051\n",
      "2023-12-30 17:58:09 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.022 | 0.051\n",
      "2023-12-30 17:58:09 - INFO     | Early stopping: no decrease (0.050 vs 0.050); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [02:56<00:09,  9.32s/it]2023-12-30 17:58:09 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 17:58:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.008 | 0.051\n",
      "2023-12-30 17:58:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.007 | 0.051\n",
      "2023-12-30 17:58:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.008 | 0.051\n",
      "2023-12-30 17:58:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.013 | 0.052\n",
      "2023-12-30 17:58:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.013 | 0.051\n",
      "2023-12-30 17:58:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.009 | 0.052\n",
      "2023-12-30 17:58:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.014 | 0.051\n",
      "2023-12-30 17:58:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.013 | 0.052\n",
      "2023-12-30 17:58:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.016 | 0.051\n",
      "2023-12-30 17:58:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.017 | 0.051\n",
      "2023-12-30 17:58:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.016 | 0.051\n",
      "2023-12-30 17:58:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.007 | 0.051\n",
      "2023-12-30 17:58:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.012 | 0.051\n",
      "2023-12-30 17:58:13 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.013 | 0.052\n",
      "2023-12-30 17:58:14 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.009 | 0.051\n",
      "2023-12-30 17:58:14 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.009 | 0.051\n",
      "2023-12-30 17:58:14 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.009 | 0.051\n",
      "2023-12-30 17:58:15 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.016 | 0.050\n",
      "2023-12-30 17:58:15 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.015 | 0.050\n",
      "2023-12-30 17:58:15 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.012 | 0.051\n",
      "2023-12-30 17:58:15 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.029 | 0.051\n",
      "2023-12-30 17:58:16 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.013 | 0.050\n",
      "2023-12-30 17:58:16 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:16 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.012 | 0.052\n",
      "2023-12-30 17:58:17 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.012 | 0.050\n",
      "2023-12-30 17:58:17 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.009 | 0.050\n",
      "2023-12-30 17:58:17 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.013 | 0.051\n",
      "2023-12-30 17:58:18 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.015 | 0.051\n",
      "2023-12-30 17:58:18 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.015 | 0.050\n",
      "2023-12-30 17:58:18 - INFO     | Early stopping: no decrease (0.050 vs 0.050); counter: 3 out of 3\n",
      "2023-12-30 17:58:18 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:58:18 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      "100%|██████████| 20/20 [03:05<00:00,  9.28s/it]\n",
      "2023-12-30 17:58:18 - INFO     | Best validation loss: 0.050\n",
      "2023-12-30 17:58:18 - INFO     | Best early stopping index/epoch: 13\n",
      "2023-12-30 17:58:18 - INFO     | Average Loss on test set: 0.050\n",
      "2023-12-30 17:58:20 - INFO     | Weighted Precision: 0.986, Recall: 0.986, F1: 0.986\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>████████████▃▃▃▃▃▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▇▄▃▂▃▃▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▃▃▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_validation_loss</td><td>0.05016</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00025</td></tr><tr><td>step_learning_rate</td><td>0.00025</td></tr><tr><td>step_training_loss</td><td>0.01523</td></tr><tr><td>step_validation_loss</td><td>0.05016</td></tr><tr><td>test_loss</td><td>0.05018</td></tr><tr><td>weighted_f1</td><td>0.98628</td></tr><tr><td>weighted_precision</td><td>0.98631</td></tr><tr><td>weighted_recall</td><td>0.98629</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-12</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/eppo596o' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/eppo596o</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_175512-eppo596o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3fw5tcrd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [32, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_175829-3fw5tcrd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/3fw5tcrd' target=\"_blank\">polar-sweep-13</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/3fw5tcrd' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/3fw5tcrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [32, 64], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 17:58:29 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 17:58:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 25.805 | 642.185\n",
      "2023-12-30 17:58:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 43.595 | 1.804\n",
      "2023-12-30 17:58:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 1.151 | 0.757\n",
      "2023-12-30 17:58:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 0.618 | 0.626\n",
      "2023-12-30 17:58:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 0.555 | 0.517\n",
      "2023-12-30 17:58:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 0.441 | 0.421\n",
      "2023-12-30 17:58:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 0.423 | 0.330\n",
      "2023-12-30 17:58:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 0.369 | 0.334\n",
      "2023-12-30 17:58:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 0.340 | 0.295\n",
      "2023-12-30 17:58:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 0.311 | 0.310\n",
      "2023-12-30 17:58:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 0.292 | 0.331\n",
      "2023-12-30 17:58:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 0.348 | 0.254\n",
      "2023-12-30 17:58:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 0.229 | 0.265\n",
      "2023-12-30 17:58:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.228 | 0.261\n",
      "2023-12-30 17:58:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.221 | 0.209\n",
      "2023-12-30 17:58:34 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.211 | 0.220\n",
      "2023-12-30 17:58:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.240 | 0.234\n",
      "2023-12-30 17:58:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.266 | 0.346\n",
      "2023-12-30 17:58:35 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.268 | 0.242\n",
      "2023-12-30 17:58:36 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.214 | 0.239\n",
      "2023-12-30 17:58:36 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.227 | 0.253\n",
      "2023-12-30 17:58:36 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.242 | 0.210\n",
      "2023-12-30 17:58:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.204 | 0.237\n",
      "2023-12-30 17:58:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.270 | 0.234\n",
      "2023-12-30 17:58:37 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.264 | 0.197\n",
      "2023-12-30 17:58:38 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.227 | 0.181\n",
      "2023-12-30 17:58:38 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.182 | 0.200\n",
      "2023-12-30 17:58:38 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.227 | 0.202\n",
      "2023-12-30 17:58:39 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.234 | 0.221\n",
      "2023-12-30 17:58:39 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.224 | 0.209\n",
      "2023-12-30 17:58:39 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.212 | 0.241\n",
      "2023-12-30 17:58:40 - INFO     | Early stopping: loss decreased (inf -> 0.250; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:14, 10.25s/it]2023-12-30 17:58:40 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 17:58:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.137 | 0.236\n",
      "2023-12-30 17:58:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.221 | 0.209\n",
      "2023-12-30 17:58:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.194 | 0.241\n",
      "2023-12-30 17:58:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.230 | 0.322\n",
      "2023-12-30 17:58:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.224 | 0.299\n",
      "2023-12-30 17:58:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.228 | 0.231\n",
      "2023-12-30 17:58:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.169 | 0.224\n",
      "2023-12-30 17:58:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.184 | 0.285\n",
      "2023-12-30 17:58:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.258 | 0.208\n",
      "2023-12-30 17:58:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.229 | 0.214\n",
      "2023-12-30 17:58:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.198 | 0.214\n",
      "2023-12-30 17:58:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.225 | 0.200\n",
      "2023-12-30 17:58:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.211 | 0.248\n",
      "2023-12-30 17:58:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.210 | 0.244\n",
      "2023-12-30 17:58:44 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.206 | 0.189\n",
      "2023-12-30 17:58:45 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.206 | 0.198\n",
      "2023-12-30 17:58:45 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.241 | 0.179\n",
      "2023-12-30 17:58:45 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.209 | 0.178\n",
      "2023-12-30 17:58:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.181 | 0.240\n",
      "2023-12-30 17:58:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.221 | 0.272\n",
      "2023-12-30 17:58:46 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.218 | 0.227\n",
      "2023-12-30 17:58:47 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.237 | 0.321\n",
      "2023-12-30 17:58:47 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.239 | 0.220\n",
      "2023-12-30 17:58:47 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.202 | 0.219\n",
      "2023-12-30 17:58:48 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.237 | 0.248\n",
      "2023-12-30 17:58:48 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.211 | 0.270\n",
      "2023-12-30 17:58:48 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.240 | 0.217\n",
      "2023-12-30 17:58:49 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.220 | 0.217\n",
      "2023-12-30 17:58:49 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.232 | 0.197\n",
      "2023-12-30 17:58:49 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.226 | 0.214\n",
      "2023-12-30 17:58:49 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.211 | 0.228\n",
      "2023-12-30 17:58:50 - INFO     | Early stopping: no decrease (0.250 vs 0.242); counter: 1 out of 3\n",
      " 10%|█         | 2/20 [00:20<03:04, 10.22s/it]2023-12-30 17:58:50 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 17:58:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.151 | 0.257\n",
      "2023-12-30 17:58:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.166 | 0.215\n",
      "2023-12-30 17:58:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.179 | 0.256\n",
      "2023-12-30 17:58:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.220 | 0.250\n",
      "2023-12-30 17:58:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.213 | 0.207\n",
      "2023-12-30 17:58:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.153 | 0.195\n",
      "2023-12-30 17:58:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.178 | 0.194\n",
      "2023-12-30 17:58:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.195 | 0.211\n",
      "2023-12-30 17:58:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.226 | 0.306\n",
      "2023-12-30 17:58:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.226 | 0.223\n",
      "2023-12-30 17:58:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.181 | 0.232\n",
      "2023-12-30 17:58:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.216 | 0.275\n",
      "2023-12-30 17:58:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.230 | 0.213\n",
      "2023-12-30 17:58:54 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.180 | 0.187\n",
      "2023-12-30 17:58:55 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.175 | 0.209\n",
      "2023-12-30 17:58:55 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.219 | 0.231\n",
      "2023-12-30 17:58:55 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.209 | 0.222\n",
      "2023-12-30 17:58:56 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.210 | 0.242\n",
      "2023-12-30 17:58:56 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.236 | 0.236\n",
      "2023-12-30 17:58:56 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.232 | 0.213\n",
      "2023-12-30 17:58:57 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.227 | 0.241\n",
      "2023-12-30 17:58:57 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.226 | 0.230\n",
      "2023-12-30 17:58:57 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.277 | 0.218\n",
      "2023-12-30 17:58:58 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.180 | 0.184\n",
      "2023-12-30 17:58:58 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.208 | 0.186\n",
      "2023-12-30 17:58:58 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.237 | 0.236\n",
      "2023-12-30 17:58:58 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.260 | 0.199\n",
      "2023-12-30 17:58:59 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.233 | 0.210\n",
      "2023-12-30 17:58:59 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.183 | 0.213\n",
      "2023-12-30 17:58:59 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.209 | 0.203\n",
      "2023-12-30 17:59:00 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.242 | 0.221\n",
      "2023-12-30 17:59:00 - INFO     | Early stopping: loss decreased (0.250 -> 0.214; -14.3%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:53, 10.22s/it]2023-12-30 17:59:00 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 17:59:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.129 | 0.215\n",
      "2023-12-30 17:59:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.176 | 0.227\n",
      "2023-12-30 17:59:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.234 | 0.222\n",
      "2023-12-30 17:59:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.234 | 0.348\n",
      "2023-12-30 17:59:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.285 | 0.262\n",
      "2023-12-30 17:59:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.242 | 0.240\n",
      "2023-12-30 17:59:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.215 | 0.257\n",
      "2023-12-30 17:59:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.192 | 0.244\n",
      "2023-12-30 17:59:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.263 | 0.226\n",
      "2023-12-30 17:59:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.222 | 0.183\n",
      "2023-12-30 17:59:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.187 | 0.207\n",
      "2023-12-30 17:59:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.194 | 0.247\n",
      "2023-12-30 17:59:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.183 | 0.220\n",
      "2023-12-30 17:59:04 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.213 | 0.184\n",
      "2023-12-30 17:59:05 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.228 | 0.200\n",
      "2023-12-30 17:59:05 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.154 | 0.222\n",
      "2023-12-30 17:59:05 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.168 | 0.191\n",
      "2023-12-30 17:59:06 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.250 | 0.202\n",
      "2023-12-30 17:59:06 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.162 | 0.203\n",
      "2023-12-30 17:59:06 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.252 | 0.279\n",
      "2023-12-30 17:59:07 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.254 | 0.272\n",
      "2023-12-30 17:59:07 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.250 | 0.229\n",
      "2023-12-30 17:59:07 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.283 | 0.241\n",
      "2023-12-30 17:59:07 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.226 | 0.209\n",
      "2023-12-30 17:59:08 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.161 | 0.201\n",
      "2023-12-30 17:59:08 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.199 | 0.253\n",
      "2023-12-30 17:59:08 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.220 | 0.185\n",
      "2023-12-30 17:59:09 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.219 | 0.223\n",
      "2023-12-30 17:59:09 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.209 | 0.209\n",
      "2023-12-30 17:59:09 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.217 | 0.195\n",
      "2023-12-30 17:59:10 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.174 | 0.224\n",
      "2023-12-30 17:59:10 - INFO     | Early stopping: no decrease (0.214 vs 0.236); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:41, 10.12s/it]2023-12-30 17:59:10 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 17:59:10 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.077 | 0.241\n",
      "2023-12-30 17:59:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.208 | 0.317\n",
      "2023-12-30 17:59:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.248 | 0.226\n",
      "2023-12-30 17:59:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.196 | 0.220\n",
      "2023-12-30 17:59:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.183 | 0.186\n",
      "2023-12-30 17:59:12 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.129 | 0.206\n",
      "2023-12-30 17:59:12 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.250 | 0.246\n",
      "2023-12-30 17:59:12 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.259 | 0.265\n",
      "2023-12-30 17:59:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.213 | 0.202\n",
      "2023-12-30 17:59:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.171 | 0.180\n",
      "2023-12-30 17:59:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.198 | 0.199\n",
      "2023-12-30 17:59:14 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.134 | 0.196\n",
      "2023-12-30 17:59:14 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.166 | 0.203\n",
      "2023-12-30 17:59:14 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.227 | 0.209\n",
      "2023-12-30 17:59:15 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.185 | 0.244\n",
      "2023-12-30 17:59:15 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.199 | 0.217\n",
      "2023-12-30 17:59:15 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.189 | 0.214\n",
      "2023-12-30 17:59:16 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.175 | 0.235\n",
      "2023-12-30 17:59:16 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.176 | 0.211\n",
      "2023-12-30 17:59:16 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.189 | 0.199\n",
      "2023-12-30 17:59:17 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.187 | 0.235\n",
      "2023-12-30 17:59:17 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.262 | 0.218\n",
      "2023-12-30 17:59:17 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.204 | 0.203\n",
      "2023-12-30 17:59:17 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.170 | 0.194\n",
      "2023-12-30 17:59:18 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.195 | 0.233\n",
      "2023-12-30 17:59:18 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.195 | 0.174\n",
      "2023-12-30 17:59:18 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.239 | 0.195\n",
      "2023-12-30 17:59:19 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.191 | 0.204\n",
      "2023-12-30 17:59:19 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.177 | 0.190\n",
      "2023-12-30 17:59:19 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.185 | 0.191\n",
      "2023-12-30 17:59:20 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.157 | 0.196\n",
      "2023-12-30 17:59:20 - INFO     | Early stopping: no decrease (0.214 vs 0.222); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:50<02:31, 10.08s/it]2023-12-30 17:59:20 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 17:59:20 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.291 | 0.225\n",
      "2023-12-30 17:59:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.181 | 0.180\n",
      "2023-12-30 17:59:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.166 | 0.183\n",
      "2023-12-30 17:59:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.144 | 0.179\n",
      "2023-12-30 17:59:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.211 | 0.194\n",
      "2023-12-30 17:59:22 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.199 | 0.166\n",
      "2023-12-30 17:59:22 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.166 | 0.167\n",
      "2023-12-30 17:59:22 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.218 | 0.191\n",
      "2023-12-30 17:59:23 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.197 | 0.184\n",
      "2023-12-30 17:59:23 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.189 | 0.213\n",
      "2023-12-30 17:59:23 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.189 | 0.166\n",
      "2023-12-30 17:59:24 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.151 | 0.166\n",
      "2023-12-30 17:59:24 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.149 | 0.197\n",
      "2023-12-30 17:59:24 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.195 | 0.192\n",
      "2023-12-30 17:59:25 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.194 | 0.148\n",
      "2023-12-30 17:59:25 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.136 | 0.154\n",
      "2023-12-30 17:59:25 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.191 | 0.226\n",
      "2023-12-30 17:59:26 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.245 | 0.227\n",
      "2023-12-30 17:59:26 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.209 | 0.176\n",
      "2023-12-30 17:59:26 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.167 | 0.252\n",
      "2023-12-30 17:59:26 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.174 | 0.203\n",
      "2023-12-30 17:59:27 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.208 | 0.230\n",
      "2023-12-30 17:59:27 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.191 | 0.203\n",
      "2023-12-30 17:59:27 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.260 | 0.251\n",
      "2023-12-30 17:59:28 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.195 | 0.186\n",
      "2023-12-30 17:59:28 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.186 | 0.215\n",
      "2023-12-30 17:59:28 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.208 | 0.225\n",
      "2023-12-30 17:59:29 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.151 | 0.194\n",
      "2023-12-30 17:59:29 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.191 | 0.192\n",
      "2023-12-30 17:59:29 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.213 | 0.206\n",
      "2023-12-30 17:59:30 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.256 | 0.223\n",
      "2023-12-30 17:59:30 - INFO     | Early stopping: no decrease (0.214 vs 0.250); counter: 3 out of 3\n",
      "2023-12-30 17:59:30 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 17:59:30 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 30%|███       | 6/20 [01:00<02:20, 10.04s/it]2023-12-30 17:59:30 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 17:59:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.400 | 0.218\n",
      "2023-12-30 17:59:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.163 | 0.164\n",
      "2023-12-30 17:59:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.149 | 0.169\n",
      "2023-12-30 17:59:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.125 | 0.146\n",
      "2023-12-30 17:59:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.146 | 0.163\n",
      "2023-12-30 17:59:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.142 | 0.167\n",
      "2023-12-30 17:59:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.103 | 0.156\n",
      "2023-12-30 17:59:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.147 | 0.166\n",
      "2023-12-30 17:59:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.107 | 0.182\n",
      "2023-12-30 17:59:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.152 | 0.152\n",
      "2023-12-30 17:59:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.122 | 0.149\n",
      "2023-12-30 17:59:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.166 | 0.151\n",
      "2023-12-30 17:59:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.160 | 0.178\n",
      "2023-12-30 17:59:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.160 | 0.178\n",
      "2023-12-30 17:59:35 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.133 | 0.174\n",
      "2023-12-30 17:59:35 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.163 | 0.157\n",
      "2023-12-30 17:59:35 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.140 | 0.133\n",
      "2023-12-30 17:59:36 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.132 | 0.140\n",
      "2023-12-30 17:59:36 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.134 | 0.149\n",
      "2023-12-30 17:59:36 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.109 | 0.132\n",
      "2023-12-30 17:59:37 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.130 | 0.122\n",
      "2023-12-30 17:59:37 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.118 | 0.128\n",
      "2023-12-30 17:59:37 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.131 | 0.159\n",
      "2023-12-30 17:59:38 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.113 | 0.142\n",
      "2023-12-30 17:59:38 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.111 | 0.170\n",
      "2023-12-30 17:59:38 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.117 | 0.129\n",
      "2023-12-30 17:59:39 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.140 | 0.135\n",
      "2023-12-30 17:59:39 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.131 | 0.140\n",
      "2023-12-30 17:59:39 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.110 | 0.146\n",
      "2023-12-30 17:59:40 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.136 | 0.125\n",
      "2023-12-30 17:59:40 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.158 | 0.130\n",
      "2023-12-30 17:59:40 - INFO     | Early stopping: loss decreased (0.214 -> 0.122; -43.0%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:10<02:11, 10.11s/it]2023-12-30 17:59:40 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 17:59:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.054 | 0.121\n",
      "2023-12-30 17:59:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.108 | 0.158\n",
      "2023-12-30 17:59:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.112 | 0.138\n",
      "2023-12-30 17:59:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.112 | 0.141\n",
      "2023-12-30 17:59:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.116 | 0.157\n",
      "2023-12-30 17:59:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.140 | 0.136\n",
      "2023-12-30 17:59:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.111 | 0.134\n",
      "2023-12-30 17:59:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.116 | 0.147\n",
      "2023-12-30 17:59:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.120 | 0.134\n",
      "2023-12-30 17:59:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.134 | 0.136\n",
      "2023-12-30 17:59:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.123 | 0.168\n",
      "2023-12-30 17:59:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.107 | 0.135\n",
      "2023-12-30 17:59:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.112 | 0.140\n",
      "2023-12-30 17:59:45 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.135 | 0.156\n",
      "2023-12-30 17:59:45 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.094 | 0.141\n",
      "2023-12-30 17:59:45 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.113 | 0.136\n",
      "2023-12-30 17:59:46 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.104 | 0.128\n",
      "2023-12-30 17:59:46 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.125 | 0.130\n",
      "2023-12-30 17:59:46 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.106 | 0.142\n",
      "2023-12-30 17:59:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.128 | 0.157\n",
      "2023-12-30 17:59:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.084 | 0.130\n",
      "2023-12-30 17:59:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.114 | 0.149\n",
      "2023-12-30 17:59:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.107 | 0.130\n",
      "2023-12-30 17:59:48 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.106 | 0.148\n",
      "2023-12-30 17:59:48 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.145 | 0.149\n",
      "2023-12-30 17:59:48 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.103 | 0.135\n",
      "2023-12-30 17:59:49 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.094 | 0.132\n",
      "2023-12-30 17:59:49 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.122 | 0.141\n",
      "2023-12-30 17:59:49 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.141 | 0.128\n",
      "2023-12-30 17:59:50 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.112 | 0.137\n",
      "2023-12-30 17:59:50 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.129 | 0.131\n",
      "2023-12-30 17:59:50 - INFO     | Early stopping: no decrease (0.122 vs 0.135); counter: 1 out of 3\n",
      " 40%|████      | 8/20 [01:21<02:01, 10.13s/it]2023-12-30 17:59:50 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 17:59:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.039 | 0.139\n",
      "2023-12-30 17:59:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.086 | 0.134\n",
      "2023-12-30 17:59:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.093 | 0.123\n",
      "2023-12-30 17:59:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.115 | 0.118\n",
      "2023-12-30 17:59:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.078 | 0.128\n",
      "2023-12-30 17:59:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.102 | 0.127\n",
      "2023-12-30 17:59:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.127 | 0.149\n",
      "2023-12-30 17:59:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.127 | 0.131\n",
      "2023-12-30 17:59:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.110 | 0.142\n",
      "2023-12-30 17:59:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.105 | 0.143\n",
      "2023-12-30 17:59:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.082 | 0.142\n",
      "2023-12-30 17:59:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.120 | 0.142\n",
      "2023-12-30 17:59:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.087 | 0.129\n",
      "2023-12-30 17:59:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.115 | 0.147\n",
      "2023-12-30 17:59:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.108 | 0.131\n",
      "2023-12-30 17:59:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.127 | 0.149\n",
      "2023-12-30 17:59:56 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.126 | 0.137\n",
      "2023-12-30 17:59:56 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.118 | 0.145\n",
      "2023-12-30 17:59:56 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.146 | 0.173\n",
      "2023-12-30 17:59:57 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.130 | 0.162\n",
      "2023-12-30 17:59:57 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.138 | 0.147\n",
      "2023-12-30 17:59:57 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.120 | 0.139\n",
      "2023-12-30 17:59:58 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.112 | 0.149\n",
      "2023-12-30 17:59:58 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.118 | 0.138\n",
      "2023-12-30 17:59:58 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.113 | 0.144\n",
      "2023-12-30 17:59:59 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.144 | 0.152\n",
      "2023-12-30 17:59:59 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.108 | 0.151\n",
      "2023-12-30 17:59:59 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.095 | 0.164\n",
      "2023-12-30 17:59:59 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.150 | 0.207\n",
      "2023-12-30 18:00:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.153 | 0.145\n",
      "2023-12-30 18:00:00 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.105 | 0.143\n",
      "2023-12-30 18:00:00 - INFO     | Early stopping: no decrease (0.122 vs 0.145); counter: 2 out of 3\n",
      " 45%|████▌     | 9/20 [01:31<01:50, 10.09s/it]2023-12-30 18:00:00 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 18:00:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.071 | 0.145\n",
      "2023-12-30 18:00:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.093 | 0.139\n",
      "2023-12-30 18:00:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.099 | 0.129\n",
      "2023-12-30 18:00:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.100 | 0.149\n",
      "2023-12-30 18:00:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.130 | 0.127\n",
      "2023-12-30 18:00:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.125 | 0.137\n",
      "2023-12-30 18:00:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.143 | 0.152\n",
      "2023-12-30 18:00:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.118 | 0.141\n",
      "2023-12-30 18:00:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.117 | 0.155\n",
      "2023-12-30 18:00:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.108 | 0.144\n",
      "2023-12-30 18:00:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.138 | 0.179\n",
      "2023-12-30 18:00:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.121 | 0.135\n",
      "2023-12-30 18:00:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.106 | 0.142\n",
      "2023-12-30 18:00:05 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.103 | 0.118\n",
      "2023-12-30 18:00:05 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.126 | 0.131\n",
      "2023-12-30 18:00:05 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.091 | 0.131\n",
      "2023-12-30 18:00:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.118 | 0.141\n",
      "2023-12-30 18:00:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.125 | 0.129\n",
      "2023-12-30 18:00:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.112 | 0.135\n",
      "2023-12-30 18:00:07 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.083 | 0.141\n",
      "2023-12-30 18:00:07 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.135 | 0.131\n",
      "2023-12-30 18:00:07 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.120 | 0.133\n",
      "2023-12-30 18:00:08 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.116 | 0.140\n",
      "2023-12-30 18:00:08 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.102 | 0.126\n",
      "2023-12-30 18:00:08 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.095 | 0.143\n",
      "2023-12-30 18:00:08 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.129 | 0.153\n",
      "2023-12-30 18:00:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.115 | 0.145\n",
      "2023-12-30 18:00:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.096 | 0.136\n",
      "2023-12-30 18:00:09 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.087 | 0.143\n",
      "2023-12-30 18:00:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.141 | 0.197\n",
      "2023-12-30 18:00:10 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.156 | 0.192\n",
      "2023-12-30 18:00:10 - INFO     | Early stopping: no decrease (0.122 vs 0.202); counter: 3 out of 3\n",
      "2023-12-30 18:00:10 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:00:10 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 50%|█████     | 10/20 [01:41<01:40, 10.06s/it]2023-12-30 18:00:10 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 18:00:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.108 | 0.190\n",
      "2023-12-30 18:00:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.141 | 0.130\n",
      "2023-12-30 18:00:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.072 | 0.135\n",
      "2023-12-30 18:00:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.087 | 0.132\n",
      "2023-12-30 18:00:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.102 | 0.128\n",
      "2023-12-30 18:00:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.084 | 0.121\n",
      "2023-12-30 18:00:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.084 | 0.132\n",
      "2023-12-30 18:00:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.079 | 0.119\n",
      "2023-12-30 18:00:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.103 | 0.113\n",
      "2023-12-30 18:00:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.111 | 0.111\n",
      "2023-12-30 18:00:14 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.081 | 0.130\n",
      "2023-12-30 18:00:14 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.088 | 0.118\n",
      "2023-12-30 18:00:14 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.084 | 0.121\n",
      "2023-12-30 18:00:15 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.086 | 0.129\n",
      "2023-12-30 18:00:15 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.065 | 0.120\n",
      "2023-12-30 18:00:15 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.082 | 0.113\n",
      "2023-12-30 18:00:16 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.085 | 0.116\n",
      "2023-12-30 18:00:16 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.070 | 0.109\n",
      "2023-12-30 18:00:16 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.074 | 0.115\n",
      "2023-12-30 18:00:17 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.081 | 0.111\n",
      "2023-12-30 18:00:17 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.079 | 0.112\n",
      "2023-12-30 18:00:17 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.081 | 0.111\n",
      "2023-12-30 18:00:18 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.079 | 0.117\n",
      "2023-12-30 18:00:18 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.074 | 0.116\n",
      "2023-12-30 18:00:18 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.082 | 0.117\n",
      "2023-12-30 18:00:19 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.069 | 0.109\n",
      "2023-12-30 18:00:19 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.063 | 0.111\n",
      "2023-12-30 18:00:19 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.077 | 0.116\n",
      "2023-12-30 18:00:19 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.087 | 0.130\n",
      "2023-12-30 18:00:20 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.078 | 0.122\n",
      "2023-12-30 18:00:20 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.083 | 0.129\n",
      "2023-12-30 18:00:20 - INFO     | Early stopping: no decrease (0.122 vs 0.123); counter: 1 out of 3\n",
      " 55%|█████▌    | 11/20 [01:51<01:30, 10.05s/it]2023-12-30 18:00:20 - INFO     | Epoch: 11 | Learning Rate: 0.003\n",
      "2023-12-30 18:00:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 616064 examples: 0.034 | 0.122\n",
      "2023-12-30 18:00:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 617920 examples: 0.066 | 0.128\n",
      "2023-12-30 18:00:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 619776 examples: 0.070 | 0.149\n",
      "2023-12-30 18:00:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 621632 examples: 0.056 | 0.125\n",
      "2023-12-30 18:00:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 623488 examples: 0.070 | 0.114\n",
      "2023-12-30 18:00:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 625344 examples: 0.058 | 0.117\n",
      "2023-12-30 18:00:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 627200 examples: 0.050 | 0.119\n",
      "2023-12-30 18:00:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 629056 examples: 0.065 | 0.116\n",
      "2023-12-30 18:00:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 630912 examples: 0.065 | 0.115\n",
      "2023-12-30 18:00:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 632768 examples: 0.075 | 0.123\n",
      "2023-12-30 18:00:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 634624 examples: 0.072 | 0.117\n",
      "2023-12-30 18:00:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 636480 examples: 0.072 | 0.121\n",
      "2023-12-30 18:00:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 638336 examples: 0.111 | 0.115\n",
      "2023-12-30 18:00:25 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 640192 examples: 0.065 | 0.114\n",
      "2023-12-30 18:00:25 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 642048 examples: 0.079 | 0.119\n",
      "2023-12-30 18:00:25 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 643904 examples: 0.081 | 0.116\n",
      "2023-12-30 18:00:26 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 645760 examples: 0.107 | 0.136\n",
      "2023-12-30 18:00:26 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 647616 examples: 0.084 | 0.116\n",
      "2023-12-30 18:00:26 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 649472 examples: 0.060 | 0.109\n",
      "2023-12-30 18:00:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 651328 examples: 0.076 | 0.124\n",
      "2023-12-30 18:00:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 653184 examples: 0.094 | 0.118\n",
      "2023-12-30 18:00:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 655040 examples: 0.061 | 0.120\n",
      "2023-12-30 18:00:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 656896 examples: 0.065 | 0.114\n",
      "2023-12-30 18:00:28 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 658752 examples: 0.072 | 0.117\n",
      "2023-12-30 18:00:28 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 660608 examples: 0.076 | 0.122\n",
      "2023-12-30 18:00:28 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 662464 examples: 0.060 | 0.124\n",
      "2023-12-30 18:00:29 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 664320 examples: 0.081 | 0.128\n",
      "2023-12-30 18:00:29 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 666176 examples: 0.076 | 0.124\n",
      "2023-12-30 18:00:29 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 668032 examples: 0.062 | 0.122\n",
      "2023-12-30 18:00:30 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 669888 examples: 0.083 | 0.123\n",
      "2023-12-30 18:00:30 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 671744 examples: 0.080 | 0.122\n",
      "2023-12-30 18:00:30 - INFO     | Early stopping: no decrease (0.122 vs 0.122); counter: 2 out of 3\n",
      " 60%|██████    | 12/20 [02:01<01:20, 10.07s/it]2023-12-30 18:00:30 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 18:00:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.072 | 0.122\n",
      "2023-12-30 18:00:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.043 | 0.127\n",
      "2023-12-30 18:00:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.084 | 0.130\n",
      "2023-12-30 18:00:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.051 | 0.121\n",
      "2023-12-30 18:00:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.063 | 0.131\n",
      "2023-12-30 18:00:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.058 | 0.126\n",
      "2023-12-30 18:00:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.077 | 0.126\n",
      "2023-12-30 18:00:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.056 | 0.122\n",
      "2023-12-30 18:00:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.059 | 0.129\n",
      "2023-12-30 18:00:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.081 | 0.125\n",
      "2023-12-30 18:00:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.062 | 0.135\n",
      "2023-12-30 18:00:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.059 | 0.132\n",
      "2023-12-30 18:00:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.056 | 0.129\n",
      "2023-12-30 18:00:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.089 | 0.136\n",
      "2023-12-30 18:00:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.088 | 0.122\n",
      "2023-12-30 18:00:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.059 | 0.126\n",
      "2023-12-30 18:00:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.065 | 0.132\n",
      "2023-12-30 18:00:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.061 | 0.130\n",
      "2023-12-30 18:00:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.056 | 0.118\n",
      "2023-12-30 18:00:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.061 | 0.122\n",
      "2023-12-30 18:00:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.067 | 0.122\n",
      "2023-12-30 18:00:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.071 | 0.131\n",
      "2023-12-30 18:00:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.084 | 0.135\n",
      "2023-12-30 18:00:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.085 | 0.131\n",
      "2023-12-30 18:00:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.099 | 0.126\n",
      "2023-12-30 18:00:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.069 | 0.119\n",
      "2023-12-30 18:00:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.068 | 0.111\n",
      "2023-12-30 18:00:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.053 | 0.119\n",
      "2023-12-30 18:00:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.083 | 0.113\n",
      "2023-12-30 18:00:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.065 | 0.109\n",
      "2023-12-30 18:00:40 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.067 | 0.115\n",
      "2023-12-30 18:00:41 - INFO     | Early stopping: no decrease (0.122 vs 0.119); counter: 3 out of 3\n",
      "2023-12-30 18:00:41 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:00:41 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 65%|██████▌   | 13/20 [02:11<01:10, 10.13s/it]2023-12-30 18:00:41 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 18:00:41 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.034 | 0.117\n",
      "2023-12-30 18:00:41 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.054 | 0.115\n",
      "2023-12-30 18:00:42 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.051 | 0.116\n",
      "2023-12-30 18:00:42 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.030 | 0.114\n",
      "2023-12-30 18:00:42 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.041 | 0.115\n",
      "2023-12-30 18:00:43 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.070 | 0.120\n",
      "2023-12-30 18:00:43 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.043 | 0.110\n",
      "2023-12-30 18:00:43 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.041 | 0.115\n",
      "2023-12-30 18:00:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.046 | 0.113\n",
      "2023-12-30 18:00:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.036 | 0.107\n",
      "2023-12-30 18:00:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.054 | 0.108\n",
      "2023-12-30 18:00:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.047 | 0.113\n",
      "2023-12-30 18:00:45 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.059 | 0.107\n",
      "2023-12-30 18:00:45 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.057 | 0.109\n",
      "2023-12-30 18:00:45 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.047 | 0.109\n",
      "2023-12-30 18:00:46 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.053 | 0.108\n",
      "2023-12-30 18:00:46 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.057 | 0.122\n",
      "2023-12-30 18:00:46 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.060 | 0.107\n",
      "2023-12-30 18:00:47 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.060 | 0.109\n",
      "2023-12-30 18:00:47 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.055 | 0.115\n",
      "2023-12-30 18:00:47 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.058 | 0.113\n",
      "2023-12-30 18:00:48 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.040 | 0.113\n",
      "2023-12-30 18:00:48 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.057 | 0.110\n",
      "2023-12-30 18:00:48 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.041 | 0.113\n",
      "2023-12-30 18:00:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.045 | 0.112\n",
      "2023-12-30 18:00:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.046 | 0.108\n",
      "2023-12-30 18:00:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.067 | 0.107\n",
      "2023-12-30 18:00:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.042 | 0.113\n",
      "2023-12-30 18:00:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.047 | 0.107\n",
      "2023-12-30 18:00:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.065 | 0.110\n",
      "2023-12-30 18:00:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.043 | 0.113\n",
      "2023-12-30 18:00:51 - INFO     | Early stopping: loss decreased (0.122 -> 0.112; -8.2%). Caching model state.\n",
      " 70%|███████   | 14/20 [02:21<01:00, 10.10s/it]2023-12-30 18:00:51 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:00:51 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.019 | 0.112\n",
      "2023-12-30 18:00:51 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.023 | 0.109\n",
      "2023-12-30 18:00:52 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.067 | 0.111\n",
      "2023-12-30 18:00:52 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.034 | 0.106\n",
      "2023-12-30 18:00:52 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.048 | 0.106\n",
      "2023-12-30 18:00:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.047 | 0.116\n",
      "2023-12-30 18:00:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.036 | 0.114\n",
      "2023-12-30 18:00:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.041 | 0.111\n",
      "2023-12-30 18:00:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.031 | 0.113\n",
      "2023-12-30 18:00:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.047 | 0.114\n",
      "2023-12-30 18:00:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.035 | 0.113\n",
      "2023-12-30 18:00:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.034 | 0.108\n",
      "2023-12-30 18:00:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.048 | 0.110\n",
      "2023-12-30 18:00:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.046 | 0.109\n",
      "2023-12-30 18:00:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.049 | 0.111\n",
      "2023-12-30 18:00:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.039 | 0.105\n",
      "2023-12-30 18:00:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.045 | 0.104\n",
      "2023-12-30 18:00:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.042 | 0.106\n",
      "2023-12-30 18:00:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.042 | 0.109\n",
      "2023-12-30 18:00:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.044 | 0.109\n",
      "2023-12-30 18:00:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.036 | 0.113\n",
      "2023-12-30 18:00:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.047 | 0.108\n",
      "2023-12-30 18:00:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.052 | 0.107\n",
      "2023-12-30 18:00:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.069 | 0.108\n",
      "2023-12-30 18:00:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.051 | 0.104\n",
      "2023-12-30 18:00:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.039 | 0.113\n",
      "2023-12-30 18:00:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.050 | 0.107\n",
      "2023-12-30 18:01:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.061 | 0.106\n",
      "2023-12-30 18:01:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.031 | 0.111\n",
      "2023-12-30 18:01:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.064 | 0.111\n",
      "2023-12-30 18:01:01 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.056 | 0.110\n",
      "2023-12-30 18:01:01 - INFO     | Early stopping: no decrease (0.112 vs 0.112); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:31<00:50, 10.07s/it]2023-12-30 18:01:01 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:01:01 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.046 | 0.112\n",
      "2023-12-30 18:01:01 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.043 | 0.110\n",
      "2023-12-30 18:01:02 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.026 | 0.104\n",
      "2023-12-30 18:01:02 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.025 | 0.111\n",
      "2023-12-30 18:01:02 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.039 | 0.108\n",
      "2023-12-30 18:01:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.031 | 0.109\n",
      "2023-12-30 18:01:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.045 | 0.106\n",
      "2023-12-30 18:01:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.036 | 0.107\n",
      "2023-12-30 18:01:04 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.037 | 0.112\n",
      "2023-12-30 18:01:04 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.028 | 0.111\n",
      "2023-12-30 18:01:04 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.038 | 0.108\n",
      "2023-12-30 18:01:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.032 | 0.111\n",
      "2023-12-30 18:01:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.043 | 0.118\n",
      "2023-12-30 18:01:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.026 | 0.114\n",
      "2023-12-30 18:01:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.040 | 0.127\n",
      "2023-12-30 18:01:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.048 | 0.118\n",
      "2023-12-30 18:01:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.048 | 0.114\n",
      "2023-12-30 18:01:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.052 | 0.110\n",
      "2023-12-30 18:01:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.055 | 0.118\n",
      "2023-12-30 18:01:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.028 | 0.115\n",
      "2023-12-30 18:01:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.042 | 0.123\n",
      "2023-12-30 18:01:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.054 | 0.122\n",
      "2023-12-30 18:01:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.063 | 0.115\n",
      "2023-12-30 18:01:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.041 | 0.114\n",
      "2023-12-30 18:01:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.033 | 0.113\n",
      "2023-12-30 18:01:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.058 | 0.116\n",
      "2023-12-30 18:01:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.064 | 0.110\n",
      "2023-12-30 18:01:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.045 | 0.110\n",
      "2023-12-30 18:01:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.050 | 0.109\n",
      "2023-12-30 18:01:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.045 | 0.109\n",
      "2023-12-30 18:01:11 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.043 | 0.110\n",
      "2023-12-30 18:01:11 - INFO     | Early stopping: no decrease (0.112 vs 0.110); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:41<00:40, 10.07s/it]2023-12-30 18:01:11 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:01:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.029 | 0.110\n",
      "2023-12-30 18:01:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.032 | 0.112\n",
      "2023-12-30 18:01:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.029 | 0.109\n",
      "2023-12-30 18:01:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.026 | 0.109\n",
      "2023-12-30 18:01:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.041 | 0.110\n",
      "2023-12-30 18:01:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.033 | 0.111\n",
      "2023-12-30 18:01:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.033 | 0.115\n",
      "2023-12-30 18:01:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.035 | 0.120\n",
      "2023-12-30 18:01:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.039 | 0.117\n",
      "2023-12-30 18:01:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.035 | 0.129\n",
      "2023-12-30 18:01:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.029 | 0.126\n",
      "2023-12-30 18:01:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.041 | 0.121\n",
      "2023-12-30 18:01:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.040 | 0.123\n",
      "2023-12-30 18:01:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.037 | 0.122\n",
      "2023-12-30 18:01:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.041 | 0.116\n",
      "2023-12-30 18:01:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.037 | 0.116\n",
      "2023-12-30 18:01:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.036 | 0.114\n",
      "2023-12-30 18:01:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.037 | 0.113\n",
      "2023-12-30 18:01:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.039 | 0.112\n",
      "2023-12-30 18:01:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.043 | 0.122\n",
      "2023-12-30 18:01:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.040 | 0.113\n",
      "2023-12-30 18:01:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.027 | 0.117\n",
      "2023-12-30 18:01:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.048 | 0.116\n",
      "2023-12-30 18:01:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.044 | 0.123\n",
      "2023-12-30 18:01:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.058 | 0.113\n",
      "2023-12-30 18:01:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.062 | 0.120\n",
      "2023-12-30 18:01:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.033 | 0.116\n",
      "2023-12-30 18:01:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.042 | 0.123\n",
      "2023-12-30 18:01:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.036 | 0.127\n",
      "2023-12-30 18:01:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.057 | 0.121\n",
      "2023-12-30 18:01:21 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.041 | 0.124\n",
      "2023-12-30 18:01:21 - INFO     | Early stopping: no decrease (0.112 vs 0.124); counter: 3 out of 3\n",
      "2023-12-30 18:01:21 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:01:21 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 85%|████████▌ | 17/20 [02:51<00:30, 10.08s/it]2023-12-30 18:01:21 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:01:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.015 | 0.123\n",
      "2023-12-30 18:01:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.030 | 0.118\n",
      "2023-12-30 18:01:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.027 | 0.121\n",
      "2023-12-30 18:01:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.022 | 0.119\n",
      "2023-12-30 18:01:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.027 | 0.121\n",
      "2023-12-30 18:01:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.022 | 0.125\n",
      "2023-12-30 18:01:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.025 | 0.120\n",
      "2023-12-30 18:01:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.019 | 0.121\n",
      "2023-12-30 18:01:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.025 | 0.118\n",
      "2023-12-30 18:01:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.018 | 0.118\n",
      "2023-12-30 18:01:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.028 | 0.120\n",
      "2023-12-30 18:01:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.032 | 0.121\n",
      "2023-12-30 18:01:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.031 | 0.123\n",
      "2023-12-30 18:01:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.033 | 0.121\n",
      "2023-12-30 18:01:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.026 | 0.122\n",
      "2023-12-30 18:01:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.030 | 0.123\n",
      "2023-12-30 18:01:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.025 | 0.120\n",
      "2023-12-30 18:01:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.028 | 0.120\n",
      "2023-12-30 18:01:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.030 | 0.123\n",
      "2023-12-30 18:01:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.029 | 0.122\n",
      "2023-12-30 18:01:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.035 | 0.124\n",
      "2023-12-30 18:01:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.028 | 0.120\n",
      "2023-12-30 18:01:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.031 | 0.121\n",
      "2023-12-30 18:01:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.026 | 0.123\n",
      "2023-12-30 18:01:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.039 | 0.126\n",
      "2023-12-30 18:01:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.032 | 0.124\n",
      "2023-12-30 18:01:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.042 | 0.122\n",
      "2023-12-30 18:01:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.058 | 0.120\n",
      "2023-12-30 18:01:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.028 | 0.122\n",
      "2023-12-30 18:01:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.032 | 0.124\n",
      "2023-12-30 18:01:31 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.032 | 0.124\n",
      "2023-12-30 18:01:31 - INFO     | Early stopping: no decrease (0.112 vs 0.124); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [03:01<00:20, 10.07s/it]2023-12-30 18:01:31 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 18:01:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.128 | 0.124\n",
      "2023-12-30 18:01:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.020 | 0.123\n",
      "2023-12-30 18:01:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.018 | 0.121\n",
      "2023-12-30 18:01:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.025 | 0.123\n",
      "2023-12-30 18:01:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.023 | 0.121\n",
      "2023-12-30 18:01:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.046 | 0.122\n",
      "2023-12-30 18:01:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.029 | 0.122\n",
      "2023-12-30 18:01:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.024 | 0.122\n",
      "2023-12-30 18:01:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.026 | 0.121\n",
      "2023-12-30 18:01:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.023 | 0.123\n",
      "2023-12-30 18:01:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.030 | 0.126\n",
      "2023-12-30 18:01:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.027 | 0.122\n",
      "2023-12-30 18:01:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.026 | 0.122\n",
      "2023-12-30 18:01:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.028 | 0.124\n",
      "2023-12-30 18:01:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.026 | 0.122\n",
      "2023-12-30 18:01:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.026 | 0.122\n",
      "2023-12-30 18:01:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.021 | 0.124\n",
      "2023-12-30 18:01:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.023 | 0.123\n",
      "2023-12-30 18:01:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.017 | 0.126\n",
      "2023-12-30 18:01:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.015 | 0.128\n",
      "2023-12-30 18:01:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.025 | 0.125\n",
      "2023-12-30 18:01:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.025 | 0.124\n",
      "2023-12-30 18:01:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.026 | 0.124\n",
      "2023-12-30 18:01:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.031 | 0.128\n",
      "2023-12-30 18:01:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.027 | 0.127\n",
      "2023-12-30 18:01:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.020 | 0.127\n",
      "2023-12-30 18:01:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.030 | 0.125\n",
      "2023-12-30 18:01:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.026 | 0.127\n",
      "2023-12-30 18:01:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.029 | 0.126\n",
      "2023-12-30 18:01:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.019 | 0.127\n",
      "2023-12-30 18:01:41 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.026 | 0.129\n",
      "2023-12-30 18:01:41 - INFO     | Early stopping: no decrease (0.112 vs 0.127); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [03:11<00:10, 10.04s/it]2023-12-30 18:01:41 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 18:01:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.003 | 0.127\n",
      "2023-12-30 18:01:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.013 | 0.125\n",
      "2023-12-30 18:01:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.025 | 0.127\n",
      "2023-12-30 18:01:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.033 | 0.124\n",
      "2023-12-30 18:01:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.016 | 0.127\n",
      "2023-12-30 18:01:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.020 | 0.128\n",
      "2023-12-30 18:01:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.023 | 0.122\n",
      "2023-12-30 18:01:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.017 | 0.124\n",
      "2023-12-30 18:01:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.038 | 0.125\n",
      "2023-12-30 18:01:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.031 | 0.125\n",
      "2023-12-30 18:01:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.019 | 0.124\n",
      "2023-12-30 18:01:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.027 | 0.126\n",
      "2023-12-30 18:01:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.034 | 0.131\n",
      "2023-12-30 18:01:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.023 | 0.128\n",
      "2023-12-30 18:01:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.031 | 0.129\n",
      "2023-12-30 18:01:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.023 | 0.129\n",
      "2023-12-30 18:01:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.024 | 0.128\n",
      "2023-12-30 18:01:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.027 | 0.127\n",
      "2023-12-30 18:01:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.019 | 0.127\n",
      "2023-12-30 18:01:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.020 | 0.127\n",
      "2023-12-30 18:01:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.019 | 0.129\n",
      "2023-12-30 18:01:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.018 | 0.130\n",
      "2023-12-30 18:01:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.021 | 0.135\n",
      "2023-12-30 18:01:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.030 | 0.131\n",
      "2023-12-30 18:01:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.032 | 0.128\n",
      "2023-12-30 18:01:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.020 | 0.130\n",
      "2023-12-30 18:01:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.024 | 0.131\n",
      "2023-12-30 18:01:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.018 | 0.128\n",
      "2023-12-30 18:01:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.027 | 0.128\n",
      "2023-12-30 18:01:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.028 | 0.130\n",
      "2023-12-30 18:01:51 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.024 | 0.132\n",
      "2023-12-30 18:01:51 - INFO     | Early stopping: no decrease (0.112 vs 0.133); counter: 3 out of 3\n",
      "2023-12-30 18:01:51 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:01:51 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      "100%|██████████| 20/20 [03:21<00:00, 10.08s/it]\n",
      "2023-12-30 18:01:51 - INFO     | Best validation loss: 0.112\n",
      "2023-12-30 18:01:51 - INFO     | Best early stopping index/epoch: 13\n",
      "2023-12-30 18:01:51 - INFO     | Average Loss on test set: 0.120\n",
      "2023-12-30 18:01:53 - INFO     | Weighted Precision: 0.972, Recall: 0.972, F1: 0.972\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▃▅▄▅▅▄▅▄▅▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▄▅▃▄▅▅▄▄▄▄▄▃▂▂▂▂▂▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_validation_loss</td><td>0.11208</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.02443</td></tr><tr><td>step_validation_loss</td><td>0.13153</td></tr><tr><td>test_loss</td><td>0.12021</td></tr><tr><td>weighted_f1</td><td>0.97228</td></tr><tr><td>weighted_precision</td><td>0.97243</td></tr><tr><td>weighted_recall</td><td>0.97229</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-13</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/3fw5tcrd' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/3fw5tcrd</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_175829-3fw5tcrd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1lzchgcg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [32, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_180212-1lzchgcg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/1lzchgcg' target=\"_blank\">feasible-sweep-14</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/1lzchgcg' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/1lzchgcg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [32, 64], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:02:13 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 18:02:13 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 34.320 | 1958.334\n",
      "2023-12-30 18:02:14 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 68.074 | 2.285\n",
      "2023-12-30 18:02:14 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 2.161 | 1.673\n",
      "2023-12-30 18:02:14 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 1.259 | 0.808\n",
      "2023-12-30 18:02:15 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 0.733 | 0.674\n",
      "2023-12-30 18:02:15 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 0.607 | 0.479\n",
      "2023-12-30 18:02:15 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 0.458 | 0.453\n",
      "2023-12-30 18:02:16 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 0.471 | 0.318\n",
      "2023-12-30 18:02:16 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 0.371 | 0.294\n",
      "2023-12-30 18:02:16 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 0.329 | 0.320\n",
      "2023-12-30 18:02:16 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 0.292 | 0.260\n",
      "2023-12-30 18:02:17 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 0.277 | 0.253\n",
      "2023-12-30 18:02:17 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 0.278 | 0.254\n",
      "2023-12-30 18:02:17 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.251 | 0.273\n",
      "2023-12-30 18:02:18 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.267 | 0.224\n",
      "2023-12-30 18:02:18 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.247 | 0.208\n",
      "2023-12-30 18:02:18 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.234 | 0.371\n",
      "2023-12-30 18:02:19 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.228 | 0.214\n",
      "2023-12-30 18:02:19 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.227 | 0.288\n",
      "2023-12-30 18:02:19 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.195 | 0.188\n",
      "2023-12-30 18:02:20 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.190 | 0.198\n",
      "2023-12-30 18:02:20 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.172 | 0.222\n",
      "2023-12-30 18:02:20 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.202 | 0.216\n",
      "2023-12-30 18:02:20 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.191 | 0.195\n",
      "2023-12-30 18:02:21 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.197 | 0.185\n",
      "2023-12-30 18:02:21 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.173 | 0.205\n",
      "2023-12-30 18:02:21 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.162 | 0.174\n",
      "2023-12-30 18:02:22 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.171 | 0.168\n",
      "2023-12-30 18:02:22 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.176 | 0.170\n",
      "2023-12-30 18:02:22 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.165 | 0.169\n",
      "2023-12-30 18:02:23 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.173 | 0.179\n",
      "2023-12-30 18:02:23 - INFO     | Early stopping: loss decreased (inf -> 0.175; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:09<03:09, 10.00s/it]2023-12-30 18:02:23 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 18:02:23 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.193 | 0.169\n",
      "2023-12-30 18:02:24 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.149 | 0.188\n",
      "2023-12-30 18:02:24 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.159 | 0.238\n",
      "2023-12-30 18:02:24 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.150 | 0.169\n",
      "2023-12-30 18:02:25 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.163 | 0.180\n",
      "2023-12-30 18:02:25 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.165 | 0.160\n",
      "2023-12-30 18:02:25 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.178 | 0.160\n",
      "2023-12-30 18:02:26 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.147 | 0.155\n",
      "2023-12-30 18:02:26 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.137 | 0.147\n",
      "2023-12-30 18:02:26 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.120 | 0.179\n",
      "2023-12-30 18:02:27 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.160 | 0.166\n",
      "2023-12-30 18:02:27 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.126 | 0.178\n",
      "2023-12-30 18:02:27 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.137 | 0.159\n",
      "2023-12-30 18:02:28 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.167 | 0.147\n",
      "2023-12-30 18:02:28 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.130 | 0.170\n",
      "2023-12-30 18:02:28 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.122 | 0.144\n",
      "2023-12-30 18:02:28 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.137 | 0.186\n",
      "2023-12-30 18:02:29 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.129 | 0.140\n",
      "2023-12-30 18:02:29 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.134 | 0.146\n",
      "2023-12-30 18:02:29 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.114 | 0.149\n",
      "2023-12-30 18:02:30 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.136 | 0.177\n",
      "2023-12-30 18:02:30 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.137 | 0.146\n",
      "2023-12-30 18:02:30 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.143 | 0.128\n",
      "2023-12-30 18:02:31 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.146 | 0.145\n",
      "2023-12-30 18:02:31 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.132 | 0.147\n",
      "2023-12-30 18:02:31 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.133 | 0.158\n",
      "2023-12-30 18:02:32 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.166 | 0.135\n",
      "2023-12-30 18:02:32 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.138 | 0.138\n",
      "2023-12-30 18:02:32 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.115 | 0.139\n",
      "2023-12-30 18:02:33 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.114 | 0.137\n",
      "2023-12-30 18:02:33 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.123 | 0.142\n",
      "2023-12-30 18:02:33 - INFO     | Early stopping: loss decreased (0.175 -> 0.141; -19.7%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:02, 10.12s/it]2023-12-30 18:02:33 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 18:02:33 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.085 | 0.130\n",
      "2023-12-30 18:02:34 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.128 | 0.140\n",
      "2023-12-30 18:02:34 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.116 | 0.129\n",
      "2023-12-30 18:02:34 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.104 | 0.126\n",
      "2023-12-30 18:02:35 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.106 | 0.185\n",
      "2023-12-30 18:02:35 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.091 | 0.149\n",
      "2023-12-30 18:02:35 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.121 | 0.169\n",
      "2023-12-30 18:02:36 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.135 | 0.122\n",
      "2023-12-30 18:02:36 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.110 | 0.125\n",
      "2023-12-30 18:02:36 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.106 | 0.132\n",
      "2023-12-30 18:02:37 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.101 | 0.122\n",
      "2023-12-30 18:02:37 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.097 | 0.133\n",
      "2023-12-30 18:02:37 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.110 | 0.125\n",
      "2023-12-30 18:02:38 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.093 | 0.133\n",
      "2023-12-30 18:02:38 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.084 | 0.129\n",
      "2023-12-30 18:02:38 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.116 | 0.130\n",
      "2023-12-30 18:02:38 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.112 | 0.118\n",
      "2023-12-30 18:02:39 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.090 | 0.115\n",
      "2023-12-30 18:02:39 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.119 | 0.128\n",
      "2023-12-30 18:02:39 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.113 | 0.109\n",
      "2023-12-30 18:02:40 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.094 | 0.124\n",
      "2023-12-30 18:02:40 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.107 | 0.105\n",
      "2023-12-30 18:02:40 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.096 | 0.113\n",
      "2023-12-30 18:02:41 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.107 | 0.123\n",
      "2023-12-30 18:02:41 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.107 | 0.108\n",
      "2023-12-30 18:02:41 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.101 | 0.124\n",
      "2023-12-30 18:02:42 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.093 | 0.122\n",
      "2023-12-30 18:02:42 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.109 | 0.124\n",
      "2023-12-30 18:02:42 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.093 | 0.113\n",
      "2023-12-30 18:02:42 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.097 | 0.128\n",
      "2023-12-30 18:02:43 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.098 | 0.100\n",
      "2023-12-30 18:02:43 - INFO     | Early stopping: loss decreased (0.141 -> 0.101; -27.8%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:50, 10.02s/it]2023-12-30 18:02:43 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 18:02:43 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.035 | 0.104\n",
      "2023-12-30 18:02:44 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.097 | 0.124\n",
      "2023-12-30 18:02:44 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.071 | 0.104\n",
      "2023-12-30 18:02:44 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.094 | 0.121\n",
      "2023-12-30 18:02:45 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.088 | 0.115\n",
      "2023-12-30 18:02:45 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.060 | 0.126\n",
      "2023-12-30 18:02:45 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.080 | 0.124\n",
      "2023-12-30 18:02:46 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.068 | 0.106\n",
      "2023-12-30 18:02:46 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.109 | 0.108\n",
      "2023-12-30 18:02:46 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.090 | 0.118\n",
      "2023-12-30 18:02:47 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.097 | 0.119\n",
      "2023-12-30 18:02:47 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.078 | 0.123\n",
      "2023-12-30 18:02:47 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.081 | 0.114\n",
      "2023-12-30 18:02:47 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.105 | 0.107\n",
      "2023-12-30 18:02:48 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.083 | 0.118\n",
      "2023-12-30 18:02:48 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.110 | 0.117\n",
      "2023-12-30 18:02:48 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.102 | 0.112\n",
      "2023-12-30 18:02:49 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.088 | 0.114\n",
      "2023-12-30 18:02:49 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.086 | 0.097\n",
      "2023-12-30 18:02:49 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.069 | 0.119\n",
      "2023-12-30 18:02:50 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.088 | 0.101\n",
      "2023-12-30 18:02:50 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.088 | 0.103\n",
      "2023-12-30 18:02:50 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.067 | 0.101\n",
      "2023-12-30 18:02:51 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.064 | 0.116\n",
      "2023-12-30 18:02:51 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.092 | 0.093\n",
      "2023-12-30 18:02:51 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.089 | 0.100\n",
      "2023-12-30 18:02:51 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.089 | 0.100\n",
      "2023-12-30 18:02:52 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.100 | 0.095\n",
      "2023-12-30 18:02:52 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.071 | 0.093\n",
      "2023-12-30 18:02:52 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.098 | 0.108\n",
      "2023-12-30 18:02:53 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.073 | 0.091\n",
      "2023-12-30 18:02:53 - INFO     | Early stopping: no decrease (0.101 vs 0.101); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:39<02:39,  9.97s/it]2023-12-30 18:02:53 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 18:02:53 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.036 | 0.094\n",
      "2023-12-30 18:02:54 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.061 | 0.102\n",
      "2023-12-30 18:02:54 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.089 | 0.116\n",
      "2023-12-30 18:02:54 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.064 | 0.112\n",
      "2023-12-30 18:02:54 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.089 | 0.093\n",
      "2023-12-30 18:02:55 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.062 | 0.087\n",
      "2023-12-30 18:02:55 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.067 | 0.112\n",
      "2023-12-30 18:02:55 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.070 | 0.096\n",
      "2023-12-30 18:02:56 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.069 | 0.107\n",
      "2023-12-30 18:02:56 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.091 | 0.097\n",
      "2023-12-30 18:02:56 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.090 | 0.098\n",
      "2023-12-30 18:02:57 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.074 | 0.104\n",
      "2023-12-30 18:02:57 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.087 | 0.106\n",
      "2023-12-30 18:02:57 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.084 | 0.105\n",
      "2023-12-30 18:02:58 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.068 | 0.099\n",
      "2023-12-30 18:02:58 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.091 | 0.100\n",
      "2023-12-30 18:02:58 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.056 | 0.100\n",
      "2023-12-30 18:02:59 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.064 | 0.103\n",
      "2023-12-30 18:02:59 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.070 | 0.104\n",
      "2023-12-30 18:02:59 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.095 | 0.098\n",
      "2023-12-30 18:03:00 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.057 | 0.095\n",
      "2023-12-30 18:03:00 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.062 | 0.099\n",
      "2023-12-30 18:03:00 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.074 | 0.103\n",
      "2023-12-30 18:03:01 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.075 | 0.089\n",
      "2023-12-30 18:03:01 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.064 | 0.104\n",
      "2023-12-30 18:03:01 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.090 | 0.092\n",
      "2023-12-30 18:03:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.084 | 0.094\n",
      "2023-12-30 18:03:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.067 | 0.090\n",
      "2023-12-30 18:03:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.071 | 0.089\n",
      "2023-12-30 18:03:02 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.069 | 0.098\n",
      "2023-12-30 18:03:03 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.058 | 0.087\n",
      "2023-12-30 18:03:03 - INFO     | Early stopping: loss decreased (0.101 -> 0.092; -9.3%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:50<02:30, 10.01s/it]2023-12-30 18:03:03 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 18:03:03 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.035 | 0.086\n",
      "2023-12-30 18:03:04 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.067 | 0.095\n",
      "2023-12-30 18:03:04 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.064 | 0.089\n",
      "2023-12-30 18:03:04 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.045 | 0.093\n",
      "2023-12-30 18:03:05 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.058 | 0.193\n",
      "2023-12-30 18:03:05 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.060 | 0.104\n",
      "2023-12-30 18:03:05 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.054 | 0.093\n",
      "2023-12-30 18:03:06 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.071 | 0.094\n",
      "2023-12-30 18:03:06 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.072 | 0.092\n",
      "2023-12-30 18:03:06 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.062 | 0.084\n",
      "2023-12-30 18:03:06 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.052 | 0.095\n",
      "2023-12-30 18:03:07 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.083 | 0.103\n",
      "2023-12-30 18:03:07 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.082 | 0.090\n",
      "2023-12-30 18:03:07 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.075 | 0.101\n",
      "2023-12-30 18:03:08 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.056 | 0.096\n",
      "2023-12-30 18:03:08 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.088 | 0.116\n",
      "2023-12-30 18:03:08 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.069 | 0.090\n",
      "2023-12-30 18:03:09 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.077 | 0.103\n",
      "2023-12-30 18:03:09 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.057 | 0.093\n",
      "2023-12-30 18:03:09 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.077 | 0.087\n",
      "2023-12-30 18:03:10 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.069 | 0.091\n",
      "2023-12-30 18:03:10 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.050 | 0.095\n",
      "2023-12-30 18:03:10 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.060 | 0.099\n",
      "2023-12-30 18:03:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.078 | 0.100\n",
      "2023-12-30 18:03:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.058 | 0.089\n",
      "2023-12-30 18:03:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.077 | 0.090\n",
      "2023-12-30 18:03:11 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.064 | 0.093\n",
      "2023-12-30 18:03:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.086 | 0.085\n",
      "2023-12-30 18:03:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.064 | 0.086\n",
      "2023-12-30 18:03:12 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.050 | 0.085\n",
      "2023-12-30 18:03:13 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.045 | 0.085\n",
      "2023-12-30 18:03:13 - INFO     | Early stopping: no decrease (0.092 vs 0.100); counter: 1 out of 3\n",
      " 30%|███       | 6/20 [01:00<02:19,  9.98s/it]2023-12-30 18:03:13 - INFO     | Epoch: 6 | Learning Rate: 0.010\n",
      "2023-12-30 18:03:13 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 336064 examples: 0.012 | 0.100\n",
      "2023-12-30 18:03:14 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 337920 examples: 0.061 | 0.100\n",
      "2023-12-30 18:03:14 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 339776 examples: 0.060 | 0.093\n",
      "2023-12-30 18:03:14 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 341632 examples: 0.073 | 0.083\n",
      "2023-12-30 18:03:14 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 343488 examples: 0.053 | 0.123\n",
      "2023-12-30 18:03:15 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 345344 examples: 0.091 | 0.086\n",
      "2023-12-30 18:03:15 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 347200 examples: 0.046 | 0.107\n",
      "2023-12-30 18:03:15 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 349056 examples: 0.058 | 0.087\n",
      "2023-12-30 18:03:16 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 350912 examples: 0.056 | 0.088\n",
      "2023-12-30 18:03:16 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 352768 examples: 0.049 | 0.094\n",
      "2023-12-30 18:03:16 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 354624 examples: 0.050 | 0.101\n",
      "2023-12-30 18:03:17 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 356480 examples: 0.077 | 0.086\n",
      "2023-12-30 18:03:17 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 358336 examples: 0.076 | 0.087\n",
      "2023-12-30 18:03:17 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 360192 examples: 0.056 | 0.093\n",
      "2023-12-30 18:03:18 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 362048 examples: 0.050 | 0.095\n",
      "2023-12-30 18:03:18 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 363904 examples: 0.065 | 0.087\n",
      "2023-12-30 18:03:18 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 365760 examples: 0.054 | 0.091\n",
      "2023-12-30 18:03:19 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 367616 examples: 0.077 | 0.087\n",
      "2023-12-30 18:03:19 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 369472 examples: 0.084 | 0.086\n",
      "2023-12-30 18:03:19 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 371328 examples: 0.044 | 0.090\n",
      "2023-12-30 18:03:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 373184 examples: 0.063 | 0.093\n",
      "2023-12-30 18:03:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 375040 examples: 0.066 | 0.085\n",
      "2023-12-30 18:03:20 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 376896 examples: 0.059 | 0.082\n",
      "2023-12-30 18:03:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 378752 examples: 0.063 | 0.103\n",
      "2023-12-30 18:03:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 380608 examples: 0.050 | 0.084\n",
      "2023-12-30 18:03:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 382464 examples: 0.056 | 0.081\n",
      "2023-12-30 18:03:21 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 384320 examples: 0.058 | 0.084\n",
      "2023-12-30 18:03:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 386176 examples: 0.053 | 0.092\n",
      "2023-12-30 18:03:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 388032 examples: 0.045 | 0.087\n",
      "2023-12-30 18:03:22 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 389888 examples: 0.048 | 0.081\n",
      "2023-12-30 18:03:23 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 391744 examples: 0.052 | 0.090\n",
      "2023-12-30 18:03:23 - INFO     | Early stopping: no decrease (0.092 vs 0.093); counter: 2 out of 3\n",
      " 35%|███▌      | 7/20 [01:10<02:10, 10.01s/it]2023-12-30 18:03:23 - INFO     | Epoch: 7 | Learning Rate: 0.010\n",
      "2023-12-30 18:03:23 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 392064 examples: 0.010 | 0.088\n",
      "2023-12-30 18:03:24 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 393920 examples: 0.038 | 0.082\n",
      "2023-12-30 18:03:24 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 395776 examples: 0.036 | 0.091\n",
      "2023-12-30 18:03:24 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 397632 examples: 0.063 | 0.091\n",
      "2023-12-30 18:03:25 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 399488 examples: 0.070 | 0.087\n",
      "2023-12-30 18:03:25 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 401344 examples: 0.042 | 0.083\n",
      "2023-12-30 18:03:25 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 403200 examples: 0.046 | 0.078\n",
      "2023-12-30 18:03:26 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 405056 examples: 0.061 | 0.087\n",
      "2023-12-30 18:03:26 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 406912 examples: 0.045 | 0.087\n",
      "2023-12-30 18:03:26 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 408768 examples: 0.072 | 0.103\n",
      "2023-12-30 18:03:26 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 410624 examples: 0.051 | 0.077\n",
      "2023-12-30 18:03:27 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 412480 examples: 0.048 | 0.088\n",
      "2023-12-30 18:03:27 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 414336 examples: 0.057 | 0.100\n",
      "2023-12-30 18:03:27 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 416192 examples: 0.056 | 0.084\n",
      "2023-12-30 18:03:28 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 418048 examples: 0.070 | 0.080\n",
      "2023-12-30 18:03:28 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 419904 examples: 0.052 | 0.083\n",
      "2023-12-30 18:03:28 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 421760 examples: 0.054 | 0.098\n",
      "2023-12-30 18:03:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 423616 examples: 0.077 | 0.089\n",
      "2023-12-30 18:03:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 425472 examples: 0.032 | 0.074\n",
      "2023-12-30 18:03:29 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 427328 examples: 0.043 | 0.084\n",
      "2023-12-30 18:03:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 429184 examples: 0.065 | 0.074\n",
      "2023-12-30 18:03:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 431040 examples: 0.037 | 0.084\n",
      "2023-12-30 18:03:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 432896 examples: 0.060 | 0.092\n",
      "2023-12-30 18:03:30 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 434752 examples: 0.056 | 0.087\n",
      "2023-12-30 18:03:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 436608 examples: 0.054 | 0.079\n",
      "2023-12-30 18:03:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 438464 examples: 0.054 | 0.072\n",
      "2023-12-30 18:03:31 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 440320 examples: 0.044 | 0.079\n",
      "2023-12-30 18:03:32 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 442176 examples: 0.049 | 0.077\n",
      "2023-12-30 18:03:32 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 444032 examples: 0.057 | 0.080\n",
      "2023-12-30 18:03:32 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 445888 examples: 0.061 | 0.091\n",
      "2023-12-30 18:03:33 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 447744 examples: 0.057 | 0.097\n",
      "2023-12-30 18:03:33 - INFO     | Early stopping: no decrease (0.092 vs 0.098); counter: 3 out of 3\n",
      "2023-12-30 18:03:33 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:03:33 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 40%|████      | 8/20 [01:20<01:59,  9.99s/it]2023-12-30 18:03:33 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 18:03:33 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.011 | 0.096\n",
      "2023-12-30 18:03:34 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.034 | 0.083\n",
      "2023-12-30 18:03:34 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.034 | 0.075\n",
      "2023-12-30 18:03:34 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.031 | 0.075\n",
      "2023-12-30 18:03:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.039 | 0.077\n",
      "2023-12-30 18:03:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.033 | 0.080\n",
      "2023-12-30 18:03:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.037 | 0.078\n",
      "2023-12-30 18:03:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.046 | 0.071\n",
      "2023-12-30 18:03:36 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.034 | 0.071\n",
      "2023-12-30 18:03:36 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.039 | 0.071\n",
      "2023-12-30 18:03:36 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.042 | 0.070\n",
      "2023-12-30 18:03:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.047 | 0.069\n",
      "2023-12-30 18:03:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.035 | 0.073\n",
      "2023-12-30 18:03:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.034 | 0.073\n",
      "2023-12-30 18:03:38 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.027 | 0.074\n",
      "2023-12-30 18:03:38 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.032 | 0.074\n",
      "2023-12-30 18:03:38 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.041 | 0.077\n",
      "2023-12-30 18:03:39 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.032 | 0.077\n",
      "2023-12-30 18:03:39 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.038 | 0.069\n",
      "2023-12-30 18:03:39 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.036 | 0.074\n",
      "2023-12-30 18:03:39 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.044 | 0.081\n",
      "2023-12-30 18:03:40 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.033 | 0.075\n",
      "2023-12-30 18:03:40 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.030 | 0.070\n",
      "2023-12-30 18:03:40 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.037 | 0.073\n",
      "2023-12-30 18:03:41 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.034 | 0.071\n",
      "2023-12-30 18:03:41 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.035 | 0.071\n",
      "2023-12-30 18:03:41 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.051 | 0.077\n",
      "2023-12-30 18:03:42 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.045 | 0.074\n",
      "2023-12-30 18:03:42 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.031 | 0.071\n",
      "2023-12-30 18:03:42 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.033 | 0.074\n",
      "2023-12-30 18:03:43 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.038 | 0.071\n",
      "2023-12-30 18:03:43 - INFO     | Early stopping: loss decreased (0.092 -> 0.071; -23.2%). Caching model state.\n",
      " 45%|████▌     | 9/20 [01:29<01:49,  9.96s/it]2023-12-30 18:03:43 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 18:03:43 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.017 | 0.071\n",
      "2023-12-30 18:03:43 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.035 | 0.074\n",
      "2023-12-30 18:03:44 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.030 | 0.074\n",
      "2023-12-30 18:03:44 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.028 | 0.070\n",
      "2023-12-30 18:03:44 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.026 | 0.073\n",
      "2023-12-30 18:03:45 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.033 | 0.078\n",
      "2023-12-30 18:03:45 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.031 | 0.073\n",
      "2023-12-30 18:03:45 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.032 | 0.072\n",
      "2023-12-30 18:03:46 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.033 | 0.085\n",
      "2023-12-30 18:03:46 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.025 | 0.077\n",
      "2023-12-30 18:03:46 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.025 | 0.076\n",
      "2023-12-30 18:03:47 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.031 | 0.071\n",
      "2023-12-30 18:03:47 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.033 | 0.072\n",
      "2023-12-30 18:03:47 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.033 | 0.069\n",
      "2023-12-30 18:03:48 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.036 | 0.071\n",
      "2023-12-30 18:03:48 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.035 | 0.069\n",
      "2023-12-30 18:03:48 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.025 | 0.075\n",
      "2023-12-30 18:03:49 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.034 | 0.072\n",
      "2023-12-30 18:03:49 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.041 | 0.080\n",
      "2023-12-30 18:03:49 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.041 | 0.074\n",
      "2023-12-30 18:03:49 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.039 | 0.078\n",
      "2023-12-30 18:03:50 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.045 | 0.074\n",
      "2023-12-30 18:03:50 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.045 | 0.076\n",
      "2023-12-30 18:03:50 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.036 | 0.068\n",
      "2023-12-30 18:03:51 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.023 | 0.068\n",
      "2023-12-30 18:03:51 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.026 | 0.072\n",
      "2023-12-30 18:03:51 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.025 | 0.074\n",
      "2023-12-30 18:03:52 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.030 | 0.071\n",
      "2023-12-30 18:03:52 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.041 | 0.077\n",
      "2023-12-30 18:03:52 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.038 | 0.071\n",
      "2023-12-30 18:03:53 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.037 | 0.074\n",
      "2023-12-30 18:03:53 - INFO     | Early stopping: no decrease (0.071 vs 0.071); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:39<01:39,  9.94s/it]2023-12-30 18:03:53 - INFO     | Epoch: 10 | Learning Rate: 0.005\n",
      "2023-12-30 18:03:53 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 560064 examples: 0.027 | 0.070\n",
      "2023-12-30 18:03:53 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 561920 examples: 0.023 | 0.069\n",
      "2023-12-30 18:03:54 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 563776 examples: 0.020 | 0.071\n",
      "2023-12-30 18:03:54 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 565632 examples: 0.036 | 0.070\n",
      "2023-12-30 18:03:54 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 567488 examples: 0.027 | 0.071\n",
      "2023-12-30 18:03:55 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 569344 examples: 0.036 | 0.073\n",
      "2023-12-30 18:03:55 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 571200 examples: 0.030 | 0.073\n",
      "2023-12-30 18:03:55 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 573056 examples: 0.023 | 0.068\n",
      "2023-12-30 18:03:56 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 574912 examples: 0.024 | 0.075\n",
      "2023-12-30 18:03:56 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 576768 examples: 0.041 | 0.074\n",
      "2023-12-30 18:03:56 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 578624 examples: 0.021 | 0.072\n",
      "2023-12-30 18:03:57 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 580480 examples: 0.021 | 0.077\n",
      "2023-12-30 18:03:57 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 582336 examples: 0.027 | 0.073\n",
      "2023-12-30 18:03:57 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 584192 examples: 0.020 | 0.074\n",
      "2023-12-30 18:03:58 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 586048 examples: 0.019 | 0.074\n",
      "2023-12-30 18:03:58 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 587904 examples: 0.029 | 0.075\n",
      "2023-12-30 18:03:58 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 589760 examples: 0.035 | 0.079\n",
      "2023-12-30 18:03:59 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 591616 examples: 0.027 | 0.078\n",
      "2023-12-30 18:03:59 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 593472 examples: 0.034 | 0.078\n",
      "2023-12-30 18:03:59 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 595328 examples: 0.040 | 0.074\n",
      "2023-12-30 18:03:59 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 597184 examples: 0.034 | 0.069\n",
      "2023-12-30 18:04:00 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 599040 examples: 0.036 | 0.072\n",
      "2023-12-30 18:04:00 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 600896 examples: 0.038 | 0.070\n",
      "2023-12-30 18:04:00 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 602752 examples: 0.042 | 0.072\n",
      "2023-12-30 18:04:01 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 604608 examples: 0.031 | 0.075\n",
      "2023-12-30 18:04:01 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 606464 examples: 0.036 | 0.071\n",
      "2023-12-30 18:04:01 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 608320 examples: 0.036 | 0.081\n",
      "2023-12-30 18:04:02 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 610176 examples: 0.037 | 0.070\n",
      "2023-12-30 18:04:02 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 612032 examples: 0.031 | 0.068\n",
      "2023-12-30 18:04:02 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 613888 examples: 0.033 | 0.071\n",
      "2023-12-30 18:04:03 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 615744 examples: 0.034 | 0.069\n",
      "2023-12-30 18:04:03 - INFO     | Early stopping: no decrease (0.071 vs 0.073); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:49<01:29,  9.95s/it]2023-12-30 18:04:03 - INFO     | Epoch: 11 | Learning Rate: 0.005\n",
      "2023-12-30 18:04:03 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 616064 examples: 0.045 | 0.070\n",
      "2023-12-30 18:04:03 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 617920 examples: 0.023 | 0.069\n",
      "2023-12-30 18:04:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 619776 examples: 0.022 | 0.067\n",
      "2023-12-30 18:04:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 621632 examples: 0.025 | 0.072\n",
      "2023-12-30 18:04:04 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 623488 examples: 0.019 | 0.070\n",
      "2023-12-30 18:04:05 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 625344 examples: 0.023 | 0.079\n",
      "2023-12-30 18:04:05 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 627200 examples: 0.034 | 0.071\n",
      "2023-12-30 18:04:05 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 629056 examples: 0.027 | 0.080\n",
      "2023-12-30 18:04:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 630912 examples: 0.032 | 0.075\n",
      "2023-12-30 18:04:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 632768 examples: 0.022 | 0.075\n",
      "2023-12-30 18:04:06 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 634624 examples: 0.024 | 0.070\n",
      "2023-12-30 18:04:07 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 636480 examples: 0.015 | 0.078\n",
      "2023-12-30 18:04:07 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 638336 examples: 0.047 | 0.069\n",
      "2023-12-30 18:04:07 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 640192 examples: 0.030 | 0.075\n",
      "2023-12-30 18:04:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 642048 examples: 0.035 | 0.072\n",
      "2023-12-30 18:04:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 643904 examples: 0.023 | 0.073\n",
      "2023-12-30 18:04:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 645760 examples: 0.027 | 0.073\n",
      "2023-12-30 18:04:08 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 647616 examples: 0.028 | 0.071\n",
      "2023-12-30 18:04:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 649472 examples: 0.030 | 0.078\n",
      "2023-12-30 18:04:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 651328 examples: 0.050 | 0.073\n",
      "2023-12-30 18:04:09 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 653184 examples: 0.026 | 0.069\n",
      "2023-12-30 18:04:10 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 655040 examples: 0.025 | 0.071\n",
      "2023-12-30 18:04:10 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 656896 examples: 0.019 | 0.071\n",
      "2023-12-30 18:04:10 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 658752 examples: 0.027 | 0.073\n",
      "2023-12-30 18:04:11 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 660608 examples: 0.037 | 0.068\n",
      "2023-12-30 18:04:11 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 662464 examples: 0.040 | 0.068\n",
      "2023-12-30 18:04:11 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 664320 examples: 0.047 | 0.067\n",
      "2023-12-30 18:04:12 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 666176 examples: 0.032 | 0.070\n",
      "2023-12-30 18:04:12 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 668032 examples: 0.027 | 0.075\n",
      "2023-12-30 18:04:12 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 669888 examples: 0.037 | 0.074\n",
      "2023-12-30 18:04:13 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 671744 examples: 0.033 | 0.075\n",
      "2023-12-30 18:04:13 - INFO     | Early stopping: no decrease (0.071 vs 0.073); counter: 3 out of 3\n",
      "2023-12-30 18:04:13 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:04:13 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 60%|██████    | 12/20 [01:59<01:19, 10.00s/it]2023-12-30 18:04:13 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 18:04:13 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.049 | 0.073\n",
      "2023-12-30 18:04:14 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.017 | 0.072\n",
      "2023-12-30 18:04:14 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.020 | 0.073\n",
      "2023-12-30 18:04:14 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.014 | 0.070\n",
      "2023-12-30 18:04:14 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.025 | 0.071\n",
      "2023-12-30 18:04:15 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.040 | 0.069\n",
      "2023-12-30 18:04:15 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.016 | 0.068\n",
      "2023-12-30 18:04:15 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.032 | 0.069\n",
      "2023-12-30 18:04:16 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.020 | 0.072\n",
      "2023-12-30 18:04:16 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.016 | 0.072\n",
      "2023-12-30 18:04:16 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.020 | 0.068\n",
      "2023-12-30 18:04:17 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.023 | 0.068\n",
      "2023-12-30 18:04:17 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.027 | 0.072\n",
      "2023-12-30 18:04:17 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.017 | 0.069\n",
      "2023-12-30 18:04:18 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.025 | 0.069\n",
      "2023-12-30 18:04:18 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.025 | 0.070\n",
      "2023-12-30 18:04:19 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.025 | 0.070\n",
      "2023-12-30 18:04:19 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.015 | 0.070\n",
      "2023-12-30 18:04:19 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.018 | 0.069\n",
      "2023-12-30 18:04:19 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.024 | 0.069\n",
      "2023-12-30 18:04:20 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.017 | 0.070\n",
      "2023-12-30 18:04:20 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.022 | 0.071\n",
      "2023-12-30 18:04:20 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.032 | 0.067\n",
      "2023-12-30 18:04:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.023 | 0.069\n",
      "2023-12-30 18:04:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.031 | 0.067\n",
      "2023-12-30 18:04:21 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.017 | 0.069\n",
      "2023-12-30 18:04:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.040 | 0.070\n",
      "2023-12-30 18:04:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.015 | 0.068\n",
      "2023-12-30 18:04:22 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.023 | 0.075\n",
      "2023-12-30 18:04:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.020 | 0.068\n",
      "2023-12-30 18:04:23 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.021 | 0.070\n",
      "2023-12-30 18:04:23 - INFO     | Early stopping: no decrease (0.071 vs 0.072); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:10<01:10, 10.10s/it]2023-12-30 18:04:23 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 18:04:23 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.014 | 0.071\n",
      "2023-12-30 18:04:24 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.020 | 0.070\n",
      "2023-12-30 18:04:24 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.009 | 0.072\n",
      "2023-12-30 18:04:24 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.025 | 0.071\n",
      "2023-12-30 18:04:25 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.017 | 0.071\n",
      "2023-12-30 18:04:25 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.022 | 0.072\n",
      "2023-12-30 18:04:25 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.028 | 0.070\n",
      "2023-12-30 18:04:26 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.014 | 0.072\n",
      "2023-12-30 18:04:26 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.021 | 0.069\n",
      "2023-12-30 18:04:26 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.020 | 0.071\n",
      "2023-12-30 18:04:27 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.017 | 0.072\n",
      "2023-12-30 18:04:27 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.032 | 0.073\n",
      "2023-12-30 18:04:27 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.019 | 0.070\n",
      "2023-12-30 18:04:28 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.023 | 0.070\n",
      "2023-12-30 18:04:28 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.032 | 0.068\n",
      "2023-12-30 18:04:28 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.019 | 0.068\n",
      "2023-12-30 18:04:29 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.014 | 0.071\n",
      "2023-12-30 18:04:29 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.026 | 0.069\n",
      "2023-12-30 18:04:29 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.018 | 0.068\n",
      "2023-12-30 18:04:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.022 | 0.070\n",
      "2023-12-30 18:04:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.026 | 0.071\n",
      "2023-12-30 18:04:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.015 | 0.070\n",
      "2023-12-30 18:04:30 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.023 | 0.073\n",
      "2023-12-30 18:04:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.014 | 0.070\n",
      "2023-12-30 18:04:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.015 | 0.068\n",
      "2023-12-30 18:04:31 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.041 | 0.067\n",
      "2023-12-30 18:04:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.023 | 0.066\n",
      "2023-12-30 18:04:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.016 | 0.072\n",
      "2023-12-30 18:04:32 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.021 | 0.071\n",
      "2023-12-30 18:04:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.023 | 0.075\n",
      "2023-12-30 18:04:33 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.024 | 0.070\n",
      "2023-12-30 18:04:33 - INFO     | Early stopping: no decrease (0.071 vs 0.070); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:20<01:00, 10.08s/it]2023-12-30 18:04:33 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 18:04:34 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.002 | 0.070\n",
      "2023-12-30 18:04:34 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.019 | 0.070\n",
      "2023-12-30 18:04:34 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.020 | 0.068\n",
      "2023-12-30 18:04:34 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.011 | 0.073\n",
      "2023-12-30 18:04:35 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.018 | 0.072\n",
      "2023-12-30 18:04:35 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.020 | 0.069\n",
      "2023-12-30 18:04:35 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.016 | 0.068\n",
      "2023-12-30 18:04:36 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.015 | 0.068\n",
      "2023-12-30 18:04:36 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.024 | 0.068\n",
      "2023-12-30 18:04:36 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.032 | 0.070\n",
      "2023-12-30 18:04:37 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.021 | 0.070\n",
      "2023-12-30 18:04:37 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.022 | 0.073\n",
      "2023-12-30 18:04:37 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.019 | 0.074\n",
      "2023-12-30 18:04:38 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.018 | 0.072\n",
      "2023-12-30 18:04:38 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.017 | 0.070\n",
      "2023-12-30 18:04:38 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.015 | 0.072\n",
      "2023-12-30 18:04:38 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.023 | 0.072\n",
      "2023-12-30 18:04:39 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.022 | 0.075\n",
      "2023-12-30 18:04:39 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.012 | 0.072\n",
      "2023-12-30 18:04:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.017 | 0.072\n",
      "2023-12-30 18:04:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.031 | 0.072\n",
      "2023-12-30 18:04:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.018 | 0.072\n",
      "2023-12-30 18:04:40 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.021 | 0.071\n",
      "2023-12-30 18:04:41 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.019 | 0.070\n",
      "2023-12-30 18:04:41 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.033 | 0.070\n",
      "2023-12-30 18:04:41 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.020 | 0.072\n",
      "2023-12-30 18:04:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.012 | 0.070\n",
      "2023-12-30 18:04:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.034 | 0.071\n",
      "2023-12-30 18:04:42 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.014 | 0.072\n",
      "2023-12-30 18:04:43 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.027 | 0.070\n",
      "2023-12-30 18:04:43 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.022 | 0.070\n",
      "2023-12-30 18:04:43 - INFO     | Early stopping: no decrease (0.071 vs 0.070); counter: 3 out of 3\n",
      "2023-12-30 18:04:43 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:04:43 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 75%|███████▌  | 15/20 [02:30<00:50, 10.06s/it]2023-12-30 18:04:43 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:04:44 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.055 | 0.070\n",
      "2023-12-30 18:04:44 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.016 | 0.068\n",
      "2023-12-30 18:04:44 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.021 | 0.068\n",
      "2023-12-30 18:04:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.013 | 0.068\n",
      "2023-12-30 18:04:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.015 | 0.070\n",
      "2023-12-30 18:04:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.017 | 0.069\n",
      "2023-12-30 18:04:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.011 | 0.070\n",
      "2023-12-30 18:04:46 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.018 | 0.070\n",
      "2023-12-30 18:04:46 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.013 | 0.071\n",
      "2023-12-30 18:04:46 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.013 | 0.071\n",
      "2023-12-30 18:04:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.018 | 0.071\n",
      "2023-12-30 18:04:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.022 | 0.070\n",
      "2023-12-30 18:04:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.024 | 0.071\n",
      "2023-12-30 18:04:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.017 | 0.073\n",
      "2023-12-30 18:04:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.020 | 0.072\n",
      "2023-12-30 18:04:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.012 | 0.070\n",
      "2023-12-30 18:04:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.027 | 0.071\n",
      "2023-12-30 18:04:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.026 | 0.070\n",
      "2023-12-30 18:04:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.013 | 0.070\n",
      "2023-12-30 18:04:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.027 | 0.068\n",
      "2023-12-30 18:04:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.015 | 0.069\n",
      "2023-12-30 18:04:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.016 | 0.069\n",
      "2023-12-30 18:04:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.021 | 0.070\n",
      "2023-12-30 18:04:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.018 | 0.070\n",
      "2023-12-30 18:04:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.015 | 0.071\n",
      "2023-12-30 18:04:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.010 | 0.071\n",
      "2023-12-30 18:04:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.015 | 0.069\n",
      "2023-12-30 18:04:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.010 | 0.070\n",
      "2023-12-30 18:04:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.020 | 0.070\n",
      "2023-12-30 18:04:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.019 | 0.068\n",
      "2023-12-30 18:04:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.022 | 0.069\n",
      "2023-12-30 18:04:54 - INFO     | Early stopping: no decrease (0.071 vs 0.068); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:40<00:40, 10.13s/it]2023-12-30 18:04:54 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:04:54 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.001 | 0.068\n",
      "2023-12-30 18:04:54 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.012 | 0.069\n",
      "2023-12-30 18:04:54 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.022 | 0.070\n",
      "2023-12-30 18:04:55 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.015 | 0.068\n",
      "2023-12-30 18:04:55 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.016 | 0.070\n",
      "2023-12-30 18:04:55 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.024 | 0.069\n",
      "2023-12-30 18:04:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.011 | 0.067\n",
      "2023-12-30 18:04:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.013 | 0.068\n",
      "2023-12-30 18:04:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.015 | 0.069\n",
      "2023-12-30 18:04:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.014 | 0.069\n",
      "2023-12-30 18:04:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.018 | 0.071\n",
      "2023-12-30 18:04:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.011 | 0.070\n",
      "2023-12-30 18:04:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.013 | 0.072\n",
      "2023-12-30 18:04:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.011 | 0.072\n",
      "2023-12-30 18:04:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.017 | 0.070\n",
      "2023-12-30 18:04:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.015 | 0.072\n",
      "2023-12-30 18:04:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.030 | 0.071\n",
      "2023-12-30 18:04:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.023 | 0.072\n",
      "2023-12-30 18:04:59 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.041 | 0.069\n",
      "2023-12-30 18:05:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.018 | 0.074\n",
      "2023-12-30 18:05:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.011 | 0.072\n",
      "2023-12-30 18:05:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.020 | 0.072\n",
      "2023-12-30 18:05:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.017 | 0.071\n",
      "2023-12-30 18:05:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.019 | 0.071\n",
      "2023-12-30 18:05:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.013 | 0.071\n",
      "2023-12-30 18:05:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.011 | 0.070\n",
      "2023-12-30 18:05:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.018 | 0.069\n",
      "2023-12-30 18:05:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.014 | 0.070\n",
      "2023-12-30 18:05:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.014 | 0.070\n",
      "2023-12-30 18:05:03 - INFO     | Early stopping: no decrease (0.071 vs 0.071); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:50<00:30, 10.04s/it]2023-12-30 18:05:03 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:05:04 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.023 | 0.071\n",
      "2023-12-30 18:05:04 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.012 | 0.071\n",
      "2023-12-30 18:05:04 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.011 | 0.071\n",
      "2023-12-30 18:05:05 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.013 | 0.071\n",
      "2023-12-30 18:05:05 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.016 | 0.070\n",
      "2023-12-30 18:05:05 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.013 | 0.071\n",
      "2023-12-30 18:05:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.021 | 0.074\n",
      "2023-12-30 18:05:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.020 | 0.071\n",
      "2023-12-30 18:05:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.009 | 0.071\n",
      "2023-12-30 18:05:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.018 | 0.071\n",
      "2023-12-30 18:05:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.010 | 0.071\n",
      "2023-12-30 18:05:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.013 | 0.071\n",
      "2023-12-30 18:05:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.017 | 0.072\n",
      "2023-12-30 18:05:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.022 | 0.071\n",
      "2023-12-30 18:05:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.013 | 0.071\n",
      "2023-12-30 18:05:09 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.021 | 0.072\n",
      "2023-12-30 18:05:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.014 | 0.072\n",
      "2023-12-30 18:05:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.016 | 0.072\n",
      "2023-12-30 18:05:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.021 | 0.070\n",
      "2023-12-30 18:05:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.020 | 0.070\n",
      "2023-12-30 18:05:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.019 | 0.071\n",
      "2023-12-30 18:05:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.017 | 0.070\n",
      "2023-12-30 18:05:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.017 | 0.072\n",
      "2023-12-30 18:05:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.023 | 0.072\n",
      "2023-12-30 18:05:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.017 | 0.072\n",
      "2023-12-30 18:05:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.015 | 0.074\n",
      "2023-12-30 18:05:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.018 | 0.073\n",
      "2023-12-30 18:05:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.017 | 0.074\n",
      "2023-12-30 18:05:14 - INFO     | Early stopping: no decrease (0.071 vs 0.074); counter: 3 out of 3\n",
      "2023-12-30 18:05:14 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:05:14 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 90%|█████████ | 18/20 [03:00<00:20, 10.09s/it]2023-12-30 18:05:14 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 18:05:14 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.001 | 0.074\n",
      "2023-12-30 18:05:14 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.010 | 0.073\n",
      "2023-12-30 18:05:15 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.014 | 0.074\n",
      "2023-12-30 18:05:15 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.017 | 0.073\n",
      "2023-12-30 18:05:15 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.007 | 0.074\n",
      "2023-12-30 18:05:15 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.013 | 0.074\n",
      "2023-12-30 18:05:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.010 | 0.072\n",
      "2023-12-30 18:05:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.021 | 0.072\n",
      "2023-12-30 18:05:16 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.010 | 0.072\n",
      "2023-12-30 18:05:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.013 | 0.072\n",
      "2023-12-30 18:05:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:17 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.015 | 0.071\n",
      "2023-12-30 18:05:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.012 | 0.071\n",
      "2023-12-30 18:05:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.011 | 0.071\n",
      "2023-12-30 18:05:18 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.023 | 0.071\n",
      "2023-12-30 18:05:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.017 | 0.071\n",
      "2023-12-30 18:05:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.021 | 0.071\n",
      "2023-12-30 18:05:19 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.018 | 0.072\n",
      "2023-12-30 18:05:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.014 | 0.072\n",
      "2023-12-30 18:05:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.010 | 0.071\n",
      "2023-12-30 18:05:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.011 | 0.072\n",
      "2023-12-30 18:05:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.016 | 0.071\n",
      "2023-12-30 18:05:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.010 | 0.071\n",
      "2023-12-30 18:05:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.023 | 0.072\n",
      "2023-12-30 18:05:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.023 | 0.072\n",
      "2023-12-30 18:05:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.030 | 0.072\n",
      "2023-12-30 18:05:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.017 | 0.072\n",
      "2023-12-30 18:05:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.015 | 0.071\n",
      "2023-12-30 18:05:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.013 | 0.072\n",
      "2023-12-30 18:05:23 - INFO     | Early stopping: no decrease (0.071 vs 0.072); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [03:10<00:10, 10.03s/it]2023-12-30 18:05:23 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 18:05:24 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.000 | 0.072\n",
      "2023-12-30 18:05:24 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:24 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.012 | 0.071\n",
      "2023-12-30 18:05:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.011 | 0.072\n",
      "2023-12-30 18:05:25 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.010 | 0.071\n",
      "2023-12-30 18:05:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.019 | 0.071\n",
      "2023-12-30 18:05:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.019 | 0.071\n",
      "2023-12-30 18:05:26 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.014 | 0.071\n",
      "2023-12-30 18:05:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.016 | 0.072\n",
      "2023-12-30 18:05:27 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.011 | 0.071\n",
      "2023-12-30 18:05:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.020 | 0.072\n",
      "2023-12-30 18:05:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.013 | 0.072\n",
      "2023-12-30 18:05:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.010 | 0.073\n",
      "2023-12-30 18:05:28 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.025 | 0.073\n",
      "2023-12-30 18:05:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.013 | 0.073\n",
      "2023-12-30 18:05:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.016 | 0.072\n",
      "2023-12-30 18:05:29 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.008 | 0.072\n",
      "2023-12-30 18:05:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.011 | 0.073\n",
      "2023-12-30 18:05:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.012 | 0.073\n",
      "2023-12-30 18:05:30 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.016 | 0.073\n",
      "2023-12-30 18:05:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.021 | 0.072\n",
      "2023-12-30 18:05:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.013 | 0.073\n",
      "2023-12-30 18:05:31 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.009 | 0.072\n",
      "2023-12-30 18:05:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.008 | 0.072\n",
      "2023-12-30 18:05:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.013 | 0.072\n",
      "2023-12-30 18:05:32 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.024 | 0.073\n",
      "2023-12-30 18:05:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.015 | 0.072\n",
      "2023-12-30 18:05:33 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.024 | 0.072\n",
      "2023-12-30 18:05:33 - INFO     | Early stopping: no decrease (0.071 vs 0.072); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:20<00:00, 10.02s/it]\n",
      "2023-12-30 18:05:33 - INFO     | Best validation loss: 0.071\n",
      "2023-12-30 18:05:33 - INFO     | Best early stopping index/epoch: 8\n",
      "2023-12-30 18:05:34 - INFO     | Average Loss on test set: 0.064\n",
      "2023-12-30 18:05:36 - INFO     | Weighted Precision: 0.982, Recall: 0.982, F1: 0.982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>████████▄▄▄▄▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▄▄▃▂▂▂▂▂▅▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_validation_loss</td><td>0.07067</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.02438</td></tr><tr><td>step_validation_loss</td><td>0.07152</td></tr><tr><td>test_loss</td><td>0.06443</td></tr><tr><td>weighted_f1</td><td>0.98171</td></tr><tr><td>weighted_precision</td><td>0.98175</td></tr><tr><td>weighted_recall</td><td>0.98171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-14</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/1lzchgcg' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/1lzchgcg</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_180212-1lzchgcg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2y6sm2wd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [32, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_180546-2y6sm2wd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2y6sm2wd' target=\"_blank\">playful-sweep-15</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2y6sm2wd' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2y6sm2wd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [32, 64], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:05:47 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 18:05:47 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 8.997 | 270.698\n",
      "2023-12-30 18:05:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 12.507 | 1.496\n",
      "2023-12-30 18:05:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 1.049 | 0.667\n",
      "2023-12-30 18:05:48 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 0.589 | 0.529\n",
      "2023-12-30 18:05:49 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 0.460 | 0.469\n",
      "2023-12-30 18:05:49 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 0.419 | 0.424\n",
      "2023-12-30 18:05:49 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 0.416 | 0.380\n",
      "2023-12-30 18:05:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.417 | 0.353\n",
      "2023-12-30 18:05:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.340 | 0.313\n",
      "2023-12-30 18:05:50 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.340 | 0.302\n",
      "2023-12-30 18:05:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.325 | 0.290\n",
      "2023-12-30 18:05:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.344 | 0.327\n",
      "2023-12-30 18:05:51 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.329 | 0.287\n",
      "2023-12-30 18:05:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.323 | 0.293\n",
      "2023-12-30 18:05:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.296 | 0.252\n",
      "2023-12-30 18:05:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.276 | 0.237\n",
      "2023-12-30 18:05:52 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.231 | 0.265\n",
      "2023-12-30 18:05:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.268 | 0.239\n",
      "2023-12-30 18:05:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.266 | 0.230\n",
      "2023-12-30 18:05:53 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.242 | 0.241\n",
      "2023-12-30 18:05:54 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.265 | 0.238\n",
      "2023-12-30 18:05:54 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.262 | 0.213\n",
      "2023-12-30 18:05:54 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.192 | 0.211\n",
      "2023-12-30 18:05:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.204 | 0.317\n",
      "2023-12-30 18:05:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.225 | 0.234\n",
      "2023-12-30 18:05:55 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.263 | 0.311\n",
      "2023-12-30 18:05:56 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.225 | 0.237\n",
      "2023-12-30 18:05:56 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.259 | 0.240\n",
      "2023-12-30 18:05:56 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.234 | 0.234\n",
      "2023-12-30 18:05:57 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.232 | 0.234\n",
      "2023-12-30 18:05:57 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.196 | 0.189\n",
      "2023-12-30 18:05:57 - INFO     | Early stopping: loss decreased (inf -> 0.181; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:12, 10.13s/it]2023-12-30 18:05:57 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 18:05:57 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.123 | 0.189\n",
      "2023-12-30 18:05:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.201 | 0.202\n",
      "2023-12-30 18:05:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.194 | 0.235\n",
      "2023-12-30 18:05:58 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.178 | 0.206\n",
      "2023-12-30 18:05:59 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.169 | 0.206\n",
      "2023-12-30 18:05:59 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.160 | 0.252\n",
      "2023-12-30 18:05:59 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.189 | 0.158\n",
      "2023-12-30 18:06:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.172 | 0.155\n",
      "2023-12-30 18:06:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.159 | 0.172\n",
      "2023-12-30 18:06:00 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.155 | 0.187\n",
      "2023-12-30 18:06:01 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.202 | 0.166\n",
      "2023-12-30 18:06:01 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.171 | 0.169\n",
      "2023-12-30 18:06:01 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.125 | 0.160\n",
      "2023-12-30 18:06:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.173 | 0.169\n",
      "2023-12-30 18:06:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.154 | 0.182\n",
      "2023-12-30 18:06:02 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.149 | 0.157\n",
      "2023-12-30 18:06:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.135 | 0.151\n",
      "2023-12-30 18:06:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.162 | 0.204\n",
      "2023-12-30 18:06:03 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.179 | 0.178\n",
      "2023-12-30 18:06:04 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.177 | 0.227\n",
      "2023-12-30 18:06:04 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.179 | 0.179\n",
      "2023-12-30 18:06:04 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.184 | 0.166\n",
      "2023-12-30 18:06:05 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.149 | 0.198\n",
      "2023-12-30 18:06:05 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.192 | 0.179\n",
      "2023-12-30 18:06:05 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.180 | 0.177\n",
      "2023-12-30 18:06:06 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.170 | 0.173\n",
      "2023-12-30 18:06:06 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.168 | 0.158\n",
      "2023-12-30 18:06:06 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.175 | 0.172\n",
      "2023-12-30 18:06:06 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.154 | 0.149\n",
      "2023-12-30 18:06:07 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.153 | 0.136\n",
      "2023-12-30 18:06:07 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.123 | 0.163\n",
      "2023-12-30 18:06:07 - INFO     | Early stopping: loss decreased (0.181 -> 0.152; -15.8%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:02, 10.15s/it]2023-12-30 18:06:07 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 18:06:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.070 | 0.159\n",
      "2023-12-30 18:06:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.120 | 0.175\n",
      "2023-12-30 18:06:08 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.124 | 0.158\n",
      "2023-12-30 18:06:09 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.149 | 0.160\n",
      "2023-12-30 18:06:09 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.115 | 0.161\n",
      "2023-12-30 18:06:09 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.109 | 0.133\n",
      "2023-12-30 18:06:10 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.135 | 0.147\n",
      "2023-12-30 18:06:10 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.142 | 0.154\n",
      "2023-12-30 18:06:10 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.155 | 0.244\n",
      "2023-12-30 18:06:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.162 | 0.180\n",
      "2023-12-30 18:06:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.156 | 0.188\n",
      "2023-12-30 18:06:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.176 | 0.162\n",
      "2023-12-30 18:06:11 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.163 | 0.157\n",
      "2023-12-30 18:06:12 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.150 | 0.134\n",
      "2023-12-30 18:06:12 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.139 | 0.129\n",
      "2023-12-30 18:06:12 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.097 | 0.146\n",
      "2023-12-30 18:06:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.134 | 0.132\n",
      "2023-12-30 18:06:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.127 | 0.157\n",
      "2023-12-30 18:06:13 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.179 | 0.163\n",
      "2023-12-30 18:06:14 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.161 | 0.199\n",
      "2023-12-30 18:06:14 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.174 | 0.193\n",
      "2023-12-30 18:06:14 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.158 | 0.135\n",
      "2023-12-30 18:06:15 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.151 | 0.136\n",
      "2023-12-30 18:06:15 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.109 | 0.161\n",
      "2023-12-30 18:06:15 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.138 | 0.147\n",
      "2023-12-30 18:06:16 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.154 | 0.141\n",
      "2023-12-30 18:06:16 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.144 | 0.166\n",
      "2023-12-30 18:06:16 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.192 | 0.167\n",
      "2023-12-30 18:06:17 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.143 | 0.154\n",
      "2023-12-30 18:06:17 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.142 | 0.159\n",
      "2023-12-30 18:06:17 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.154 | 0.130\n",
      "2023-12-30 18:06:17 - INFO     | Early stopping: loss decreased (0.152 -> 0.124; -18.8%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:52, 10.15s/it]2023-12-30 18:06:17 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 18:06:18 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.123 | 0.119\n",
      "2023-12-30 18:06:18 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.134 | 0.240\n",
      "2023-12-30 18:06:18 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.146 | 0.151\n",
      "2023-12-30 18:06:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.115 | 0.169\n",
      "2023-12-30 18:06:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.112 | 0.145\n",
      "2023-12-30 18:06:19 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.134 | 0.175\n",
      "2023-12-30 18:06:20 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.157 | 0.135\n",
      "2023-12-30 18:06:20 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.133 | 0.136\n",
      "2023-12-30 18:06:20 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.159 | 0.145\n",
      "2023-12-30 18:06:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.124 | 0.135\n",
      "2023-12-30 18:06:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.154 | 0.167\n",
      "2023-12-30 18:06:21 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.139 | 0.136\n",
      "2023-12-30 18:06:22 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.171 | 0.160\n",
      "2023-12-30 18:06:22 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.128 | 0.169\n",
      "2023-12-30 18:06:23 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.167 | 0.196\n",
      "2023-12-30 18:06:23 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.177 | 0.168\n",
      "2023-12-30 18:06:23 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.144 | 0.139\n",
      "2023-12-30 18:06:24 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.193 | 0.154\n",
      "2023-12-30 18:06:24 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.168 | 0.139\n",
      "2023-12-30 18:06:24 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.096 | 0.132\n",
      "2023-12-30 18:06:25 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.133 | 0.128\n",
      "2023-12-30 18:06:25 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.145 | 0.147\n",
      "2023-12-30 18:06:25 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.107 | 0.138\n",
      "2023-12-30 18:06:26 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.147 | 0.206\n",
      "2023-12-30 18:06:26 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.169 | 0.153\n",
      "2023-12-30 18:06:26 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.136 | 0.142\n",
      "2023-12-30 18:06:27 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.132 | 0.161\n",
      "2023-12-30 18:06:27 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.156 | 0.159\n",
      "2023-12-30 18:06:27 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.170 | 0.140\n",
      "2023-12-30 18:06:27 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.095 | 0.165\n",
      "2023-12-30 18:06:28 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.159 | 0.144\n",
      "2023-12-30 18:06:28 - INFO     | Early stopping: no decrease (0.124 vs 0.138); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:41<02:45, 10.33s/it]2023-12-30 18:06:28 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 18:06:28 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.359 | 0.140\n",
      "2023-12-30 18:06:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.135 | 0.138\n",
      "2023-12-30 18:06:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.100 | 0.124\n",
      "2023-12-30 18:06:29 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.105 | 0.152\n",
      "2023-12-30 18:06:30 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.138 | 0.134\n",
      "2023-12-30 18:06:30 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.093 | 0.149\n",
      "2023-12-30 18:06:30 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.163 | 0.157\n",
      "2023-12-30 18:06:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.137 | 0.161\n",
      "2023-12-30 18:06:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.126 | 0.130\n",
      "2023-12-30 18:06:31 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.088 | 0.147\n",
      "2023-12-30 18:06:32 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.139 | 0.140\n",
      "2023-12-30 18:06:32 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.144 | 0.135\n",
      "2023-12-30 18:06:32 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.164 | 0.136\n",
      "2023-12-30 18:06:33 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.126 | 0.124\n",
      "2023-12-30 18:06:33 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.152 | 0.172\n",
      "2023-12-30 18:06:33 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.121 | 0.134\n",
      "2023-12-30 18:06:34 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.162 | 0.153\n",
      "2023-12-30 18:06:34 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.132 | 0.166\n",
      "2023-12-30 18:06:34 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.152 | 0.214\n",
      "2023-12-30 18:06:34 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.128 | 0.135\n",
      "2023-12-30 18:06:35 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.129 | 0.153\n",
      "2023-12-30 18:06:35 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.133 | 0.116\n",
      "2023-12-30 18:06:35 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.113 | 0.152\n",
      "2023-12-30 18:06:36 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.141 | 0.132\n",
      "2023-12-30 18:06:36 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.142 | 0.175\n",
      "2023-12-30 18:06:36 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.143 | 0.172\n",
      "2023-12-30 18:06:37 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.119 | 0.142\n",
      "2023-12-30 18:06:37 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.158 | 0.162\n",
      "2023-12-30 18:06:37 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.140 | 0.137\n",
      "2023-12-30 18:06:38 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.149 | 0.136\n",
      "2023-12-30 18:06:38 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.115 | 0.114\n",
      "2023-12-30 18:06:38 - INFO     | Early stopping: no decrease (0.124 vs 0.134); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:51<02:33, 10.26s/it]2023-12-30 18:06:38 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 18:06:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.094 | 0.141\n",
      "2023-12-30 18:06:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.097 | 0.114\n",
      "2023-12-30 18:06:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.117 | 0.125\n",
      "2023-12-30 18:06:39 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.128 | 0.127\n",
      "2023-12-30 18:06:40 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.088 | 0.140\n",
      "2023-12-30 18:06:40 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.139 | 0.132\n",
      "2023-12-30 18:06:40 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.120 | 0.149\n",
      "2023-12-30 18:06:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.115 | 0.147\n",
      "2023-12-30 18:06:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.154 | 0.195\n",
      "2023-12-30 18:06:41 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.149 | 0.156\n",
      "2023-12-30 18:06:42 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.124 | 0.139\n",
      "2023-12-30 18:06:42 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.132 | 0.166\n",
      "2023-12-30 18:06:42 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.122 | 0.151\n",
      "2023-12-30 18:06:43 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.140 | 0.147\n",
      "2023-12-30 18:06:43 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.132 | 0.159\n",
      "2023-12-30 18:06:43 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.117 | 0.184\n",
      "2023-12-30 18:06:44 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.142 | 0.156\n",
      "2023-12-30 18:06:44 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.145 | 0.149\n",
      "2023-12-30 18:06:44 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.138 | 0.141\n",
      "2023-12-30 18:06:44 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.113 | 0.129\n",
      "2023-12-30 18:06:45 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.104 | 0.180\n",
      "2023-12-30 18:06:45 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.166 | 0.171\n",
      "2023-12-30 18:06:45 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.118 | 0.193\n",
      "2023-12-30 18:06:46 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.176 | 0.155\n",
      "2023-12-30 18:06:46 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.152 | 0.197\n",
      "2023-12-30 18:06:46 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.134 | 0.142\n",
      "2023-12-30 18:06:47 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.115 | 0.181\n",
      "2023-12-30 18:06:47 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.141 | 0.144\n",
      "2023-12-30 18:06:47 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.161 | 0.170\n",
      "2023-12-30 18:06:48 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.177 | 0.185\n",
      "2023-12-30 18:06:48 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.166 | 0.145\n",
      "2023-12-30 18:06:48 - INFO     | Early stopping: no decrease (0.124 vs 0.133); counter: 3 out of 3\n",
      "2023-12-30 18:06:48 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:06:48 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 30%|███       | 6/20 [01:01<02:22, 10.19s/it]2023-12-30 18:06:48 - INFO     | Epoch: 6 | Learning Rate: 0.003\n",
      "2023-12-30 18:06:49 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 336064 examples: 0.099 | 0.145\n",
      "2023-12-30 18:06:49 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 337920 examples: 0.095 | 0.124\n",
      "2023-12-30 18:06:49 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 339776 examples: 0.097 | 0.111\n",
      "2023-12-30 18:06:50 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 341632 examples: 0.097 | 0.133\n",
      "2023-12-30 18:06:50 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 343488 examples: 0.093 | 0.127\n",
      "2023-12-30 18:06:50 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 345344 examples: 0.061 | 0.117\n",
      "2023-12-30 18:06:50 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 347200 examples: 0.083 | 0.130\n",
      "2023-12-30 18:06:51 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 349056 examples: 0.076 | 0.115\n",
      "2023-12-30 18:06:51 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 350912 examples: 0.084 | 0.111\n",
      "2023-12-30 18:06:51 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 352768 examples: 0.078 | 0.124\n",
      "2023-12-30 18:06:52 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 354624 examples: 0.098 | 0.124\n",
      "2023-12-30 18:06:52 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 356480 examples: 0.077 | 0.138\n",
      "2023-12-30 18:06:52 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 358336 examples: 0.115 | 0.119\n",
      "2023-12-30 18:06:53 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 360192 examples: 0.101 | 0.123\n",
      "2023-12-30 18:06:53 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 362048 examples: 0.118 | 0.115\n",
      "2023-12-30 18:06:53 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 363904 examples: 0.067 | 0.111\n",
      "2023-12-30 18:06:54 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 365760 examples: 0.094 | 0.107\n",
      "2023-12-30 18:06:54 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 367616 examples: 0.124 | 0.111\n",
      "2023-12-30 18:06:54 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 369472 examples: 0.081 | 0.133\n",
      "2023-12-30 18:06:55 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 371328 examples: 0.116 | 0.124\n",
      "2023-12-30 18:06:55 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 373184 examples: 0.125 | 0.107\n",
      "2023-12-30 18:06:55 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 375040 examples: 0.076 | 0.117\n",
      "2023-12-30 18:06:55 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 376896 examples: 0.093 | 0.115\n",
      "2023-12-30 18:06:56 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 378752 examples: 0.112 | 0.125\n",
      "2023-12-30 18:06:56 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 380608 examples: 0.083 | 0.105\n",
      "2023-12-30 18:06:56 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 382464 examples: 0.094 | 0.130\n",
      "2023-12-30 18:06:57 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 384320 examples: 0.092 | 0.134\n",
      "2023-12-30 18:06:57 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 386176 examples: 0.128 | 0.118\n",
      "2023-12-30 18:06:57 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 388032 examples: 0.080 | 0.125\n",
      "2023-12-30 18:06:58 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 389888 examples: 0.080 | 0.111\n",
      "2023-12-30 18:06:58 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 391744 examples: 0.104 | 0.131\n",
      "2023-12-30 18:06:58 - INFO     | Early stopping: no decrease (0.124 vs 0.163); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:11<02:11, 10.13s/it]2023-12-30 18:06:58 - INFO     | Epoch: 7 | Learning Rate: 0.003\n",
      "2023-12-30 18:06:59 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 392064 examples: 0.082 | 0.157\n",
      "2023-12-30 18:06:59 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 393920 examples: 0.080 | 0.133\n",
      "2023-12-30 18:06:59 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 395776 examples: 0.086 | 0.116\n",
      "2023-12-30 18:07:00 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 397632 examples: 0.083 | 0.113\n",
      "2023-12-30 18:07:00 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 399488 examples: 0.074 | 0.120\n",
      "2023-12-30 18:07:00 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 401344 examples: 0.076 | 0.102\n",
      "2023-12-30 18:07:01 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 403200 examples: 0.086 | 0.150\n",
      "2023-12-30 18:07:01 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 405056 examples: 0.114 | 0.119\n",
      "2023-12-30 18:07:01 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 406912 examples: 0.058 | 0.132\n",
      "2023-12-30 18:07:01 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 408768 examples: 0.074 | 0.103\n",
      "2023-12-30 18:07:02 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 410624 examples: 0.077 | 0.112\n",
      "2023-12-30 18:07:02 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 412480 examples: 0.076 | 0.118\n",
      "2023-12-30 18:07:02 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 414336 examples: 0.095 | 0.112\n",
      "2023-12-30 18:07:03 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 416192 examples: 0.075 | 0.107\n",
      "2023-12-30 18:07:03 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 418048 examples: 0.067 | 0.119\n",
      "2023-12-30 18:07:03 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 419904 examples: 0.076 | 0.116\n",
      "2023-12-30 18:07:04 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 421760 examples: 0.082 | 0.129\n",
      "2023-12-30 18:07:04 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 423616 examples: 0.101 | 0.132\n",
      "2023-12-30 18:07:04 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 425472 examples: 0.076 | 0.112\n",
      "2023-12-30 18:07:05 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 427328 examples: 0.090 | 0.105\n",
      "2023-12-30 18:07:05 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 429184 examples: 0.075 | 0.097\n",
      "2023-12-30 18:07:05 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 431040 examples: 0.077 | 0.103\n",
      "2023-12-30 18:07:06 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 432896 examples: 0.070 | 0.104\n",
      "2023-12-30 18:07:06 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 434752 examples: 0.053 | 0.093\n",
      "2023-12-30 18:07:06 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 436608 examples: 0.088 | 0.116\n",
      "2023-12-30 18:07:07 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 438464 examples: 0.081 | 0.111\n",
      "2023-12-30 18:07:07 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 440320 examples: 0.097 | 0.136\n",
      "2023-12-30 18:07:07 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 442176 examples: 0.095 | 0.108\n",
      "2023-12-30 18:07:08 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 444032 examples: 0.058 | 0.125\n",
      "2023-12-30 18:07:08 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 445888 examples: 0.110 | 0.115\n",
      "2023-12-30 18:07:08 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 447744 examples: 0.075 | 0.101\n",
      "2023-12-30 18:07:09 - INFO     | Early stopping: loss decreased (0.124 -> 0.099; -19.7%). Caching model state.\n",
      " 40%|████      | 8/20 [01:21<02:03, 10.27s/it]2023-12-30 18:07:09 - INFO     | Epoch: 8 | Learning Rate: 0.003\n",
      "2023-12-30 18:07:09 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 448064 examples: 0.045 | 0.098\n",
      "2023-12-30 18:07:10 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 449920 examples: 0.034 | 0.105\n",
      "2023-12-30 18:07:10 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 451776 examples: 0.072 | 0.123\n",
      "2023-12-30 18:07:10 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 453632 examples: 0.071 | 0.096\n",
      "2023-12-30 18:07:10 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 455488 examples: 0.050 | 0.120\n",
      "2023-12-30 18:07:11 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 457344 examples: 0.055 | 0.107\n",
      "2023-12-30 18:07:11 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 459200 examples: 0.061 | 0.118\n",
      "2023-12-30 18:07:11 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 461056 examples: 0.088 | 0.115\n",
      "2023-12-30 18:07:12 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 462912 examples: 0.089 | 0.119\n",
      "2023-12-30 18:07:12 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 464768 examples: 0.061 | 0.097\n",
      "2023-12-30 18:07:12 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 466624 examples: 0.051 | 0.100\n",
      "2023-12-30 18:07:13 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 468480 examples: 0.065 | 0.098\n",
      "2023-12-30 18:07:13 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 470336 examples: 0.076 | 0.100\n",
      "2023-12-30 18:07:13 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 472192 examples: 0.057 | 0.113\n",
      "2023-12-30 18:07:14 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 474048 examples: 0.068 | 0.113\n",
      "2023-12-30 18:07:14 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 475904 examples: 0.082 | 0.111\n",
      "2023-12-30 18:07:14 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 477760 examples: 0.088 | 0.110\n",
      "2023-12-30 18:07:15 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 479616 examples: 0.067 | 0.112\n",
      "2023-12-30 18:07:15 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 481472 examples: 0.093 | 0.140\n",
      "2023-12-30 18:07:15 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 483328 examples: 0.052 | 0.097\n",
      "2023-12-30 18:07:16 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 485184 examples: 0.082 | 0.122\n",
      "2023-12-30 18:07:16 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 487040 examples: 0.082 | 0.110\n",
      "2023-12-30 18:07:16 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 488896 examples: 0.084 | 0.109\n",
      "2023-12-30 18:07:17 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 490752 examples: 0.070 | 0.127\n",
      "2023-12-30 18:07:17 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 492608 examples: 0.082 | 0.122\n",
      "2023-12-30 18:07:17 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 494464 examples: 0.062 | 0.136\n",
      "2023-12-30 18:07:18 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 496320 examples: 0.100 | 0.124\n",
      "2023-12-30 18:07:18 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 498176 examples: 0.098 | 0.183\n",
      "2023-12-30 18:07:18 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 500032 examples: 0.101 | 0.129\n",
      "2023-12-30 18:07:19 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 501888 examples: 0.126 | 0.123\n",
      "2023-12-30 18:07:19 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 503744 examples: 0.084 | 0.116\n",
      "2023-12-30 18:07:19 - INFO     | Early stopping: no decrease (0.099 vs 0.109); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:32<01:52, 10.27s/it]2023-12-30 18:07:19 - INFO     | Epoch: 9 | Learning Rate: 0.003\n",
      "2023-12-30 18:07:19 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 504064 examples: 0.182 | 0.107\n",
      "2023-12-30 18:07:20 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 505920 examples: 0.045 | 0.098\n",
      "2023-12-30 18:07:20 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 507776 examples: 0.051 | 0.103\n",
      "2023-12-30 18:07:20 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 509632 examples: 0.052 | 0.118\n",
      "2023-12-30 18:07:21 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 511488 examples: 0.073 | 0.094\n",
      "2023-12-30 18:07:21 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 513344 examples: 0.042 | 0.100\n",
      "2023-12-30 18:07:21 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 515200 examples: 0.054 | 0.097\n",
      "2023-12-30 18:07:22 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 517056 examples: 0.057 | 0.109\n",
      "2023-12-30 18:07:22 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 518912 examples: 0.098 | 0.108\n",
      "2023-12-30 18:07:22 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 520768 examples: 0.060 | 0.136\n",
      "2023-12-30 18:07:23 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 522624 examples: 0.054 | 0.123\n",
      "2023-12-30 18:07:23 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 524480 examples: 0.077 | 0.132\n",
      "2023-12-30 18:07:23 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 526336 examples: 0.078 | 0.121\n",
      "2023-12-30 18:07:24 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 528192 examples: 0.069 | 0.110\n",
      "2023-12-30 18:07:24 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 530048 examples: 0.063 | 0.110\n",
      "2023-12-30 18:07:24 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 531904 examples: 0.072 | 0.122\n",
      "2023-12-30 18:07:24 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 533760 examples: 0.058 | 0.117\n",
      "2023-12-30 18:07:25 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 535616 examples: 0.073 | 0.134\n",
      "2023-12-30 18:07:25 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 537472 examples: 0.094 | 0.117\n",
      "2023-12-30 18:07:25 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 539328 examples: 0.079 | 0.154\n",
      "2023-12-30 18:07:26 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 541184 examples: 0.061 | 0.146\n",
      "2023-12-30 18:07:26 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 543040 examples: 0.118 | 0.128\n",
      "2023-12-30 18:07:26 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 544896 examples: 0.086 | 0.136\n",
      "2023-12-30 18:07:27 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 546752 examples: 0.074 | 0.124\n",
      "2023-12-30 18:07:27 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 548608 examples: 0.089 | 0.127\n",
      "2023-12-30 18:07:27 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 550464 examples: 0.067 | 0.135\n",
      "2023-12-30 18:07:28 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 552320 examples: 0.112 | 0.156\n",
      "2023-12-30 18:07:28 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 554176 examples: 0.060 | 0.159\n",
      "2023-12-30 18:07:28 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 556032 examples: 0.088 | 0.137\n",
      "2023-12-30 18:07:29 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 557888 examples: 0.070 | 0.128\n",
      "2023-12-30 18:07:29 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 559744 examples: 0.104 | 0.130\n",
      "2023-12-30 18:07:29 - INFO     | Early stopping: no decrease (0.099 vs 0.128); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:42<01:41, 10.20s/it]2023-12-30 18:07:29 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 18:07:29 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.008 | 0.128\n",
      "2023-12-30 18:07:30 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.074 | 0.127\n",
      "2023-12-30 18:07:30 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.051 | 0.122\n",
      "2023-12-30 18:07:30 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.044 | 0.111\n",
      "2023-12-30 18:07:31 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.032 | 0.112\n",
      "2023-12-30 18:07:31 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.053 | 0.131\n",
      "2023-12-30 18:07:31 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.050 | 0.115\n",
      "2023-12-30 18:07:32 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.055 | 0.124\n",
      "2023-12-30 18:07:32 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.074 | 0.101\n",
      "2023-12-30 18:07:32 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.061 | 0.123\n",
      "2023-12-30 18:07:33 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.061 | 0.113\n",
      "2023-12-30 18:07:33 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.077 | 0.107\n",
      "2023-12-30 18:07:33 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.038 | 0.128\n",
      "2023-12-30 18:07:34 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.086 | 0.140\n",
      "2023-12-30 18:07:34 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.085 | 0.126\n",
      "2023-12-30 18:07:34 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.091 | 0.148\n",
      "2023-12-30 18:07:34 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.071 | 0.124\n",
      "2023-12-30 18:07:35 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.050 | 0.105\n",
      "2023-12-30 18:07:35 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.045 | 0.109\n",
      "2023-12-30 18:07:36 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.032 | 0.123\n",
      "2023-12-30 18:07:36 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.050 | 0.140\n",
      "2023-12-30 18:07:36 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.092 | 0.136\n",
      "2023-12-30 18:07:36 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.075 | 0.115\n",
      "2023-12-30 18:07:37 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.061 | 0.138\n",
      "2023-12-30 18:07:37 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.089 | 0.127\n",
      "2023-12-30 18:07:37 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.078 | 0.125\n",
      "2023-12-30 18:07:38 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.064 | 0.120\n",
      "2023-12-30 18:07:38 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.087 | 0.121\n",
      "2023-12-30 18:07:38 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.072 | 0.140\n",
      "2023-12-30 18:07:39 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.088 | 0.185\n",
      "2023-12-30 18:07:39 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.091 | 0.148\n",
      "2023-12-30 18:07:39 - INFO     | Early stopping: no decrease (0.099 vs 0.151); counter: 3 out of 3\n",
      "2023-12-30 18:07:39 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:07:39 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 55%|█████▌    | 11/20 [01:52<01:31, 10.18s/it]2023-12-30 18:07:39 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 18:07:40 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.010 | 0.149\n",
      "2023-12-30 18:07:40 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.049 | 0.118\n",
      "2023-12-30 18:07:40 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.025 | 0.120\n",
      "2023-12-30 18:07:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.031 | 0.117\n",
      "2023-12-30 18:07:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.034 | 0.118\n",
      "2023-12-30 18:07:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.049 | 0.106\n",
      "2023-12-30 18:07:42 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.043 | 0.119\n",
      "2023-12-30 18:07:42 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.055 | 0.106\n",
      "2023-12-30 18:07:42 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.039 | 0.106\n",
      "2023-12-30 18:07:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.034 | 0.110\n",
      "2023-12-30 18:07:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.029 | 0.102\n",
      "2023-12-30 18:07:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.030 | 0.106\n",
      "2023-12-30 18:07:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.057 | 0.113\n",
      "2023-12-30 18:07:44 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.044 | 0.107\n",
      "2023-12-30 18:07:44 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.018 | 0.114\n",
      "2023-12-30 18:07:44 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.037 | 0.127\n",
      "2023-12-30 18:07:45 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.024 | 0.116\n",
      "2023-12-30 18:07:45 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.031 | 0.111\n",
      "2023-12-30 18:07:45 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.036 | 0.108\n",
      "2023-12-30 18:07:46 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.024 | 0.110\n",
      "2023-12-30 18:07:46 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.021 | 0.106\n",
      "2023-12-30 18:07:46 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.029 | 0.111\n",
      "2023-12-30 18:07:47 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.031 | 0.105\n",
      "2023-12-30 18:07:47 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.041 | 0.115\n",
      "2023-12-30 18:07:47 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.043 | 0.114\n",
      "2023-12-30 18:07:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.036 | 0.128\n",
      "2023-12-30 18:07:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.047 | 0.108\n",
      "2023-12-30 18:07:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.066 | 0.106\n",
      "2023-12-30 18:07:49 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.027 | 0.119\n",
      "2023-12-30 18:07:49 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.037 | 0.118\n",
      "2023-12-30 18:07:50 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.024 | 0.119\n",
      "2023-12-30 18:07:50 - INFO     | Early stopping: no decrease (0.099 vs 0.116); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [02:02<01:22, 10.30s/it]2023-12-30 18:07:50 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 18:07:50 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.001 | 0.116\n",
      "2023-12-30 18:07:51 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.007 | 0.115\n",
      "2023-12-30 18:07:51 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.017 | 0.112\n",
      "2023-12-30 18:07:51 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.019 | 0.112\n",
      "2023-12-30 18:07:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.033 | 0.107\n",
      "2023-12-30 18:07:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.011 | 0.104\n",
      "2023-12-30 18:07:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.012 | 0.094\n",
      "2023-12-30 18:07:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.019 | 0.106\n",
      "2023-12-30 18:07:53 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.030 | 0.112\n",
      "2023-12-30 18:07:53 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.035 | 0.106\n",
      "2023-12-30 18:07:53 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.018 | 0.109\n",
      "2023-12-30 18:07:54 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.030 | 0.115\n",
      "2023-12-30 18:07:54 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.023 | 0.104\n",
      "2023-12-30 18:07:54 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.015 | 0.113\n",
      "2023-12-30 18:07:55 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.032 | 0.110\n",
      "2023-12-30 18:07:55 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.023 | 0.112\n",
      "2023-12-30 18:07:56 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.027 | 0.114\n",
      "2023-12-30 18:07:56 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.019 | 0.113\n",
      "2023-12-30 18:07:56 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.015 | 0.117\n",
      "2023-12-30 18:07:57 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.037 | 0.122\n",
      "2023-12-30 18:07:57 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.037 | 0.107\n",
      "2023-12-30 18:07:57 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.035 | 0.117\n",
      "2023-12-30 18:07:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.011 | 0.117\n",
      "2023-12-30 18:07:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.035 | 0.121\n",
      "2023-12-30 18:07:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.022 | 0.117\n",
      "2023-12-30 18:07:59 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.032 | 0.120\n",
      "2023-12-30 18:07:59 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.031 | 0.118\n",
      "2023-12-30 18:07:59 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.040 | 0.115\n",
      "2023-12-30 18:08:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.034 | 0.127\n",
      "2023-12-30 18:08:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.056 | 0.110\n",
      "2023-12-30 18:08:00 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.029 | 0.108\n",
      "2023-12-30 18:08:01 - INFO     | Early stopping: no decrease (0.099 vs 0.110); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:13<01:12, 10.42s/it]2023-12-30 18:08:01 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 18:08:01 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.004 | 0.110\n",
      "2023-12-30 18:08:01 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.021 | 0.101\n",
      "2023-12-30 18:08:02 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.014 | 0.108\n",
      "2023-12-30 18:08:02 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.011 | 0.118\n",
      "2023-12-30 18:08:02 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.041 | 0.117\n",
      "2023-12-30 18:08:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.011 | 0.113\n",
      "2023-12-30 18:08:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.008 | 0.116\n",
      "2023-12-30 18:08:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.016 | 0.130\n",
      "2023-12-30 18:08:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.029 | 0.126\n",
      "2023-12-30 18:08:04 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.018 | 0.126\n",
      "2023-12-30 18:08:04 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.024 | 0.123\n",
      "2023-12-30 18:08:04 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.013 | 0.121\n",
      "2023-12-30 18:08:05 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.016 | 0.118\n",
      "2023-12-30 18:08:05 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.019 | 0.123\n",
      "2023-12-30 18:08:05 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.038 | 0.122\n",
      "2023-12-30 18:08:06 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.014 | 0.126\n",
      "2023-12-30 18:08:06 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.017 | 0.114\n",
      "2023-12-30 18:08:06 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.015 | 0.114\n",
      "2023-12-30 18:08:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.036 | 0.117\n",
      "2023-12-30 18:08:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.012 | 0.131\n",
      "2023-12-30 18:08:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.011 | 0.123\n",
      "2023-12-30 18:08:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.019 | 0.124\n",
      "2023-12-30 18:08:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.031 | 0.122\n",
      "2023-12-30 18:08:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.021 | 0.122\n",
      "2023-12-30 18:08:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.019 | 0.127\n",
      "2023-12-30 18:08:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.026 | 0.124\n",
      "2023-12-30 18:08:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.023 | 0.124\n",
      "2023-12-30 18:08:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.031 | 0.119\n",
      "2023-12-30 18:08:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.027 | 0.138\n",
      "2023-12-30 18:08:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.026 | 0.139\n",
      "2023-12-30 18:08:10 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.045 | 0.137\n",
      "2023-12-30 18:08:11 - INFO     | Early stopping: no decrease (0.099 vs 0.132); counter: 3 out of 3\n",
      "2023-12-30 18:08:11 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:08:11 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 70%|███████   | 14/20 [02:23<01:01, 10.33s/it]2023-12-30 18:08:11 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:08:11 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.018 | 0.130\n",
      "2023-12-30 18:08:11 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.008 | 0.124\n",
      "2023-12-30 18:08:12 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.021 | 0.125\n",
      "2023-12-30 18:08:12 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.009 | 0.124\n",
      "2023-12-30 18:08:12 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.010 | 0.115\n",
      "2023-12-30 18:08:13 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.018 | 0.116\n",
      "2023-12-30 18:08:13 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.005 | 0.114\n",
      "2023-12-30 18:08:13 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.018 | 0.122\n",
      "2023-12-30 18:08:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.011 | 0.125\n",
      "2023-12-30 18:08:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.007 | 0.116\n",
      "2023-12-30 18:08:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.009 | 0.114\n",
      "2023-12-30 18:08:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.010 | 0.115\n",
      "2023-12-30 18:08:15 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.009 | 0.113\n",
      "2023-12-30 18:08:15 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.005 | 0.117\n",
      "2023-12-30 18:08:15 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.011 | 0.125\n",
      "2023-12-30 18:08:16 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.004 | 0.121\n",
      "2023-12-30 18:08:16 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.008 | 0.122\n",
      "2023-12-30 18:08:16 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.016 | 0.130\n",
      "2023-12-30 18:08:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.008 | 0.126\n",
      "2023-12-30 18:08:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.007 | 0.124\n",
      "2023-12-30 18:08:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.008 | 0.117\n",
      "2023-12-30 18:08:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.018 | 0.121\n",
      "2023-12-30 18:08:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.014 | 0.124\n",
      "2023-12-30 18:08:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.010 | 0.109\n",
      "2023-12-30 18:08:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.008 | 0.107\n",
      "2023-12-30 18:08:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.006 | 0.112\n",
      "2023-12-30 18:08:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.013 | 0.116\n",
      "2023-12-30 18:08:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.005 | 0.113\n",
      "2023-12-30 18:08:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.010 | 0.116\n",
      "2023-12-30 18:08:20 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.006 | 0.114\n",
      "2023-12-30 18:08:21 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.017 | 0.117\n",
      "2023-12-30 18:08:21 - INFO     | Early stopping: no decrease (0.099 vs 0.118); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:33<00:51, 10.27s/it]2023-12-30 18:08:21 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:08:21 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.000 | 0.118\n",
      "2023-12-30 18:08:21 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.001 | 0.115\n",
      "2023-12-30 18:08:22 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.002 | 0.115\n",
      "2023-12-30 18:08:22 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.002 | 0.119\n",
      "2023-12-30 18:08:22 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.002 | 0.115\n",
      "2023-12-30 18:08:23 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.008 | 0.110\n",
      "2023-12-30 18:08:23 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.001 | 0.109\n",
      "2023-12-30 18:08:23 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.007 | 0.107\n",
      "2023-12-30 18:08:24 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.011 | 0.106\n",
      "2023-12-30 18:08:24 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.001 | 0.108\n",
      "2023-12-30 18:08:24 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.002 | 0.109\n",
      "2023-12-30 18:08:25 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.001 | 0.109\n",
      "2023-12-30 18:08:25 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.003 | 0.111\n",
      "2023-12-30 18:08:25 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.002 | 0.119\n",
      "2023-12-30 18:08:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.004 | 0.116\n",
      "2023-12-30 18:08:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.002 | 0.116\n",
      "2023-12-30 18:08:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.001 | 0.114\n",
      "2023-12-30 18:08:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.001 | 0.114\n",
      "2023-12-30 18:08:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.002 | 0.113\n",
      "2023-12-30 18:08:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.006 | 0.113\n",
      "2023-12-30 18:08:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.005 | 0.121\n",
      "2023-12-30 18:08:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.003 | 0.119\n",
      "2023-12-30 18:08:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.002 | 0.121\n",
      "2023-12-30 18:08:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.004 | 0.113\n",
      "2023-12-30 18:08:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.013 | 0.116\n",
      "2023-12-30 18:08:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.005 | 0.115\n",
      "2023-12-30 18:08:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.001 | 0.112\n",
      "2023-12-30 18:08:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.001 | 0.114\n",
      "2023-12-30 18:08:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.005 | 0.116\n",
      "2023-12-30 18:08:30 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.009 | 0.117\n",
      "2023-12-30 18:08:31 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.009 | 0.115\n",
      "2023-12-30 18:08:31 - INFO     | Early stopping: no decrease (0.099 vs 0.115); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:44<00:41, 10.27s/it]2023-12-30 18:08:31 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:08:31 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.000 | 0.115\n",
      "2023-12-30 18:08:32 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.001 | 0.116\n",
      "2023-12-30 18:08:32 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.005 | 0.117\n",
      "2023-12-30 18:08:32 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.004 | 0.116\n",
      "2023-12-30 18:08:33 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.004 | 0.113\n",
      "2023-12-30 18:08:33 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.003 | 0.109\n",
      "2023-12-30 18:08:33 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.005 | 0.110\n",
      "2023-12-30 18:08:34 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.003 | 0.113\n",
      "2023-12-30 18:08:34 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.003 | 0.116\n",
      "2023-12-30 18:08:34 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.001 | 0.116\n",
      "2023-12-30 18:08:35 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.001 | 0.112\n",
      "2023-12-30 18:08:35 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.003 | 0.110\n",
      "2023-12-30 18:08:35 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.002 | 0.115\n",
      "2023-12-30 18:08:36 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.002 | 0.116\n",
      "2023-12-30 18:08:36 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.005 | 0.118\n",
      "2023-12-30 18:08:36 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.001 | 0.119\n",
      "2023-12-30 18:08:37 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.003 | 0.114\n",
      "2023-12-30 18:08:37 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.003 | 0.118\n",
      "2023-12-30 18:08:37 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.001 | 0.119\n",
      "2023-12-30 18:08:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.005 | 0.113\n",
      "2023-12-30 18:08:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.003 | 0.117\n",
      "2023-12-30 18:08:38 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.012 | 0.120\n",
      "2023-12-30 18:08:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.003 | 0.128\n",
      "2023-12-30 18:08:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.006 | 0.121\n",
      "2023-12-30 18:08:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.002 | 0.121\n",
      "2023-12-30 18:08:39 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.001 | 0.118\n",
      "2023-12-30 18:08:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.002 | 0.117\n",
      "2023-12-30 18:08:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.001 | 0.119\n",
      "2023-12-30 18:08:40 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.002 | 0.117\n",
      "2023-12-30 18:08:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.004 | 0.115\n",
      "2023-12-30 18:08:41 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.005 | 0.120\n",
      "2023-12-30 18:08:41 - INFO     | Early stopping: no decrease (0.099 vs 0.121); counter: 3 out of 3\n",
      "2023-12-30 18:08:41 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:08:41 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      " 85%|████████▌ | 17/20 [02:54<00:30, 10.25s/it]2023-12-30 18:08:41 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 18:08:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.122\n",
      "2023-12-30 18:08:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.001 | 0.121\n",
      "2023-12-30 18:08:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.002 | 0.123\n",
      "2023-12-30 18:08:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.001 | 0.120\n",
      "2023-12-30 18:08:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.001 | 0.117\n",
      "2023-12-30 18:08:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.008 | 0.117\n",
      "2023-12-30 18:08:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.001 | 0.117\n",
      "2023-12-30 18:08:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.000 | 0.118\n",
      "2023-12-30 18:08:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.000 | 0.119\n",
      "2023-12-30 18:08:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.000 | 0.118\n",
      "2023-12-30 18:08:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.001 | 0.120\n",
      "2023-12-30 18:08:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.001 | 0.120\n",
      "2023-12-30 18:08:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.001 | 0.117\n",
      "2023-12-30 18:08:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.000 | 0.119\n",
      "2023-12-30 18:08:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.000 | 0.119\n",
      "2023-12-30 18:08:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.001 | 0.118\n",
      "2023-12-30 18:08:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.002 | 0.119\n",
      "2023-12-30 18:08:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.001 | 0.123\n",
      "2023-12-30 18:08:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.001 | 0.119\n",
      "2023-12-30 18:08:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.001 | 0.122\n",
      "2023-12-30 18:08:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.001 | 0.120\n",
      "2023-12-30 18:08:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.001 | 0.117\n",
      "2023-12-30 18:08:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.001 | 0.122\n",
      "2023-12-30 18:08:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.001 | 0.118\n",
      "2023-12-30 18:08:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.002 | 0.120\n",
      "2023-12-30 18:08:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.000 | 0.120\n",
      "2023-12-30 18:08:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.000 | 0.120\n",
      "2023-12-30 18:08:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.001 | 0.119\n",
      "2023-12-30 18:08:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.002 | 0.115\n",
      "2023-12-30 18:08:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.003 | 0.122\n",
      "2023-12-30 18:08:51 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.003 | 0.109\n",
      "2023-12-30 18:08:51 - INFO     | Early stopping: no decrease (0.099 vs 0.109); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [03:04<00:20, 10.21s/it]2023-12-30 18:08:51 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 18:08:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.000 | 0.109\n",
      "2023-12-30 18:08:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.001 | 0.117\n",
      "2023-12-30 18:08:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.001 | 0.115\n",
      "2023-12-30 18:08:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.116\n",
      "2023-12-30 18:08:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.001 | 0.115\n",
      "2023-12-30 18:08:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.113\n",
      "2023-12-30 18:08:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.113\n",
      "2023-12-30 18:08:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.000 | 0.113\n",
      "2023-12-30 18:08:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.113\n",
      "2023-12-30 18:08:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.114\n",
      "2023-12-30 18:08:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.114\n",
      "2023-12-30 18:08:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.001 | 0.122\n",
      "2023-12-30 18:08:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.000 | 0.122\n",
      "2023-12-30 18:08:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.120\n",
      "2023-12-30 18:08:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.001 | 0.119\n",
      "2023-12-30 18:08:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.006 | 0.122\n",
      "2023-12-30 18:08:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.000 | 0.122\n",
      "2023-12-30 18:08:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.122\n",
      "2023-12-30 18:08:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.120\n",
      "2023-12-30 18:08:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.121\n",
      "2023-12-30 18:08:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.000 | 0.121\n",
      "2023-12-30 18:08:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.120\n",
      "2023-12-30 18:08:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.120\n",
      "2023-12-30 18:08:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.000 | 0.120\n",
      "2023-12-30 18:09:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.120\n",
      "2023-12-30 18:09:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.000 | 0.121\n",
      "2023-12-30 18:09:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.000 | 0.121\n",
      "2023-12-30 18:09:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.000 | 0.121\n",
      "2023-12-30 18:09:01 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:02 - INFO     | Early stopping: no decrease (0.099 vs 0.122); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [03:14<00:10, 10.25s/it]2023-12-30 18:09:02 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 18:09:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.000 | 0.121\n",
      "2023-12-30 18:09:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.122\n",
      "2023-12-30 18:09:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.002 | 0.126\n",
      "2023-12-30 18:09:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.000 | 0.130\n",
      "2023-12-30 18:09:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.000 | 0.127\n",
      "2023-12-30 18:09:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.125\n",
      "2023-12-30 18:09:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.126\n",
      "2023-12-30 18:09:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.124\n",
      "2023-12-30 18:09:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.123\n",
      "2023-12-30 18:09:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.125\n",
      "2023-12-30 18:09:12 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.125\n",
      "2023-12-30 18:09:12 - INFO     | Early stopping: no decrease (0.099 vs 0.125); counter: 3 out of 3\n",
      "2023-12-30 18:09:12 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:09:12 - INFO     | Reducing learning rate: 0.0003125 -> 0.00015625\n",
      "100%|██████████| 20/20 [03:24<00:00, 10.25s/it]\n",
      "2023-12-30 18:09:12 - INFO     | Best validation loss: 0.099\n",
      "2023-12-30 18:09:12 - INFO     | Best early stopping index/epoch: 7\n",
      "2023-12-30 18:09:12 - INFO     | Average Loss on test set: 0.127\n",
      "2023-12-30 18:09:15 - INFO     | Weighted Precision: 0.984, Recall: 0.984, F1: 0.984\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▃▃▃▃▃▄▃▃▂▃▂▂▂▂▂▂▃▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▃▂▃▂▂▂▂▂▂▃▁▁▂▁▁▂▁▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_validation_loss</td><td>0.09933</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00031</td></tr><tr><td>step_learning_rate</td><td>0.00031</td></tr><tr><td>step_training_loss</td><td>6e-05</td></tr><tr><td>step_validation_loss</td><td>0.12528</td></tr><tr><td>test_loss</td><td>0.12657</td></tr><tr><td>weighted_f1</td><td>0.98401</td></tr><tr><td>weighted_precision</td><td>0.98403</td></tr><tr><td>weighted_recall</td><td>0.984</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-15</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2y6sm2wd' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2y6sm2wd</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_180546-2y6sm2wd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: buofn5s6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [32, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_180925-buofn5s6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/buofn5s6' target=\"_blank\">genial-sweep-16</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/buofn5s6' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/buofn5s6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [32, 64], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:09:26 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 18:09:26 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 41.767 | 3215.631\n",
      "2023-12-30 18:09:27 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 114.459 | 2.304\n",
      "2023-12-30 18:09:27 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 2.298 | 2.285\n",
      "2023-12-30 18:09:27 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 2.274 | 2.255\n",
      "2023-12-30 18:09:28 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 2.199 | 2.071\n",
      "2023-12-30 18:09:28 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 1.502 | 0.900\n",
      "2023-12-30 18:09:28 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 0.876 | 0.762\n",
      "2023-12-30 18:09:29 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.605 | 0.601\n",
      "2023-12-30 18:09:29 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.496 | 0.640\n",
      "2023-12-30 18:09:29 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.472 | 0.484\n",
      "2023-12-30 18:09:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.338 | 0.333\n",
      "2023-12-30 18:09:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.363 | 0.305\n",
      "2023-12-30 18:09:30 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.289 | 0.292\n",
      "2023-12-30 18:09:31 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.278 | 0.245\n",
      "2023-12-30 18:09:31 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.230 | 0.280\n",
      "2023-12-30 18:09:31 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.208 | 0.242\n",
      "2023-12-30 18:09:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.177 | 0.187\n",
      "2023-12-30 18:09:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.232 | 0.180\n",
      "2023-12-30 18:09:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.194 | 0.195\n",
      "2023-12-30 18:09:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.200 | 0.206\n",
      "2023-12-30 18:09:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.164 | 0.151\n",
      "2023-12-30 18:09:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.136 | 0.156\n",
      "2023-12-30 18:09:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.156 | 0.156\n",
      "2023-12-30 18:09:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.155 | 0.158\n",
      "2023-12-30 18:09:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.152 | 0.145\n",
      "2023-12-30 18:09:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.160 | 0.141\n",
      "2023-12-30 18:09:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.145 | 0.141\n",
      "2023-12-30 18:09:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.134 | 0.153\n",
      "2023-12-30 18:09:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.153 | 0.133\n",
      "2023-12-30 18:09:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.155 | 0.136\n",
      "2023-12-30 18:09:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.124 | 0.131\n",
      "2023-12-30 18:09:37 - INFO     | Early stopping: loss decreased (inf -> 0.120; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:21, 10.59s/it]2023-12-30 18:09:37 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 18:09:37 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.035 | 0.120\n",
      "2023-12-30 18:09:37 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.109 | 0.116\n",
      "2023-12-30 18:09:38 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.133 | 0.121\n",
      "2023-12-30 18:09:38 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.114 | 0.144\n",
      "2023-12-30 18:09:38 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.128 | 0.108\n",
      "2023-12-30 18:09:39 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.116 | 0.122\n",
      "2023-12-30 18:09:39 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.101 | 0.126\n",
      "2023-12-30 18:09:39 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.109 | 0.135\n",
      "2023-12-30 18:09:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.104 | 0.108\n",
      "2023-12-30 18:09:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.136 | 0.116\n",
      "2023-12-30 18:09:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.105 | 0.112\n",
      "2023-12-30 18:09:40 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.145 | 0.105\n",
      "2023-12-30 18:09:41 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.086 | 0.107\n",
      "2023-12-30 18:09:41 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.110 | 0.103\n",
      "2023-12-30 18:09:41 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.102 | 0.101\n",
      "2023-12-30 18:09:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.111 | 0.098\n",
      "2023-12-30 18:09:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.111 | 0.109\n",
      "2023-12-30 18:09:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.096 | 0.099\n",
      "2023-12-30 18:09:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.100 | 0.094\n",
      "2023-12-30 18:09:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.090 | 0.095\n",
      "2023-12-30 18:09:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.094 | 0.111\n",
      "2023-12-30 18:09:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.098 | 0.098\n",
      "2023-12-30 18:09:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.102 | 0.093\n",
      "2023-12-30 18:09:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.093 | 0.096\n",
      "2023-12-30 18:09:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.089 | 0.101\n",
      "2023-12-30 18:09:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.078 | 0.092\n",
      "2023-12-30 18:09:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.112 | 0.099\n",
      "2023-12-30 18:09:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.100 | 0.108\n",
      "2023-12-30 18:09:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.092 | 0.121\n",
      "2023-12-30 18:09:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.121 | 0.090\n",
      "2023-12-30 18:09:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.095 | 0.093\n",
      "2023-12-30 18:09:47 - INFO     | Early stopping: loss decreased (0.120 -> 0.088; -26.4%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:04, 10.25s/it]2023-12-30 18:09:47 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 18:09:47 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.168 | 0.089\n",
      "2023-12-30 18:09:47 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.081 | 0.083\n",
      "2023-12-30 18:09:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.075 | 0.088\n",
      "2023-12-30 18:09:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.075 | 0.099\n",
      "2023-12-30 18:09:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.104 | 0.105\n",
      "2023-12-30 18:09:48 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.082 | 0.087\n",
      "2023-12-30 18:09:49 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.063 | 0.085\n",
      "2023-12-30 18:09:49 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.068 | 0.096\n",
      "2023-12-30 18:09:49 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.093 | 0.110\n",
      "2023-12-30 18:09:50 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.086 | 0.095\n",
      "2023-12-30 18:09:50 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.080 | 0.092\n",
      "2023-12-30 18:09:50 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.085 | 0.091\n",
      "2023-12-30 18:09:51 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.085 | 0.092\n",
      "2023-12-30 18:09:51 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.091 | 0.093\n",
      "2023-12-30 18:09:51 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.063 | 0.085\n",
      "2023-12-30 18:09:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.078 | 0.085\n",
      "2023-12-30 18:09:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.096 | 0.079\n",
      "2023-12-30 18:09:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.059 | 0.080\n",
      "2023-12-30 18:09:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.081 | 0.088\n",
      "2023-12-30 18:09:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.081 | 0.085\n",
      "2023-12-30 18:09:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.089 | 0.103\n",
      "2023-12-30 18:09:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.089 | 0.080\n",
      "2023-12-30 18:09:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.089 | 0.077\n",
      "2023-12-30 18:09:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.089 | 0.072\n",
      "2023-12-30 18:09:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.081 | 0.079\n",
      "2023-12-30 18:09:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.076 | 0.084\n",
      "2023-12-30 18:09:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.068 | 0.090\n",
      "2023-12-30 18:09:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.068 | 0.078\n",
      "2023-12-30 18:09:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.085 | 0.080\n",
      "2023-12-30 18:09:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.086 | 0.079\n",
      "2023-12-30 18:09:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.083 | 0.082\n",
      "2023-12-30 18:09:57 - INFO     | Early stopping: loss decreased (0.088 -> 0.073; -17.4%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:52, 10.17s/it]2023-12-30 18:09:57 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 18:09:57 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.039 | 0.074\n",
      "2023-12-30 18:09:57 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.053 | 0.083\n",
      "2023-12-30 18:09:58 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.065 | 0.078\n",
      "2023-12-30 18:09:58 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.083 | 0.072\n",
      "2023-12-30 18:09:58 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.076 | 0.073\n",
      "2023-12-30 18:09:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.053 | 0.070\n",
      "2023-12-30 18:09:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.057 | 0.074\n",
      "2023-12-30 18:09:59 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.077 | 0.080\n",
      "2023-12-30 18:10:00 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.058 | 0.070\n",
      "2023-12-30 18:10:00 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.066 | 0.082\n",
      "2023-12-30 18:10:00 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.053 | 0.091\n",
      "2023-12-30 18:10:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.066 | 0.076\n",
      "2023-12-30 18:10:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.072 | 0.078\n",
      "2023-12-30 18:10:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.080 | 0.076\n",
      "2023-12-30 18:10:01 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.072 | 0.073\n",
      "2023-12-30 18:10:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.058 | 0.073\n",
      "2023-12-30 18:10:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.056 | 0.072\n",
      "2023-12-30 18:10:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.096 | 0.068\n",
      "2023-12-30 18:10:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.067 | 0.075\n",
      "2023-12-30 18:10:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.070 | 0.078\n",
      "2023-12-30 18:10:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.068 | 0.074\n",
      "2023-12-30 18:10:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.056 | 0.076\n",
      "2023-12-30 18:10:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.073 | 0.075\n",
      "2023-12-30 18:10:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.063 | 0.075\n",
      "2023-12-30 18:10:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.072 | 0.074\n",
      "2023-12-30 18:10:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.074 | 0.080\n",
      "2023-12-30 18:10:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.067 | 0.068\n",
      "2023-12-30 18:10:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.072 | 0.068\n",
      "2023-12-30 18:10:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.059 | 0.070\n",
      "2023-12-30 18:10:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.067 | 0.069\n",
      "2023-12-30 18:10:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.081 | 0.083\n",
      "2023-12-30 18:10:07 - INFO     | Early stopping: no decrease (0.073 vs 0.078); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:41<02:43, 10.24s/it]2023-12-30 18:10:07 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 18:10:07 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.025 | 0.071\n",
      "2023-12-30 18:10:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.053 | 0.069\n",
      "2023-12-30 18:10:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.044 | 0.080\n",
      "2023-12-30 18:10:08 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.064 | 0.087\n",
      "2023-12-30 18:10:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.046 | 0.080\n",
      "2023-12-30 18:10:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.072 | 0.070\n",
      "2023-12-30 18:10:09 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.058 | 0.066\n",
      "2023-12-30 18:10:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.061 | 0.071\n",
      "2023-12-30 18:10:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.063 | 0.082\n",
      "2023-12-30 18:10:10 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.078 | 0.075\n",
      "2023-12-30 18:10:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.059 | 0.076\n",
      "2023-12-30 18:10:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.056 | 0.066\n",
      "2023-12-30 18:10:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.063 | 0.064\n",
      "2023-12-30 18:10:11 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.042 | 0.066\n",
      "2023-12-30 18:10:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.052 | 0.068\n",
      "2023-12-30 18:10:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.077 | 0.076\n",
      "2023-12-30 18:10:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.061 | 0.065\n",
      "2023-12-30 18:10:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.054 | 0.068\n",
      "2023-12-30 18:10:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.061 | 0.066\n",
      "2023-12-30 18:10:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.064 | 0.065\n",
      "2023-12-30 18:10:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.043 | 0.068\n",
      "2023-12-30 18:10:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.061 | 0.069\n",
      "2023-12-30 18:10:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.065 | 0.080\n",
      "2023-12-30 18:10:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.069 | 0.067\n",
      "2023-12-30 18:10:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.061 | 0.066\n",
      "2023-12-30 18:10:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.086 | 0.074\n",
      "2023-12-30 18:10:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.042 | 0.072\n",
      "2023-12-30 18:10:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.039 | 0.077\n",
      "2023-12-30 18:10:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.066 | 0.062\n",
      "2023-12-30 18:10:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.061 | 0.063\n",
      "2023-12-30 18:10:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.051 | 0.083\n",
      "2023-12-30 18:10:18 - INFO     | Early stopping: loss decreased (0.073 -> 0.064; -12.7%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:51<02:35, 10.38s/it]2023-12-30 18:10:18 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 18:10:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.079 | 0.065\n",
      "2023-12-30 18:10:18 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.049 | 0.076\n",
      "2023-12-30 18:10:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.072 | 0.083\n",
      "2023-12-30 18:10:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.070 | 0.065\n",
      "2023-12-30 18:10:19 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.054 | 0.065\n",
      "2023-12-30 18:10:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.064 | 0.065\n",
      "2023-12-30 18:10:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.049 | 0.063\n",
      "2023-12-30 18:10:20 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.065 | 0.063\n",
      "2023-12-30 18:10:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.049 | 0.060\n",
      "2023-12-30 18:10:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.050 | 0.061\n",
      "2023-12-30 18:10:21 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.048 | 0.063\n",
      "2023-12-30 18:10:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.055 | 0.063\n",
      "2023-12-30 18:10:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.056 | 0.071\n",
      "2023-12-30 18:10:22 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.040 | 0.062\n",
      "2023-12-30 18:10:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.062 | 0.067\n",
      "2023-12-30 18:10:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.069 | 0.061\n",
      "2023-12-30 18:10:23 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.060 | 0.064\n",
      "2023-12-30 18:10:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.045 | 0.060\n",
      "2023-12-30 18:10:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.051 | 0.061\n",
      "2023-12-30 18:10:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.062 | 0.063\n",
      "2023-12-30 18:10:24 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.040 | 0.063\n",
      "2023-12-30 18:10:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.062 | 0.067\n",
      "2023-12-30 18:10:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.056 | 0.070\n",
      "2023-12-30 18:10:25 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.049 | 0.067\n",
      "2023-12-30 18:10:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.060 | 0.065\n",
      "2023-12-30 18:10:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.061 | 0.079\n",
      "2023-12-30 18:10:26 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.051 | 0.065\n",
      "2023-12-30 18:10:27 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.054 | 0.067\n",
      "2023-12-30 18:10:27 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.047 | 0.064\n",
      "2023-12-30 18:10:27 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.051 | 0.065\n",
      "2023-12-30 18:10:28 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.031 | 0.068\n",
      "2023-12-30 18:10:28 - INFO     | Early stopping: no decrease (0.064 vs 0.070); counter: 1 out of 3\n",
      " 30%|███       | 6/20 [01:01<02:24, 10.34s/it]2023-12-30 18:10:28 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 18:10:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.007 | 0.068\n",
      "2023-12-30 18:10:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.044 | 0.084\n",
      "2023-12-30 18:10:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.037 | 0.058\n",
      "2023-12-30 18:10:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.036 | 0.061\n",
      "2023-12-30 18:10:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.048 | 0.064\n",
      "2023-12-30 18:10:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.048 | 0.069\n",
      "2023-12-30 18:10:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.060 | 0.064\n",
      "2023-12-30 18:10:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.044 | 0.061\n",
      "2023-12-30 18:10:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.048 | 0.074\n",
      "2023-12-30 18:10:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.038 | 0.064\n",
      "2023-12-30 18:10:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.052 | 0.064\n",
      "2023-12-30 18:10:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.065 | 0.063\n",
      "2023-12-30 18:10:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.049 | 0.065\n",
      "2023-12-30 18:10:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.052 | 0.078\n",
      "2023-12-30 18:10:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.044 | 0.065\n",
      "2023-12-30 18:10:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.069 | 0.069\n",
      "2023-12-30 18:10:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.045 | 0.059\n",
      "2023-12-30 18:10:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.049 | 0.060\n",
      "2023-12-30 18:10:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.055 | 0.064\n",
      "2023-12-30 18:10:34 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.043 | 0.060\n",
      "2023-12-30 18:10:35 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.038 | 0.057\n",
      "2023-12-30 18:10:35 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.040 | 0.065\n",
      "2023-12-30 18:10:35 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.054 | 0.065\n",
      "2023-12-30 18:10:36 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.037 | 0.055\n",
      "2023-12-30 18:10:36 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.049 | 0.061\n",
      "2023-12-30 18:10:36 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.068 | 0.065\n",
      "2023-12-30 18:10:37 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.042 | 0.063\n",
      "2023-12-30 18:10:37 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.043 | 0.059\n",
      "2023-12-30 18:10:37 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.054 | 0.062\n",
      "2023-12-30 18:10:38 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.054 | 0.071\n",
      "2023-12-30 18:10:38 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.052 | 0.058\n",
      "2023-12-30 18:10:38 - INFO     | Early stopping: loss decreased (0.064 -> 0.057; -10.0%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:12<02:14, 10.32s/it]2023-12-30 18:10:38 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 18:10:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.070 | 0.061\n",
      "2023-12-30 18:10:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.033 | 0.059\n",
      "2023-12-30 18:10:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.033 | 0.061\n",
      "2023-12-30 18:10:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.040 | 0.067\n",
      "2023-12-30 18:10:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.042 | 0.067\n",
      "2023-12-30 18:10:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.052 | 0.063\n",
      "2023-12-30 18:10:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.037 | 0.060\n",
      "2023-12-30 18:10:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.052 | 0.067\n",
      "2023-12-30 18:10:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.037 | 0.064\n",
      "2023-12-30 18:10:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.041 | 0.066\n",
      "2023-12-30 18:10:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.043 | 0.063\n",
      "2023-12-30 18:10:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.048 | 0.059\n",
      "2023-12-30 18:10:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.037 | 0.066\n",
      "2023-12-30 18:10:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.049 | 0.065\n",
      "2023-12-30 18:10:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.032 | 0.059\n",
      "2023-12-30 18:10:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.068 | 0.058\n",
      "2023-12-30 18:10:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.053 | 0.061\n",
      "2023-12-30 18:10:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.029 | 0.060\n",
      "2023-12-30 18:10:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.038 | 0.059\n",
      "2023-12-30 18:10:44 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.057 | 0.066\n",
      "2023-12-30 18:10:45 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.061 | 0.067\n",
      "2023-12-30 18:10:45 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.050 | 0.060\n",
      "2023-12-30 18:10:45 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.056 | 0.064\n",
      "2023-12-30 18:10:46 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.060 | 0.061\n",
      "2023-12-30 18:10:46 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.038 | 0.060\n",
      "2023-12-30 18:10:46 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.030 | 0.057\n",
      "2023-12-30 18:10:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.038 | 0.058\n",
      "2023-12-30 18:10:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.051 | 0.062\n",
      "2023-12-30 18:10:47 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.048 | 0.061\n",
      "2023-12-30 18:10:48 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.036 | 0.062\n",
      "2023-12-30 18:10:48 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.040 | 0.064\n",
      "2023-12-30 18:10:48 - INFO     | Early stopping: no decrease (0.057 vs 0.060); counter: 1 out of 3\n",
      " 40%|████      | 8/20 [01:22<02:03, 10.26s/it]2023-12-30 18:10:48 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 18:10:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.020 | 0.058\n",
      "2023-12-30 18:10:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.050 | 0.057\n",
      "2023-12-30 18:10:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.040 | 0.056\n",
      "2023-12-30 18:10:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.033 | 0.059\n",
      "2023-12-30 18:10:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.025 | 0.056\n",
      "2023-12-30 18:10:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.037 | 0.060\n",
      "2023-12-30 18:10:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.034 | 0.058\n",
      "2023-12-30 18:10:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.050 | 0.062\n",
      "2023-12-30 18:10:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.029 | 0.060\n",
      "2023-12-30 18:10:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.045 | 0.062\n",
      "2023-12-30 18:10:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.057 | 0.057\n",
      "2023-12-30 18:10:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.032 | 0.063\n",
      "2023-12-30 18:10:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.042 | 0.059\n",
      "2023-12-30 18:10:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.055 | 0.057\n",
      "2023-12-30 18:10:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.049 | 0.055\n",
      "2023-12-30 18:10:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.044 | 0.062\n",
      "2023-12-30 18:10:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.045 | 0.068\n",
      "2023-12-30 18:10:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.044 | 0.057\n",
      "2023-12-30 18:10:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.023 | 0.061\n",
      "2023-12-30 18:10:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.040 | 0.060\n",
      "2023-12-30 18:10:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.048 | 0.056\n",
      "2023-12-30 18:10:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.043 | 0.064\n",
      "2023-12-30 18:10:55 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.059 | 0.066\n",
      "2023-12-30 18:10:56 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.040 | 0.064\n",
      "2023-12-30 18:10:56 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.039 | 0.058\n",
      "2023-12-30 18:10:56 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.046 | 0.080\n",
      "2023-12-30 18:10:57 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.041 | 0.087\n",
      "2023-12-30 18:10:57 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.037 | 0.061\n",
      "2023-12-30 18:10:57 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.060 | 0.059\n",
      "2023-12-30 18:10:58 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.036 | 0.058\n",
      "2023-12-30 18:10:58 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.024 | 0.054\n",
      "2023-12-30 18:10:58 - INFO     | Early stopping: no decrease (0.057 vs 0.057); counter: 2 out of 3\n",
      " 45%|████▌     | 9/20 [01:32<01:51, 10.14s/it]2023-12-30 18:10:58 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 18:10:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.013 | 0.058\n",
      "2023-12-30 18:10:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.048 | 0.059\n",
      "2023-12-30 18:10:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.031 | 0.057\n",
      "2023-12-30 18:10:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.039 | 0.055\n",
      "2023-12-30 18:11:00 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.022 | 0.056\n",
      "2023-12-30 18:11:00 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.028 | 0.056\n",
      "2023-12-30 18:11:00 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.040 | 0.070\n",
      "2023-12-30 18:11:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.036 | 0.060\n",
      "2023-12-30 18:11:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.024 | 0.080\n",
      "2023-12-30 18:11:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.043 | 0.067\n",
      "2023-12-30 18:11:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.040 | 0.061\n",
      "2023-12-30 18:11:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.037 | 0.061\n",
      "2023-12-30 18:11:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.032 | 0.056\n",
      "2023-12-30 18:11:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.031 | 0.063\n",
      "2023-12-30 18:11:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.039 | 0.057\n",
      "2023-12-30 18:11:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.045 | 0.079\n",
      "2023-12-30 18:11:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.055 | 0.061\n",
      "2023-12-30 18:11:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.046 | 0.061\n",
      "2023-12-30 18:11:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.031 | 0.057\n",
      "2023-12-30 18:11:05 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.047 | 0.059\n",
      "2023-12-30 18:11:05 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.030 | 0.059\n",
      "2023-12-30 18:11:05 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.040 | 0.053\n",
      "2023-12-30 18:11:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.049 | 0.056\n",
      "2023-12-30 18:11:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.032 | 0.072\n",
      "2023-12-30 18:11:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.031 | 0.054\n",
      "2023-12-30 18:11:06 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.041 | 0.060\n",
      "2023-12-30 18:11:07 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.038 | 0.055\n",
      "2023-12-30 18:11:07 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.046 | 0.051\n",
      "2023-12-30 18:11:07 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.032 | 0.062\n",
      "2023-12-30 18:11:08 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.057 | 0.064\n",
      "2023-12-30 18:11:08 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.053 | 0.062\n",
      "2023-12-30 18:11:08 - INFO     | Early stopping: no decrease (0.057 vs 0.065); counter: 3 out of 3\n",
      "2023-12-30 18:11:08 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:11:08 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 50%|█████     | 10/20 [01:42<01:41, 10.13s/it]2023-12-30 18:11:08 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 18:11:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.016 | 0.064\n",
      "2023-12-30 18:11:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.036 | 0.052\n",
      "2023-12-30 18:11:09 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.023 | 0.053\n",
      "2023-12-30 18:11:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.018 | 0.052\n",
      "2023-12-30 18:11:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.032 | 0.055\n",
      "2023-12-30 18:11:10 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.022 | 0.052\n",
      "2023-12-30 18:11:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.025 | 0.054\n",
      "2023-12-30 18:11:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.027 | 0.052\n",
      "2023-12-30 18:11:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.020 | 0.050\n",
      "2023-12-30 18:11:11 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.032 | 0.052\n",
      "2023-12-30 18:11:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.027 | 0.050\n",
      "2023-12-30 18:11:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.038 | 0.051\n",
      "2023-12-30 18:11:12 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.021 | 0.051\n",
      "2023-12-30 18:11:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.043 | 0.051\n",
      "2023-12-30 18:11:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.034 | 0.050\n",
      "2023-12-30 18:11:13 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.026 | 0.050\n",
      "2023-12-30 18:11:14 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.027 | 0.054\n",
      "2023-12-30 18:11:14 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.031 | 0.051\n",
      "2023-12-30 18:11:14 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.023 | 0.052\n",
      "2023-12-30 18:11:15 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.027 | 0.050\n",
      "2023-12-30 18:11:15 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.027 | 0.050\n",
      "2023-12-30 18:11:15 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.025 | 0.049\n",
      "2023-12-30 18:11:16 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.022 | 0.049\n",
      "2023-12-30 18:11:16 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.018 | 0.051\n",
      "2023-12-30 18:11:16 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.026 | 0.052\n",
      "2023-12-30 18:11:17 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.045 | 0.052\n",
      "2023-12-30 18:11:17 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.026 | 0.050\n",
      "2023-12-30 18:11:17 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.024 | 0.053\n",
      "2023-12-30 18:11:18 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.025 | 0.050\n",
      "2023-12-30 18:11:18 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.036 | 0.049\n",
      "2023-12-30 18:11:18 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.022 | 0.052\n",
      "2023-12-30 18:11:19 - INFO     | Early stopping: loss decreased (0.057 -> 0.051; -11.0%). Caching model state.\n",
      " 55%|█████▌    | 11/20 [01:52<01:31, 10.19s/it]2023-12-30 18:11:19 - INFO     | Epoch: 11 | Learning Rate: 0.003\n",
      "2023-12-30 18:11:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 616064 examples: 0.001 | 0.051\n",
      "2023-12-30 18:11:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 617920 examples: 0.030 | 0.051\n",
      "2023-12-30 18:11:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 619776 examples: 0.034 | 0.054\n",
      "2023-12-30 18:11:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 621632 examples: 0.016 | 0.053\n",
      "2023-12-30 18:11:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 623488 examples: 0.021 | 0.054\n",
      "2023-12-30 18:11:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 625344 examples: 0.035 | 0.056\n",
      "2023-12-30 18:11:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 627200 examples: 0.019 | 0.051\n",
      "2023-12-30 18:11:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 629056 examples: 0.022 | 0.054\n",
      "2023-12-30 18:11:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 630912 examples: 0.032 | 0.050\n",
      "2023-12-30 18:11:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 632768 examples: 0.027 | 0.052\n",
      "2023-12-30 18:11:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 634624 examples: 0.028 | 0.051\n",
      "2023-12-30 18:11:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 636480 examples: 0.027 | 0.049\n",
      "2023-12-30 18:11:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 638336 examples: 0.016 | 0.050\n",
      "2023-12-30 18:11:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 640192 examples: 0.032 | 0.050\n",
      "2023-12-30 18:11:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 642048 examples: 0.027 | 0.054\n",
      "2023-12-30 18:11:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 643904 examples: 0.030 | 0.051\n",
      "2023-12-30 18:11:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 645760 examples: 0.023 | 0.051\n",
      "2023-12-30 18:11:25 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 647616 examples: 0.022 | 0.053\n",
      "2023-12-30 18:11:25 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 649472 examples: 0.016 | 0.051\n",
      "2023-12-30 18:11:25 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 651328 examples: 0.024 | 0.052\n",
      "2023-12-30 18:11:26 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 653184 examples: 0.025 | 0.052\n",
      "2023-12-30 18:11:26 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 655040 examples: 0.023 | 0.050\n",
      "2023-12-30 18:11:26 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 656896 examples: 0.033 | 0.051\n",
      "2023-12-30 18:11:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 658752 examples: 0.022 | 0.051\n",
      "2023-12-30 18:11:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 660608 examples: 0.023 | 0.051\n",
      "2023-12-30 18:11:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 662464 examples: 0.029 | 0.049\n",
      "2023-12-30 18:11:27 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 664320 examples: 0.020 | 0.052\n",
      "2023-12-30 18:11:28 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 666176 examples: 0.025 | 0.054\n",
      "2023-12-30 18:11:28 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 668032 examples: 0.030 | 0.050\n",
      "2023-12-30 18:11:28 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 669888 examples: 0.032 | 0.048\n",
      "2023-12-30 18:11:29 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 671744 examples: 0.030 | 0.050\n",
      "2023-12-30 18:11:29 - INFO     | Early stopping: no decrease (0.051 vs 0.051); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [02:02<01:21, 10.23s/it]2023-12-30 18:11:29 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 18:11:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.003 | 0.051\n",
      "2023-12-30 18:11:30 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.022 | 0.051\n",
      "2023-12-30 18:11:30 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.018 | 0.051\n",
      "2023-12-30 18:11:30 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.034 | 0.050\n",
      "2023-12-30 18:11:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.022 | 0.051\n",
      "2023-12-30 18:11:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.023 | 0.053\n",
      "2023-12-30 18:11:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.027 | 0.051\n",
      "2023-12-30 18:11:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.020 | 0.051\n",
      "2023-12-30 18:11:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.024 | 0.054\n",
      "2023-12-30 18:11:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.024 | 0.054\n",
      "2023-12-30 18:11:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.026 | 0.053\n",
      "2023-12-30 18:11:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.018 | 0.054\n",
      "2023-12-30 18:11:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.020 | 0.050\n",
      "2023-12-30 18:11:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.022 | 0.051\n",
      "2023-12-30 18:11:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.010 | 0.050\n",
      "2023-12-30 18:11:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.022 | 0.054\n",
      "2023-12-30 18:11:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.027 | 0.053\n",
      "2023-12-30 18:11:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.026 | 0.054\n",
      "2023-12-30 18:11:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.031 | 0.050\n",
      "2023-12-30 18:11:35 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.029 | 0.053\n",
      "2023-12-30 18:11:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.030 | 0.051\n",
      "2023-12-30 18:11:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.033 | 0.052\n",
      "2023-12-30 18:11:36 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.028 | 0.050\n",
      "2023-12-30 18:11:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.029 | 0.049\n",
      "2023-12-30 18:11:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.028 | 0.053\n",
      "2023-12-30 18:11:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.028 | 0.051\n",
      "2023-12-30 18:11:37 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.025 | 0.054\n",
      "2023-12-30 18:11:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.015 | 0.051\n",
      "2023-12-30 18:11:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.011 | 0.052\n",
      "2023-12-30 18:11:38 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.032 | 0.055\n",
      "2023-12-30 18:11:39 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.026 | 0.050\n",
      "2023-12-30 18:11:39 - INFO     | Early stopping: no decrease (0.051 vs 0.051); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:13<01:11, 10.19s/it]2023-12-30 18:11:39 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 18:11:39 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.033 | 0.051\n",
      "2023-12-30 18:11:40 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.022 | 0.053\n",
      "2023-12-30 18:11:40 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.022 | 0.051\n",
      "2023-12-30 18:11:40 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.022 | 0.052\n",
      "2023-12-30 18:11:41 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.026 | 0.055\n",
      "2023-12-30 18:11:41 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.025 | 0.053\n",
      "2023-12-30 18:11:42 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.021 | 0.052\n",
      "2023-12-30 18:11:42 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.035 | 0.051\n",
      "2023-12-30 18:11:42 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.020 | 0.050\n",
      "2023-12-30 18:11:43 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.025 | 0.051\n",
      "2023-12-30 18:11:43 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.018 | 0.053\n",
      "2023-12-30 18:11:43 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.021 | 0.051\n",
      "2023-12-30 18:11:44 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.031 | 0.052\n",
      "2023-12-30 18:11:44 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.018 | 0.055\n",
      "2023-12-30 18:11:44 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.022 | 0.050\n",
      "2023-12-30 18:11:44 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.017 | 0.053\n",
      "2023-12-30 18:11:45 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.015 | 0.053\n",
      "2023-12-30 18:11:45 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.015 | 0.055\n",
      "2023-12-30 18:11:45 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.017 | 0.055\n",
      "2023-12-30 18:11:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.020 | 0.055\n",
      "2023-12-30 18:11:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.016 | 0.052\n",
      "2023-12-30 18:11:46 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.024 | 0.051\n",
      "2023-12-30 18:11:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.031 | 0.050\n",
      "2023-12-30 18:11:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.026 | 0.055\n",
      "2023-12-30 18:11:47 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.028 | 0.051\n",
      "2023-12-30 18:11:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.019 | 0.054\n",
      "2023-12-30 18:11:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.020 | 0.052\n",
      "2023-12-30 18:11:48 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.019 | 0.054\n",
      "2023-12-30 18:11:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.022 | 0.054\n",
      "2023-12-30 18:11:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.038 | 0.053\n",
      "2023-12-30 18:11:49 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.028 | 0.056\n",
      "2023-12-30 18:11:49 - INFO     | Early stopping: no decrease (0.051 vs 0.052); counter: 3 out of 3\n",
      "2023-12-30 18:11:49 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:11:49 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 70%|███████   | 14/20 [02:23<01:01, 10.26s/it]2023-12-30 18:11:49 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:11:50 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.065 | 0.052\n",
      "2023-12-30 18:11:50 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.011 | 0.052\n",
      "2023-12-30 18:11:50 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.016 | 0.051\n",
      "2023-12-30 18:11:51 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.023 | 0.050\n",
      "2023-12-30 18:11:51 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.020 | 0.051\n",
      "2023-12-30 18:11:51 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.013 | 0.052\n",
      "2023-12-30 18:11:52 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.016 | 0.052\n",
      "2023-12-30 18:11:52 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.023 | 0.053\n",
      "2023-12-30 18:11:52 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.023 | 0.052\n",
      "2023-12-30 18:11:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.018 | 0.050\n",
      "2023-12-30 18:11:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.020 | 0.051\n",
      "2023-12-30 18:11:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.013 | 0.052\n",
      "2023-12-30 18:11:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.008 | 0.051\n",
      "2023-12-30 18:11:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.016 | 0.051\n",
      "2023-12-30 18:11:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.028 | 0.051\n",
      "2023-12-30 18:11:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.024 | 0.051\n",
      "2023-12-30 18:11:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.020 | 0.050\n",
      "2023-12-30 18:11:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.015 | 0.050\n",
      "2023-12-30 18:11:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.019 | 0.049\n",
      "2023-12-30 18:11:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.014 | 0.051\n",
      "2023-12-30 18:11:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.025 | 0.052\n",
      "2023-12-30 18:11:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.015 | 0.051\n",
      "2023-12-30 18:11:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.010 | 0.051\n",
      "2023-12-30 18:11:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.017 | 0.051\n",
      "2023-12-30 18:11:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.024 | 0.052\n",
      "2023-12-30 18:11:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.016 | 0.050\n",
      "2023-12-30 18:11:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.015 | 0.051\n",
      "2023-12-30 18:11:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.027 | 0.051\n",
      "2023-12-30 18:11:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.023 | 0.051\n",
      "2023-12-30 18:12:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.021 | 0.051\n",
      "2023-12-30 18:12:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.017 | 0.051\n",
      "2023-12-30 18:12:00 - INFO     | Early stopping: no decrease (0.051 vs 0.051); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:34<00:51, 10.37s/it]2023-12-30 18:12:00 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:12:00 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.030 | 0.051\n",
      "2023-12-30 18:12:01 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.011 | 0.051\n",
      "2023-12-30 18:12:01 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.008 | 0.052\n",
      "2023-12-30 18:12:02 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.013 | 0.052\n",
      "2023-12-30 18:12:02 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.020 | 0.054\n",
      "2023-12-30 18:12:02 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.014 | 0.054\n",
      "2023-12-30 18:12:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.018 | 0.053\n",
      "2023-12-30 18:12:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.019 | 0.050\n",
      "2023-12-30 18:12:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.016 | 0.050\n",
      "2023-12-30 18:12:03 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.014 | 0.050\n",
      "2023-12-30 18:12:04 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.018 | 0.052\n",
      "2023-12-30 18:12:04 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.021 | 0.051\n",
      "2023-12-30 18:12:04 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.017 | 0.051\n",
      "2023-12-30 18:12:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.018 | 0.052\n",
      "2023-12-30 18:12:05 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.020 | 0.051\n",
      "2023-12-30 18:12:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.017 | 0.052\n",
      "2023-12-30 18:12:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.018 | 0.055\n",
      "2023-12-30 18:12:06 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.024 | 0.052\n",
      "2023-12-30 18:12:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.022 | 0.051\n",
      "2023-12-30 18:12:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.014 | 0.050\n",
      "2023-12-30 18:12:07 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.017 | 0.051\n",
      "2023-12-30 18:12:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.019 | 0.050\n",
      "2023-12-30 18:12:08 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.015 | 0.053\n",
      "2023-12-30 18:12:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.021 | 0.056\n",
      "2023-12-30 18:12:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.020 | 0.052\n",
      "2023-12-30 18:12:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:09 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.013 | 0.052\n",
      "2023-12-30 18:12:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.020 | 0.051\n",
      "2023-12-30 18:12:10 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.024 | 0.050\n",
      "2023-12-30 18:12:10 - INFO     | Early stopping: no decrease (0.051 vs 0.050); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:44<00:41, 10.33s/it]2023-12-30 18:12:10 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:12:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.005 | 0.050\n",
      "2023-12-30 18:12:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.012 | 0.052\n",
      "2023-12-30 18:12:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.020 | 0.051\n",
      "2023-12-30 18:12:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.010 | 0.052\n",
      "2023-12-30 18:12:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.019 | 0.053\n",
      "2023-12-30 18:12:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.016 | 0.050\n",
      "2023-12-30 18:12:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.011 | 0.051\n",
      "2023-12-30 18:12:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.019 | 0.052\n",
      "2023-12-30 18:12:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.011 | 0.050\n",
      "2023-12-30 18:12:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:14 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.022 | 0.051\n",
      "2023-12-30 18:12:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.022 | 0.053\n",
      "2023-12-30 18:12:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.014 | 0.051\n",
      "2023-12-30 18:12:15 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.011 | 0.051\n",
      "2023-12-30 18:12:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.017 | 0.051\n",
      "2023-12-30 18:12:16 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.017 | 0.052\n",
      "2023-12-30 18:12:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.017 | 0.052\n",
      "2023-12-30 18:12:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.016 | 0.053\n",
      "2023-12-30 18:12:17 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.016 | 0.052\n",
      "2023-12-30 18:12:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.014 | 0.051\n",
      "2023-12-30 18:12:18 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.023 | 0.051\n",
      "2023-12-30 18:12:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.015 | 0.050\n",
      "2023-12-30 18:12:19 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.017 | 0.050\n",
      "2023-12-30 18:12:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.032 | 0.053\n",
      "2023-12-30 18:12:20 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.022 | 0.052\n",
      "2023-12-30 18:12:21 - INFO     | Early stopping: no decrease (0.051 vs 0.052); counter: 3 out of 3\n",
      "2023-12-30 18:12:21 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:12:21 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 85%|████████▌ | 17/20 [02:54<00:30, 10.30s/it]2023-12-30 18:12:21 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:12:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.009 | 0.052\n",
      "2023-12-30 18:12:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.012 | 0.052\n",
      "2023-12-30 18:12:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.010 | 0.052\n",
      "2023-12-30 18:12:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.015 | 0.053\n",
      "2023-12-30 18:12:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.020 | 0.052\n",
      "2023-12-30 18:12:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.011 | 0.052\n",
      "2023-12-30 18:12:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.010 | 0.051\n",
      "2023-12-30 18:12:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.016 | 0.052\n",
      "2023-12-30 18:12:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.019 | 0.052\n",
      "2023-12-30 18:12:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:24 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.024 | 0.052\n",
      "2023-12-30 18:12:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.020 | 0.051\n",
      "2023-12-30 18:12:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.011 | 0.051\n",
      "2023-12-30 18:12:25 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.010 | 0.052\n",
      "2023-12-30 18:12:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.010 | 0.050\n",
      "2023-12-30 18:12:26 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.012 | 0.051\n",
      "2023-12-30 18:12:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.016 | 0.050\n",
      "2023-12-30 18:12:27 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.012 | 0.051\n",
      "2023-12-30 18:12:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.024 | 0.051\n",
      "2023-12-30 18:12:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:28 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.019 | 0.051\n",
      "2023-12-30 18:12:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.011 | 0.050\n",
      "2023-12-30 18:12:29 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.014 | 0.050\n",
      "2023-12-30 18:12:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.018 | 0.050\n",
      "2023-12-30 18:12:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.011 | 0.050\n",
      "2023-12-30 18:12:30 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.016 | 0.050\n",
      "2023-12-30 18:12:31 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.018 | 0.053\n",
      "2023-12-30 18:12:31 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.017 | 0.052\n",
      "2023-12-30 18:12:31 - INFO     | Early stopping: no decrease (0.051 vs 0.052); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [03:05<00:20, 10.40s/it]2023-12-30 18:12:31 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 18:12:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.003 | 0.052\n",
      "2023-12-30 18:12:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.015 | 0.052\n",
      "2023-12-30 18:12:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.009 | 0.051\n",
      "2023-12-30 18:12:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.019 | 0.050\n",
      "2023-12-30 18:12:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.015 | 0.050\n",
      "2023-12-30 18:12:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.008 | 0.051\n",
      "2023-12-30 18:12:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.012 | 0.052\n",
      "2023-12-30 18:12:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.030 | 0.052\n",
      "2023-12-30 18:12:34 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.015 | 0.050\n",
      "2023-12-30 18:12:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.012 | 0.051\n",
      "2023-12-30 18:12:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.009 | 0.051\n",
      "2023-12-30 18:12:35 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.012 | 0.052\n",
      "2023-12-30 18:12:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.024 | 0.051\n",
      "2023-12-30 18:12:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:36 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.010 | 0.051\n",
      "2023-12-30 18:12:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.015 | 0.050\n",
      "2023-12-30 18:12:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.008 | 0.051\n",
      "2023-12-30 18:12:37 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.020 | 0.051\n",
      "2023-12-30 18:12:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.012 | 0.051\n",
      "2023-12-30 18:12:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:38 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.017 | 0.050\n",
      "2023-12-30 18:12:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.009 | 0.050\n",
      "2023-12-30 18:12:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.014 | 0.050\n",
      "2023-12-30 18:12:39 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.016 | 0.050\n",
      "2023-12-30 18:12:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.009 | 0.050\n",
      "2023-12-30 18:12:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.019 | 0.050\n",
      "2023-12-30 18:12:40 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:41 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:41 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.014 | 0.052\n",
      "2023-12-30 18:12:41 - INFO     | Early stopping: no decrease (0.051 vs 0.052); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [03:15<00:10, 10.32s/it]2023-12-30 18:12:41 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 18:12:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.019 | 0.052\n",
      "2023-12-30 18:12:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.014 | 0.051\n",
      "2023-12-30 18:12:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.007 | 0.051\n",
      "2023-12-30 18:12:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.014 | 0.051\n",
      "2023-12-30 18:12:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.013 | 0.052\n",
      "2023-12-30 18:12:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.012 | 0.052\n",
      "2023-12-30 18:12:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.013 | 0.052\n",
      "2023-12-30 18:12:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.019 | 0.051\n",
      "2023-12-30 18:12:44 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.014 | 0.051\n",
      "2023-12-30 18:12:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.014 | 0.051\n",
      "2023-12-30 18:12:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.012 | 0.051\n",
      "2023-12-30 18:12:45 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.012 | 0.051\n",
      "2023-12-30 18:12:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.014 | 0.052\n",
      "2023-12-30 18:12:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.010 | 0.052\n",
      "2023-12-30 18:12:46 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.022 | 0.051\n",
      "2023-12-30 18:12:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.028 | 0.051\n",
      "2023-12-30 18:12:47 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.017 | 0.051\n",
      "2023-12-30 18:12:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.016 | 0.052\n",
      "2023-12-30 18:12:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.011 | 0.052\n",
      "2023-12-30 18:12:48 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.015 | 0.051\n",
      "2023-12-30 18:12:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:49 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.009 | 0.051\n",
      "2023-12-30 18:12:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.013 | 0.051\n",
      "2023-12-30 18:12:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.015 | 0.052\n",
      "2023-12-30 18:12:50 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.010 | 0.051\n",
      "2023-12-30 18:12:51 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.016 | 0.051\n",
      "2023-12-30 18:12:51 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.018 | 0.051\n",
      "2023-12-30 18:12:51 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.009 | 0.052\n",
      "2023-12-30 18:12:51 - INFO     | Early stopping: no decrease (0.051 vs 0.052); counter: 3 out of 3\n",
      "2023-12-30 18:12:51 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:12:51 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      "100%|██████████| 20/20 [03:25<00:00, 10.27s/it]\n",
      "2023-12-30 18:12:51 - INFO     | Best validation loss: 0.051\n",
      "2023-12-30 18:12:51 - INFO     | Best early stopping index/epoch: 10\n",
      "2023-12-30 18:12:52 - INFO     | Average Loss on test set: 0.057\n",
      "2023-12-30 18:12:54 - INFO     | Weighted Precision: 0.985, Recall: 0.985, F1: 0.985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████████▄▄▄▄▂▂▂▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▃▁▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_validation_loss</td><td>0.05098</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.00879</td></tr><tr><td>step_validation_loss</td><td>0.05176</td></tr><tr><td>test_loss</td><td>0.05726</td></tr><tr><td>weighted_f1</td><td>0.98485</td></tr><tr><td>weighted_precision</td><td>0.9849</td></tr><tr><td>weighted_recall</td><td>0.98486</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-sweep-16</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/buofn5s6' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/buofn5s6</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_180925-buofn5s6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i0f8lyh3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [32, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_181304-i0f8lyh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i0f8lyh3' target=\"_blank\">glowing-sweep-17</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i0f8lyh3' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i0f8lyh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [32, 64], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:13:05 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 18:13:05 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 22.205 | 34.140\n",
      "2023-12-30 18:13:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 9.329 | 0.684\n",
      "2023-12-30 18:13:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.460 | 0.301\n",
      "2023-12-30 18:13:06 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.238 | 0.237\n",
      "2023-12-30 18:13:07 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.213 | 0.198\n",
      "2023-12-30 18:13:07 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.171 | 0.161\n",
      "2023-12-30 18:13:07 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.160 | 0.171\n",
      "2023-12-30 18:13:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.179 | 0.140\n",
      "2023-12-30 18:13:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.133 | 0.135\n",
      "2023-12-30 18:13:08 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.145 | 0.166\n",
      "2023-12-30 18:13:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.119 | 0.140\n",
      "2023-12-30 18:13:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.150 | 0.151\n",
      "2023-12-30 18:13:09 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.119 | 0.121\n",
      "2023-12-30 18:13:10 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.120 | 0.111\n",
      "2023-12-30 18:13:10 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.098 | 0.129\n",
      "2023-12-30 18:13:10 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.099 | 0.114\n",
      "2023-12-30 18:13:11 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.090 | 0.122\n",
      "2023-12-30 18:13:11 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.126 | 0.120\n",
      "2023-12-30 18:13:11 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.138 | 0.138\n",
      "2023-12-30 18:13:12 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.146 | 0.113\n",
      "2023-12-30 18:13:12 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.120 | 0.110\n",
      "2023-12-30 18:13:12 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.094 | 0.114\n",
      "2023-12-30 18:13:13 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.131 | 0.115\n",
      "2023-12-30 18:13:13 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.109 | 0.121\n",
      "2023-12-30 18:13:13 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.087 | 0.095\n",
      "2023-12-30 18:13:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.126 | 0.100\n",
      "2023-12-30 18:13:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.094 | 0.095\n",
      "2023-12-30 18:13:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.115 | 0.107\n",
      "2023-12-30 18:13:14 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.124 | 0.089\n",
      "2023-12-30 18:13:15 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.106 | 0.096\n",
      "2023-12-30 18:13:15 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.111 | 0.103\n",
      "2023-12-30 18:13:15 - INFO     | Early stopping: loss decreased (inf -> 0.118; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:15, 10.30s/it]2023-12-30 18:13:15 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 18:13:16 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.107 | 0.121\n",
      "2023-12-30 18:13:16 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.092 | 0.104\n",
      "2023-12-30 18:13:16 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.076 | 0.091\n",
      "2023-12-30 18:13:17 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.073 | 0.084\n",
      "2023-12-30 18:13:17 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.074 | 0.103\n",
      "2023-12-30 18:13:17 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.083 | 0.087\n",
      "2023-12-30 18:13:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.091 | 0.128\n",
      "2023-12-30 18:13:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.094 | 0.108\n",
      "2023-12-30 18:13:18 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.103 | 0.092\n",
      "2023-12-30 18:13:19 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.080 | 0.104\n",
      "2023-12-30 18:13:19 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.066 | 0.090\n",
      "2023-12-30 18:13:19 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.110 | 0.160\n",
      "2023-12-30 18:13:20 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.124 | 0.100\n",
      "2023-12-30 18:13:20 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.079 | 0.090\n",
      "2023-12-30 18:13:21 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.099 | 0.087\n",
      "2023-12-30 18:13:21 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.063 | 0.077\n",
      "2023-12-30 18:13:21 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.078 | 0.077\n",
      "2023-12-30 18:13:22 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.102 | 0.081\n",
      "2023-12-30 18:13:22 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.080 | 0.087\n",
      "2023-12-30 18:13:22 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.077 | 0.068\n",
      "2023-12-30 18:13:23 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.054 | 0.080\n",
      "2023-12-30 18:13:23 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.075 | 0.075\n",
      "2023-12-30 18:13:23 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.061 | 0.084\n",
      "2023-12-30 18:13:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.073 | 0.089\n",
      "2023-12-30 18:13:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.082 | 0.083\n",
      "2023-12-30 18:13:24 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.060 | 0.081\n",
      "2023-12-30 18:13:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.077 | 0.094\n",
      "2023-12-30 18:13:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.085 | 0.108\n",
      "2023-12-30 18:13:25 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.092 | 0.081\n",
      "2023-12-30 18:13:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.086 | 0.075\n",
      "2023-12-30 18:13:26 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.064 | 0.093\n",
      "2023-12-30 18:13:26 - INFO     | Early stopping: loss decreased (0.118 -> 0.103; -13.2%). Caching model state.\n",
      " 10%|█         | 2/20 [00:21<03:11, 10.66s/it]2023-12-30 18:13:26 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 18:13:27 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.030 | 0.105\n",
      "2023-12-30 18:13:27 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.056 | 0.078\n",
      "2023-12-30 18:13:27 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.057 | 0.074\n",
      "2023-12-30 18:13:28 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.055 | 0.066\n",
      "2023-12-30 18:13:28 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.073 | 0.102\n",
      "2023-12-30 18:13:28 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.096 | 0.085\n",
      "2023-12-30 18:13:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.051 | 0.091\n",
      "2023-12-30 18:13:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.061 | 0.077\n",
      "2023-12-30 18:13:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.068 | 0.087\n",
      "2023-12-30 18:13:29 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.073 | 0.074\n",
      "2023-12-30 18:13:30 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.045 | 0.075\n",
      "2023-12-30 18:13:30 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.068 | 0.074\n",
      "2023-12-30 18:13:30 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.066 | 0.070\n",
      "2023-12-30 18:13:31 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.067 | 0.077\n",
      "2023-12-30 18:13:31 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.065 | 0.076\n",
      "2023-12-30 18:13:31 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.056 | 0.073\n",
      "2023-12-30 18:13:32 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.048 | 0.094\n",
      "2023-12-30 18:13:32 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.061 | 0.080\n",
      "2023-12-30 18:13:32 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.067 | 0.096\n",
      "2023-12-30 18:13:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.063 | 0.076\n",
      "2023-12-30 18:13:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.071 | 0.073\n",
      "2023-12-30 18:13:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.059 | 0.081\n",
      "2023-12-30 18:13:33 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.055 | 0.113\n",
      "2023-12-30 18:13:34 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.082 | 0.096\n",
      "2023-12-30 18:13:34 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.071 | 0.073\n",
      "2023-12-30 18:13:34 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.076 | 0.093\n",
      "2023-12-30 18:13:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.099 | 0.084\n",
      "2023-12-30 18:13:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.072 | 0.080\n",
      "2023-12-30 18:13:35 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.082 | 0.082\n",
      "2023-12-30 18:13:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.062 | 0.079\n",
      "2023-12-30 18:13:36 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.045 | 0.072\n",
      "2023-12-30 18:13:36 - INFO     | Early stopping: loss decreased (0.103 -> 0.080; -22.2%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:31<02:55, 10.33s/it]2023-12-30 18:13:36 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 18:13:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.003 | 0.083\n",
      "2023-12-30 18:13:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.043 | 0.092\n",
      "2023-12-30 18:13:37 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.060 | 0.090\n",
      "2023-12-30 18:13:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.060 | 0.083\n",
      "2023-12-30 18:13:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.093 | 0.076\n",
      "2023-12-30 18:13:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.040 | 0.078\n",
      "2023-12-30 18:13:38 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.037 | 0.070\n",
      "2023-12-30 18:13:39 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.051 | 0.075\n",
      "2023-12-30 18:13:39 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.049 | 0.081\n",
      "2023-12-30 18:13:39 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.048 | 0.077\n",
      "2023-12-30 18:13:40 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.043 | 0.081\n",
      "2023-12-30 18:13:40 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.050 | 0.076\n",
      "2023-12-30 18:13:40 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.047 | 0.080\n",
      "2023-12-30 18:13:41 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.058 | 0.095\n",
      "2023-12-30 18:13:41 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.066 | 0.075\n",
      "2023-12-30 18:13:41 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.049 | 0.068\n",
      "2023-12-30 18:13:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.048 | 0.076\n",
      "2023-12-30 18:13:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.065 | 0.075\n",
      "2023-12-30 18:13:42 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.032 | 0.076\n",
      "2023-12-30 18:13:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.083 | 0.076\n",
      "2023-12-30 18:13:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.041 | 0.087\n",
      "2023-12-30 18:13:43 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.052 | 0.075\n",
      "2023-12-30 18:13:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.059 | 0.082\n",
      "2023-12-30 18:13:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.058 | 0.077\n",
      "2023-12-30 18:13:44 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.059 | 0.077\n",
      "2023-12-30 18:13:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.069 | 0.097\n",
      "2023-12-30 18:13:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.062 | 0.092\n",
      "2023-12-30 18:13:45 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.052 | 0.090\n",
      "2023-12-30 18:13:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.051 | 0.091\n",
      "2023-12-30 18:13:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.068 | 0.088\n",
      "2023-12-30 18:13:46 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.055 | 0.075\n",
      "2023-12-30 18:13:46 - INFO     | Early stopping: no decrease (0.080 vs 0.078); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:41<02:44, 10.29s/it]2023-12-30 18:13:46 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 18:13:47 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.032 | 0.076\n",
      "2023-12-30 18:13:47 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.035 | 0.074\n",
      "2023-12-30 18:13:47 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.053 | 0.067\n",
      "2023-12-30 18:13:48 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.048 | 0.076\n",
      "2023-12-30 18:13:48 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.050 | 0.072\n",
      "2023-12-30 18:13:48 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.039 | 0.066\n",
      "2023-12-30 18:13:49 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.056 | 0.067\n",
      "2023-12-30 18:13:49 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.061 | 0.070\n",
      "2023-12-30 18:13:49 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.044 | 0.091\n",
      "2023-12-30 18:13:50 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.055 | 0.091\n",
      "2023-12-30 18:13:50 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.057 | 0.072\n",
      "2023-12-30 18:13:50 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.051 | 0.078\n",
      "2023-12-30 18:13:51 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.046 | 0.081\n",
      "2023-12-30 18:13:51 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.063 | 0.081\n",
      "2023-12-30 18:13:51 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.034 | 0.075\n",
      "2023-12-30 18:13:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.056 | 0.072\n",
      "2023-12-30 18:13:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.053 | 0.082\n",
      "2023-12-30 18:13:52 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.061 | 0.075\n",
      "2023-12-30 18:13:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.045 | 0.071\n",
      "2023-12-30 18:13:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.049 | 0.106\n",
      "2023-12-30 18:13:53 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.040 | 0.094\n",
      "2023-12-30 18:13:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.062 | 0.091\n",
      "2023-12-30 18:13:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.055 | 0.097\n",
      "2023-12-30 18:13:54 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.060 | 0.092\n",
      "2023-12-30 18:13:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.036 | 0.102\n",
      "2023-12-30 18:13:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.069 | 0.108\n",
      "2023-12-30 18:13:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.046 | 0.102\n",
      "2023-12-30 18:13:55 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.090 | 0.073\n",
      "2023-12-30 18:13:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.045 | 0.103\n",
      "2023-12-30 18:13:56 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.050 | 0.086\n",
      "2023-12-30 18:13:57 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.054 | 0.086\n",
      "2023-12-30 18:13:57 - INFO     | Early stopping: no decrease (0.080 vs 0.076); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:51<02:34, 10.29s/it]2023-12-30 18:13:57 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 18:13:57 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.026 | 0.075\n",
      "2023-12-30 18:13:57 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.040 | 0.075\n",
      "2023-12-30 18:13:58 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.049 | 0.068\n",
      "2023-12-30 18:13:58 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.030 | 0.082\n",
      "2023-12-30 18:13:58 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.043 | 0.080\n",
      "2023-12-30 18:13:59 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.055 | 0.086\n",
      "2023-12-30 18:13:59 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.035 | 0.071\n",
      "2023-12-30 18:13:59 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.051 | 0.084\n",
      "2023-12-30 18:14:00 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.043 | 0.077\n",
      "2023-12-30 18:14:00 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.029 | 0.091\n",
      "2023-12-30 18:14:00 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.044 | 0.099\n",
      "2023-12-30 18:14:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.044 | 0.075\n",
      "2023-12-30 18:14:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.051 | 0.077\n",
      "2023-12-30 18:14:01 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.051 | 0.068\n",
      "2023-12-30 18:14:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.063 | 0.071\n",
      "2023-12-30 18:14:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.042 | 0.078\n",
      "2023-12-30 18:14:02 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.045 | 0.063\n",
      "2023-12-30 18:14:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.053 | 0.073\n",
      "2023-12-30 18:14:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.031 | 0.067\n",
      "2023-12-30 18:14:03 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.043 | 0.069\n",
      "2023-12-30 18:14:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.041 | 0.071\n",
      "2023-12-30 18:14:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.032 | 0.074\n",
      "2023-12-30 18:14:04 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.053 | 0.067\n",
      "2023-12-30 18:14:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.045 | 0.077\n",
      "2023-12-30 18:14:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.048 | 0.069\n",
      "2023-12-30 18:14:05 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.060 | 0.078\n",
      "2023-12-30 18:14:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.041 | 0.081\n",
      "2023-12-30 18:14:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.035 | 0.073\n",
      "2023-12-30 18:14:06 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.043 | 0.095\n",
      "2023-12-30 18:14:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.053 | 0.092\n",
      "2023-12-30 18:14:07 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.053 | 0.082\n",
      "2023-12-30 18:14:07 - INFO     | Early stopping: no decrease (0.080 vs 0.080); counter: 3 out of 3\n",
      "2023-12-30 18:14:07 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:14:07 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 30%|███       | 6/20 [01:02<02:24, 10.31s/it]2023-12-30 18:14:07 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 18:14:07 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.015 | 0.075\n",
      "2023-12-30 18:14:08 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.026 | 0.075\n",
      "2023-12-30 18:14:08 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.031 | 0.078\n",
      "2023-12-30 18:14:08 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.021 | 0.077\n",
      "2023-12-30 18:14:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.035 | 0.071\n",
      "2023-12-30 18:14:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.015 | 0.067\n",
      "2023-12-30 18:14:09 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.023 | 0.072\n",
      "2023-12-30 18:14:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.036 | 0.064\n",
      "2023-12-30 18:14:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.021 | 0.062\n",
      "2023-12-30 18:14:10 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.034 | 0.067\n",
      "2023-12-30 18:14:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.012 | 0.065\n",
      "2023-12-30 18:14:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.025 | 0.064\n",
      "2023-12-30 18:14:11 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.029 | 0.067\n",
      "2023-12-30 18:14:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.020 | 0.071\n",
      "2023-12-30 18:14:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.029 | 0.064\n",
      "2023-12-30 18:14:12 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.023 | 0.072\n",
      "2023-12-30 18:14:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.019 | 0.071\n",
      "2023-12-30 18:14:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.026 | 0.069\n",
      "2023-12-30 18:14:13 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.027 | 0.068\n",
      "2023-12-30 18:14:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.021 | 0.068\n",
      "2023-12-30 18:14:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.025 | 0.074\n",
      "2023-12-30 18:14:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.042 | 0.074\n",
      "2023-12-30 18:14:14 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.038 | 0.057\n",
      "2023-12-30 18:14:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.019 | 0.054\n",
      "2023-12-30 18:14:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.020 | 0.059\n",
      "2023-12-30 18:14:15 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.032 | 0.065\n",
      "2023-12-30 18:14:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.034 | 0.066\n",
      "2023-12-30 18:14:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.032 | 0.063\n",
      "2023-12-30 18:14:16 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.022 | 0.064\n",
      "2023-12-30 18:14:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.010 | 0.061\n",
      "2023-12-30 18:14:17 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.023 | 0.066\n",
      "2023-12-30 18:14:17 - INFO     | Early stopping: loss decreased (0.080 -> 0.066; -16.7%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:12<02:13, 10.23s/it]2023-12-30 18:14:17 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 18:14:18 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.002 | 0.067\n",
      "2023-12-30 18:14:18 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.011 | 0.068\n",
      "2023-12-30 18:14:18 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.013 | 0.071\n",
      "2023-12-30 18:14:18 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.012 | 0.073\n",
      "2023-12-30 18:14:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.016 | 0.080\n",
      "2023-12-30 18:14:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.026 | 0.070\n",
      "2023-12-30 18:14:19 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.019 | 0.075\n",
      "2023-12-30 18:14:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.024 | 0.070\n",
      "2023-12-30 18:14:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.009 | 0.064\n",
      "2023-12-30 18:14:20 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.004 | 0.066\n",
      "2023-12-30 18:14:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.021 | 0.069\n",
      "2023-12-30 18:14:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.017 | 0.073\n",
      "2023-12-30 18:14:21 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.014 | 0.088\n",
      "2023-12-30 18:14:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.013 | 0.078\n",
      "2023-12-30 18:14:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.032 | 0.064\n",
      "2023-12-30 18:14:22 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.018 | 0.066\n",
      "2023-12-30 18:14:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.008 | 0.069\n",
      "2023-12-30 18:14:23 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.025 | 0.063\n",
      "2023-12-30 18:14:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.009 | 0.068\n",
      "2023-12-30 18:14:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.013 | 0.068\n",
      "2023-12-30 18:14:24 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.013 | 0.070\n",
      "2023-12-30 18:14:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.015 | 0.069\n",
      "2023-12-30 18:14:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.024 | 0.071\n",
      "2023-12-30 18:14:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.016 | 0.064\n",
      "2023-12-30 18:14:25 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.018 | 0.062\n",
      "2023-12-30 18:14:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.012 | 0.068\n",
      "2023-12-30 18:14:26 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.020 | 0.064\n",
      "2023-12-30 18:14:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.021 | 0.066\n",
      "2023-12-30 18:14:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.022 | 0.062\n",
      "2023-12-30 18:14:27 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.018 | 0.060\n",
      "2023-12-30 18:14:28 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.020 | 0.062\n",
      "2023-12-30 18:14:28 - INFO     | Early stopping: loss decreased (0.066 -> 0.062; -6.6%). Caching model state.\n",
      " 40%|████      | 8/20 [01:22<02:04, 10.37s/it]2023-12-30 18:14:28 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 18:14:28 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.007 | 0.062\n",
      "2023-12-30 18:14:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.012 | 0.071\n",
      "2023-12-30 18:14:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.010 | 0.065\n",
      "2023-12-30 18:14:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.005 | 0.068\n",
      "2023-12-30 18:14:29 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.009 | 0.062\n",
      "2023-12-30 18:14:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.005 | 0.065\n",
      "2023-12-30 18:14:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.026 | 0.071\n",
      "2023-12-30 18:14:30 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.012 | 0.064\n",
      "2023-12-30 18:14:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.008 | 0.068\n",
      "2023-12-30 18:14:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.008 | 0.069\n",
      "2023-12-30 18:14:31 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.008 | 0.074\n",
      "2023-12-30 18:14:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.007 | 0.075\n",
      "2023-12-30 18:14:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.012 | 0.070\n",
      "2023-12-30 18:14:32 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.014 | 0.081\n",
      "2023-12-30 18:14:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.022 | 0.081\n",
      "2023-12-30 18:14:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.010 | 0.084\n",
      "2023-12-30 18:14:33 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.013 | 0.095\n",
      "2023-12-30 18:14:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.010 | 0.087\n",
      "2023-12-30 18:14:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.006 | 0.084\n",
      "2023-12-30 18:14:34 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.029 | 0.074\n",
      "2023-12-30 18:14:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.033 | 0.086\n",
      "2023-12-30 18:14:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.030 | 0.105\n",
      "2023-12-30 18:14:35 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.024 | 0.088\n",
      "2023-12-30 18:14:36 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.013 | 0.079\n",
      "2023-12-30 18:14:36 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.015 | 0.069\n",
      "2023-12-30 18:14:37 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.019 | 0.075\n",
      "2023-12-30 18:14:37 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.022 | 0.079\n",
      "2023-12-30 18:14:37 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.016 | 0.070\n",
      "2023-12-30 18:14:38 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.039 | 0.087\n",
      "2023-12-30 18:14:38 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.012 | 0.085\n",
      "2023-12-30 18:14:38 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.031 | 0.075\n",
      "2023-12-30 18:14:38 - INFO     | Early stopping: no decrease (0.062 vs 0.077); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:33<01:54, 10.42s/it]2023-12-30 18:14:38 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 18:14:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.002 | 0.077\n",
      "2023-12-30 18:14:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.005 | 0.080\n",
      "2023-12-30 18:14:39 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.003 | 0.082\n",
      "2023-12-30 18:14:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.007 | 0.084\n",
      "2023-12-30 18:14:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.019 | 0.082\n",
      "2023-12-30 18:14:40 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.007 | 0.074\n",
      "2023-12-30 18:14:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.015 | 0.083\n",
      "2023-12-30 18:14:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.010 | 0.083\n",
      "2023-12-30 18:14:41 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.025 | 0.087\n",
      "2023-12-30 18:14:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.024 | 0.093\n",
      "2023-12-30 18:14:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.019 | 0.073\n",
      "2023-12-30 18:14:42 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.009 | 0.080\n",
      "2023-12-30 18:14:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.014 | 0.086\n",
      "2023-12-30 18:14:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.008 | 0.074\n",
      "2023-12-30 18:14:43 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.011 | 0.085\n",
      "2023-12-30 18:14:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.009 | 0.082\n",
      "2023-12-30 18:14:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.007 | 0.077\n",
      "2023-12-30 18:14:44 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.010 | 0.081\n",
      "2023-12-30 18:14:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.009 | 0.078\n",
      "2023-12-30 18:14:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.008 | 0.073\n",
      "2023-12-30 18:14:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.017 | 0.079\n",
      "2023-12-30 18:14:45 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.010 | 0.079\n",
      "2023-12-30 18:14:46 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.021 | 0.073\n",
      "2023-12-30 18:14:46 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.019 | 0.073\n",
      "2023-12-30 18:14:46 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.008 | 0.072\n",
      "2023-12-30 18:14:47 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.007 | 0.078\n",
      "2023-12-30 18:14:47 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.008 | 0.073\n",
      "2023-12-30 18:14:47 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.010 | 0.072\n",
      "2023-12-30 18:14:48 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.016 | 0.082\n",
      "2023-12-30 18:14:48 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.016 | 0.088\n",
      "2023-12-30 18:14:48 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.015 | 0.086\n",
      "2023-12-30 18:14:49 - INFO     | Early stopping: no decrease (0.062 vs 0.080); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:43<01:43, 10.35s/it]2023-12-30 18:14:49 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 18:14:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.000 | 0.080\n",
      "2023-12-30 18:14:49 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.009 | 0.086\n",
      "2023-12-30 18:14:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.013 | 0.077\n",
      "2023-12-30 18:14:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.004 | 0.086\n",
      "2023-12-30 18:14:50 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.018 | 0.077\n",
      "2023-12-30 18:14:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.011 | 0.082\n",
      "2023-12-30 18:14:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.010 | 0.091\n",
      "2023-12-30 18:14:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.021 | 0.075\n",
      "2023-12-30 18:14:51 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.016 | 0.088\n",
      "2023-12-30 18:14:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.010 | 0.090\n",
      "2023-12-30 18:14:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.013 | 0.080\n",
      "2023-12-30 18:14:52 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.004 | 0.084\n",
      "2023-12-30 18:14:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.007 | 0.094\n",
      "2023-12-30 18:14:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.004 | 0.081\n",
      "2023-12-30 18:14:53 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.014 | 0.081\n",
      "2023-12-30 18:14:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.007 | 0.090\n",
      "2023-12-30 18:14:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.014 | 0.083\n",
      "2023-12-30 18:14:54 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.004 | 0.081\n",
      "2023-12-30 18:14:55 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.009 | 0.074\n",
      "2023-12-30 18:14:55 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.012 | 0.084\n",
      "2023-12-30 18:14:55 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.006 | 0.085\n",
      "2023-12-30 18:14:56 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.013 | 0.083\n",
      "2023-12-30 18:14:56 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.021 | 0.087\n",
      "2023-12-30 18:14:56 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.022 | 0.079\n",
      "2023-12-30 18:14:57 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.017 | 0.083\n",
      "2023-12-30 18:14:57 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.013 | 0.083\n",
      "2023-12-30 18:14:57 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.011 | 0.091\n",
      "2023-12-30 18:14:58 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.004 | 0.099\n",
      "2023-12-30 18:14:58 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.016 | 0.095\n",
      "2023-12-30 18:14:58 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.011 | 0.084\n",
      "2023-12-30 18:14:59 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.013 | 0.079\n",
      "2023-12-30 18:14:59 - INFO     | Early stopping: no decrease (0.062 vs 0.080); counter: 3 out of 3\n",
      "2023-12-30 18:14:59 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:14:59 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 55%|█████▌    | 11/20 [01:53<01:32, 10.31s/it]2023-12-30 18:14:59 - INFO     | Epoch: 11 | Learning Rate: 0.000\n",
      "2023-12-30 18:14:59 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 616064 examples: 0.003 | 0.081\n",
      "2023-12-30 18:14:59 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 617920 examples: 0.005 | 0.074\n",
      "2023-12-30 18:15:00 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 619776 examples: 0.002 | 0.079\n",
      "2023-12-30 18:15:00 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 621632 examples: 0.001 | 0.078\n",
      "2023-12-30 18:15:00 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 623488 examples: 0.002 | 0.078\n",
      "2023-12-30 18:15:01 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 625344 examples: 0.009 | 0.087\n",
      "2023-12-30 18:15:01 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 627200 examples: 0.008 | 0.091\n",
      "2023-12-30 18:15:01 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 629056 examples: 0.004 | 0.091\n",
      "2023-12-30 18:15:02 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 630912 examples: 0.005 | 0.086\n",
      "2023-12-30 18:15:02 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 632768 examples: 0.003 | 0.084\n",
      "2023-12-30 18:15:02 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 634624 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:03 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 636480 examples: 0.002 | 0.078\n",
      "2023-12-30 18:15:03 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 638336 examples: 0.004 | 0.082\n",
      "2023-12-30 18:15:03 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 640192 examples: 0.008 | 0.079\n",
      "2023-12-30 18:15:04 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 642048 examples: 0.007 | 0.080\n",
      "2023-12-30 18:15:04 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 643904 examples: 0.005 | 0.079\n",
      "2023-12-30 18:15:04 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 645760 examples: 0.005 | 0.077\n",
      "2023-12-30 18:15:04 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 647616 examples: 0.002 | 0.075\n",
      "2023-12-30 18:15:05 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 649472 examples: 0.005 | 0.079\n",
      "2023-12-30 18:15:05 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 651328 examples: 0.007 | 0.082\n",
      "2023-12-30 18:15:05 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 653184 examples: 0.003 | 0.086\n",
      "2023-12-30 18:15:06 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 655040 examples: 0.011 | 0.087\n",
      "2023-12-30 18:15:06 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 656896 examples: 0.004 | 0.084\n",
      "2023-12-30 18:15:06 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 658752 examples: 0.005 | 0.082\n",
      "2023-12-30 18:15:07 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 660608 examples: 0.003 | 0.080\n",
      "2023-12-30 18:15:07 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 662464 examples: 0.005 | 0.083\n",
      "2023-12-30 18:15:07 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 664320 examples: 0.002 | 0.082\n",
      "2023-12-30 18:15:08 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 666176 examples: 0.004 | 0.087\n",
      "2023-12-30 18:15:08 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 668032 examples: 0.002 | 0.086\n",
      "2023-12-30 18:15:08 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 669888 examples: 0.005 | 0.090\n",
      "2023-12-30 18:15:09 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 671744 examples: 0.006 | 0.082\n",
      "2023-12-30 18:15:09 - INFO     | Early stopping: no decrease (0.062 vs 0.082); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [02:03<01:22, 10.26s/it]2023-12-30 18:15:09 - INFO     | Epoch: 12 | Learning Rate: 0.000\n",
      "2023-12-30 18:15:09 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 672064 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:10 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 673920 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:10 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 675776 examples: 0.003 | 0.086\n",
      "2023-12-30 18:15:10 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 677632 examples: 0.002 | 0.083\n",
      "2023-12-30 18:15:10 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 679488 examples: 0.001 | 0.084\n",
      "2023-12-30 18:15:11 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 681344 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:11 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 683200 examples: 0.001 | 0.081\n",
      "2023-12-30 18:15:11 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 685056 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:12 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 686912 examples: 0.006 | 0.084\n",
      "2023-12-30 18:15:12 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 688768 examples: 0.001 | 0.084\n",
      "2023-12-30 18:15:12 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 690624 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:13 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 692480 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:13 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 694336 examples: 0.000 | 0.081\n",
      "2023-12-30 18:15:13 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 696192 examples: 0.001 | 0.079\n",
      "2023-12-30 18:15:14 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 698048 examples: 0.002 | 0.090\n",
      "2023-12-30 18:15:14 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 699904 examples: 0.001 | 0.082\n",
      "2023-12-30 18:15:14 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 701760 examples: 0.001 | 0.080\n",
      "2023-12-30 18:15:15 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 703616 examples: 0.001 | 0.078\n",
      "2023-12-30 18:15:15 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 705472 examples: 0.001 | 0.080\n",
      "2023-12-30 18:15:15 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 707328 examples: 0.001 | 0.081\n",
      "2023-12-30 18:15:16 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 709184 examples: 0.001 | 0.080\n",
      "2023-12-30 18:15:16 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 711040 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:16 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 712896 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:17 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 714752 examples: 0.000 | 0.081\n",
      "2023-12-30 18:15:17 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 716608 examples: 0.006 | 0.089\n",
      "2023-12-30 18:15:17 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 718464 examples: 0.001 | 0.085\n",
      "2023-12-30 18:15:18 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 720320 examples: 0.003 | 0.084\n",
      "2023-12-30 18:15:18 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 722176 examples: 0.002 | 0.100\n",
      "2023-12-30 18:15:18 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 724032 examples: 0.003 | 0.089\n",
      "2023-12-30 18:15:19 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 725888 examples: 0.003 | 0.088\n",
      "2023-12-30 18:15:19 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 727744 examples: 0.004 | 0.090\n",
      "2023-12-30 18:15:19 - INFO     | Early stopping: no decrease (0.062 vs 0.088); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:14<01:11, 10.28s/it]2023-12-30 18:15:19 - INFO     | Epoch: 13 | Learning Rate: 0.000\n",
      "2023-12-30 18:15:20 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 728064 examples: 0.001 | 0.088\n",
      "2023-12-30 18:15:20 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 729920 examples: 0.004 | 0.082\n",
      "2023-12-30 18:15:20 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 731776 examples: 0.001 | 0.081\n",
      "2023-12-30 18:15:21 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 733632 examples: 0.000 | 0.079\n",
      "2023-12-30 18:15:21 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 735488 examples: 0.000 | 0.079\n",
      "2023-12-30 18:15:21 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 737344 examples: 0.000 | 0.079\n",
      "2023-12-30 18:15:22 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 739200 examples: 0.000 | 0.080\n",
      "2023-12-30 18:15:22 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 741056 examples: 0.000 | 0.081\n",
      "2023-12-30 18:15:22 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 742912 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:22 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 744768 examples: 0.000 | 0.081\n",
      "2023-12-30 18:15:23 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 746624 examples: 0.001 | 0.080\n",
      "2023-12-30 18:15:23 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 748480 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:23 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 750336 examples: 0.001 | 0.084\n",
      "2023-12-30 18:15:24 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 752192 examples: 0.001 | 0.084\n",
      "2023-12-30 18:15:24 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 754048 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:25 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 755904 examples: 0.001 | 0.084\n",
      "2023-12-30 18:15:25 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 757760 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:25 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 759616 examples: 0.001 | 0.086\n",
      "2023-12-30 18:15:26 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 761472 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:26 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 763328 examples: 0.002 | 0.082\n",
      "2023-12-30 18:15:26 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 765184 examples: 0.001 | 0.086\n",
      "2023-12-30 18:15:26 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 767040 examples: 0.000 | 0.087\n",
      "2023-12-30 18:15:27 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 768896 examples: 0.002 | 0.088\n",
      "2023-12-30 18:15:27 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 770752 examples: 0.001 | 0.093\n",
      "2023-12-30 18:15:27 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 772608 examples: 0.001 | 0.092\n",
      "2023-12-30 18:15:28 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 774464 examples: 0.002 | 0.093\n",
      "2023-12-30 18:15:28 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 776320 examples: 0.001 | 0.094\n",
      "2023-12-30 18:15:28 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 778176 examples: 0.002 | 0.102\n",
      "2023-12-30 18:15:29 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 780032 examples: 0.014 | 0.093\n",
      "2023-12-30 18:15:29 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 781888 examples: 0.003 | 0.089\n",
      "2023-12-30 18:15:29 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 783744 examples: 0.001 | 0.090\n",
      "2023-12-30 18:15:30 - INFO     | Early stopping: no decrease (0.062 vs 0.090); counter: 3 out of 3\n",
      "2023-12-30 18:15:30 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:15:30 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      " 70%|███████   | 14/20 [02:24<01:01, 10.27s/it]2023-12-30 18:15:30 - INFO     | Epoch: 14 | Learning Rate: 0.000\n",
      "2023-12-30 18:15:30 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 784064 examples: 0.000 | 0.089\n",
      "2023-12-30 18:15:30 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 785920 examples: 0.001 | 0.088\n",
      "2023-12-30 18:15:30 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 787776 examples: 0.002 | 0.087\n",
      "2023-12-30 18:15:31 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 789632 examples: 0.001 | 0.087\n",
      "2023-12-30 18:15:31 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 791488 examples: 0.001 | 0.085\n",
      "2023-12-30 18:15:32 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 793344 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:32 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 795200 examples: 0.001 | 0.085\n",
      "2023-12-30 18:15:32 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 797056 examples: 0.001 | 0.085\n",
      "2023-12-30 18:15:32 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 798912 examples: 0.001 | 0.087\n",
      "2023-12-30 18:15:33 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 800768 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:33 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 802624 examples: 0.001 | 0.086\n",
      "2023-12-30 18:15:33 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 804480 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:34 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 806336 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:34 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 808192 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:35 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 810048 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:35 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 811904 examples: 0.001 | 0.084\n",
      "2023-12-30 18:15:35 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 813760 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:36 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 815616 examples: 0.000 | 0.081\n",
      "2023-12-30 18:15:36 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 817472 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:36 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 819328 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:37 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 821184 examples: 0.002 | 0.088\n",
      "2023-12-30 18:15:37 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 823040 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:37 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 824896 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:37 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 826752 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:38 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 828608 examples: 0.001 | 0.083\n",
      "2023-12-30 18:15:38 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 830464 examples: 0.001 | 0.081\n",
      "2023-12-30 18:15:38 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 832320 examples: 0.001 | 0.085\n",
      "2023-12-30 18:15:39 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 834176 examples: 0.001 | 0.082\n",
      "2023-12-30 18:15:39 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 836032 examples: 0.000 | 0.087\n",
      "2023-12-30 18:15:39 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 837888 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:40 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 839744 examples: 0.001 | 0.085\n",
      "2023-12-30 18:15:40 - INFO     | Early stopping: no decrease (0.062 vs 0.086); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:34<00:51, 10.32s/it]2023-12-30 18:15:40 - INFO     | Epoch: 15 | Learning Rate: 0.000\n",
      "2023-12-30 18:15:40 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 840064 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:41 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 841920 examples: 0.000 | 0.087\n",
      "2023-12-30 18:15:41 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 843776 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:41 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 845632 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:42 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 847488 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:42 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 849344 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:42 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 851200 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:43 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 853056 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:43 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 854912 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:43 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 856768 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:44 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 858624 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:44 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 860480 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:44 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 862336 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:44 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 864192 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:45 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 866048 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:45 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 867904 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:45 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 869760 examples: 0.000 | 0.082\n",
      "2023-12-30 18:15:46 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 871616 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:46 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 873472 examples: 0.000 | 0.083\n",
      "2023-12-30 18:15:46 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 875328 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:47 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 877184 examples: 0.000 | 0.088\n",
      "2023-12-30 18:15:47 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 879040 examples: 0.000 | 0.088\n",
      "2023-12-30 18:15:47 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 880896 examples: 0.000 | 0.087\n",
      "2023-12-30 18:15:48 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 882752 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:48 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 884608 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:48 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 886464 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:49 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 888320 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:49 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 890176 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:49 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 892032 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:50 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 893888 examples: 0.000 | 0.087\n",
      "2023-12-30 18:15:50 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 895744 examples: 0.000 | 0.086\n",
      "2023-12-30 18:15:50 - INFO     | Early stopping: no decrease (0.062 vs 0.085); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:45<00:41, 10.28s/it]2023-12-30 18:15:50 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 18:15:50 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:51 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:51 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:51 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:52 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:52 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:52 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:53 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:53 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:53 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:54 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:54 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:54 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:55 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:55 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:55 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:56 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:56 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:56 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:57 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.000 | 0.084\n",
      "2023-12-30 18:15:57 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:57 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:57 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:58 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:58 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:58 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:59 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:59 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.000 | 0.085\n",
      "2023-12-30 18:15:59 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.000 | 0.086\n",
      "2023-12-30 18:16:00 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.000 | 0.086\n",
      "2023-12-30 18:16:00 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.000 | 0.086\n",
      "2023-12-30 18:16:00 - INFO     | Early stopping: no decrease (0.062 vs 0.086); counter: 3 out of 3\n",
      "2023-12-30 18:16:00 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:16:00 - INFO     | Reducing learning rate: 0.000125 -> 6.25e-05\n",
      " 85%|████████▌ | 17/20 [02:55<00:30, 10.27s/it]2023-12-30 18:16:00 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 18:16:01 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.086\n",
      "2023-12-30 18:16:01 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.000 | 0.087\n",
      "2023-12-30 18:16:01 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.000 | 0.087\n",
      "2023-12-30 18:16:02 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:02 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:02 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.000 | 0.087\n",
      "2023-12-30 18:16:03 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.000 | 0.087\n",
      "2023-12-30 18:16:03 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:03 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:04 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:04 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:04 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.000 | 0.088\n",
      "2023-12-30 18:16:05 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.000 | 0.089\n",
      "2023-12-30 18:16:05 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.000 | 0.090\n",
      "2023-12-30 18:16:05 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.000 | 0.090\n",
      "2023-12-30 18:16:06 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:06 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:06 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:06 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:07 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:07 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:07 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.000 | 0.090\n",
      "2023-12-30 18:16:08 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.000 | 0.090\n",
      "2023-12-30 18:16:08 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.000 | 0.090\n",
      "2023-12-30 18:16:08 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.000 | 0.089\n",
      "2023-12-30 18:16:09 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.000 | 0.096\n",
      "2023-12-30 18:16:09 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:09 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:10 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:10 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:10 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:11 - INFO     | Early stopping: no decrease (0.062 vs 0.091); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [03:05<00:20, 10.32s/it]2023-12-30 18:16:11 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 18:16:11 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:12 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:12 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:12 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:13 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:13 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:13 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:15 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:15 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:15 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:19 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:20 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:20 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:20 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.000 | 0.091\n",
      "2023-12-30 18:16:21 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:21 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:21 - INFO     | Early stopping: no decrease (0.062 vs 0.092); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [03:16<00:10, 10.36s/it]2023-12-30 18:16:21 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 18:16:22 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:22 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:22 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:23 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:23 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:23 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.092\n",
      "2023-12-30 18:16:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:29 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:29 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:29 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:30 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:31 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:31 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.094\n",
      "2023-12-30 18:16:31 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.093\n",
      "2023-12-30 18:16:32 - INFO     | Early stopping: no decrease (0.062 vs 0.094); counter: 3 out of 3\n",
      "2023-12-30 18:16:32 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:16:32 - INFO     | Reducing learning rate: 6.25e-05 -> 3.125e-05\n",
      "100%|██████████| 20/20 [03:26<00:00, 10.32s/it]\n",
      "2023-12-30 18:16:32 - INFO     | Best validation loss: 0.062\n",
      "2023-12-30 18:16:32 - INFO     | Best early stopping index/epoch: 7\n",
      "2023-12-30 18:16:32 - INFO     | Average Loss on test set: 0.078\n",
      "2023-12-30 18:16:34 - INFO     | Weighted Precision: 0.988, Recall: 0.988, F1: 0.988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▅▄▃▃▃▂▃▃▃▃▂▃▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▆▆▂▂▂▃▂▂▅▂▂▁▂▂▂▁▂▃▂▃▃▂▂▃▃▂▃▃▃▃▃▃▃▃▃▄▄▄▄</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_validation_loss</td><td>0.06213</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>6e-05</td></tr><tr><td>step_learning_rate</td><td>6e-05</td></tr><tr><td>step_training_loss</td><td>0.0</td></tr><tr><td>step_validation_loss</td><td>0.09349</td></tr><tr><td>test_loss</td><td>0.07763</td></tr><tr><td>weighted_f1</td><td>0.98815</td></tr><tr><td>weighted_precision</td><td>0.98817</td></tr><tr><td>weighted_recall</td><td>0.98814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-sweep-17</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i0f8lyh3' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/i0f8lyh3</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_181304-i0f8lyh3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47batujz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [32, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_181644-47batujz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/47batujz' target=\"_blank\">tough-sweep-18</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/47batujz' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/47batujz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [32, 64], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:16:44 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 18:16:44 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 19.099 | 172.169\n",
      "2023-12-30 18:16:45 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 7.454 | 0.651\n",
      "2023-12-30 18:16:45 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.522 | 0.409\n",
      "2023-12-30 18:16:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.335 | 0.289\n",
      "2023-12-30 18:16:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.292 | 0.252\n",
      "2023-12-30 18:16:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.280 | 0.238\n",
      "2023-12-30 18:16:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.241 | 0.204\n",
      "2023-12-30 18:16:47 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.190 | 0.196\n",
      "2023-12-30 18:16:47 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.194 | 0.174\n",
      "2023-12-30 18:16:47 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.174 | 0.176\n",
      "2023-12-30 18:16:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.194 | 0.168\n",
      "2023-12-30 18:16:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.175 | 0.147\n",
      "2023-12-30 18:16:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.129 | 0.149\n",
      "2023-12-30 18:16:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.138 | 0.136\n",
      "2023-12-30 18:16:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.149 | 0.142\n",
      "2023-12-30 18:16:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.107 | 0.122\n",
      "2023-12-30 18:16:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.110 | 0.121\n",
      "2023-12-30 18:16:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.109 | 0.128\n",
      "2023-12-30 18:16:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.126 | 0.128\n",
      "2023-12-30 18:16:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.105 | 0.119\n",
      "2023-12-30 18:16:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.113 | 0.119\n",
      "2023-12-30 18:16:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.094 | 0.111\n",
      "2023-12-30 18:16:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.098 | 0.106\n",
      "2023-12-30 18:16:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.103 | 0.119\n",
      "2023-12-30 18:16:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.106 | 0.115\n",
      "2023-12-30 18:16:53 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.098 | 0.102\n",
      "2023-12-30 18:16:53 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.099 | 0.105\n",
      "2023-12-30 18:16:53 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.088 | 0.102\n",
      "2023-12-30 18:16:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.109 | 0.120\n",
      "2023-12-30 18:16:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.106 | 0.094\n",
      "2023-12-30 18:16:54 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.098 | 0.098\n",
      "2023-12-30 18:16:55 - INFO     | Early stopping: loss decreased (inf -> 0.103; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:16, 10.36s/it]2023-12-30 18:16:55 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 18:16:55 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.086 | 0.099\n",
      "2023-12-30 18:16:55 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.080 | 0.101\n",
      "2023-12-30 18:16:55 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.086 | 0.104\n",
      "2023-12-30 18:16:56 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.085 | 0.099\n",
      "2023-12-30 18:16:56 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.097 | 0.094\n",
      "2023-12-30 18:16:56 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.077 | 0.103\n",
      "2023-12-30 18:16:57 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.086 | 0.093\n",
      "2023-12-30 18:16:57 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.080 | 0.093\n",
      "2023-12-30 18:16:57 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.066 | 0.090\n",
      "2023-12-30 18:16:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.059 | 0.086\n",
      "2023-12-30 18:16:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.068 | 0.090\n",
      "2023-12-30 18:16:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.077 | 0.090\n",
      "2023-12-30 18:16:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.064 | 0.090\n",
      "2023-12-30 18:16:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.054 | 0.081\n",
      "2023-12-30 18:16:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.082 | 0.085\n",
      "2023-12-30 18:17:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.073 | 0.081\n",
      "2023-12-30 18:17:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.076 | 0.080\n",
      "2023-12-30 18:17:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.090 | 0.082\n",
      "2023-12-30 18:17:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.052 | 0.081\n",
      "2023-12-30 18:17:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.058 | 0.078\n",
      "2023-12-30 18:17:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.076 | 0.077\n",
      "2023-12-30 18:17:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.058 | 0.076\n",
      "2023-12-30 18:17:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.074 | 0.078\n",
      "2023-12-30 18:17:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.053 | 0.077\n",
      "2023-12-30 18:17:03 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.068 | 0.081\n",
      "2023-12-30 18:17:03 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.076 | 0.082\n",
      "2023-12-30 18:17:03 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.066 | 0.075\n",
      "2023-12-30 18:17:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.069 | 0.078\n",
      "2023-12-30 18:17:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.068 | 0.076\n",
      "2023-12-30 18:17:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.059 | 0.076\n",
      "2023-12-30 18:17:04 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.064 | 0.077\n",
      "2023-12-30 18:17:05 - INFO     | Early stopping: loss decreased (0.103 -> 0.076; -25.5%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:05, 10.30s/it]2023-12-30 18:17:05 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 18:17:05 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.016 | 0.076\n",
      "2023-12-30 18:17:05 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.058 | 0.072\n",
      "2023-12-30 18:17:06 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.050 | 0.079\n",
      "2023-12-30 18:17:06 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.059 | 0.075\n",
      "2023-12-30 18:17:06 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.057 | 0.074\n",
      "2023-12-30 18:17:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.071 | 0.074\n",
      "2023-12-30 18:17:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.057 | 0.076\n",
      "2023-12-30 18:17:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.053 | 0.077\n",
      "2023-12-30 18:17:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.039 | 0.075\n",
      "2023-12-30 18:17:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.048 | 0.070\n",
      "2023-12-30 18:17:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.051 | 0.073\n",
      "2023-12-30 18:17:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.048 | 0.071\n",
      "2023-12-30 18:17:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.058 | 0.067\n",
      "2023-12-30 18:17:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.048 | 0.073\n",
      "2023-12-30 18:17:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.055 | 0.069\n",
      "2023-12-30 18:17:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.045 | 0.078\n",
      "2023-12-30 18:17:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.050 | 0.076\n",
      "2023-12-30 18:17:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.039 | 0.069\n",
      "2023-12-30 18:17:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.045 | 0.072\n",
      "2023-12-30 18:17:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.058 | 0.074\n",
      "2023-12-30 18:17:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.051 | 0.069\n",
      "2023-12-30 18:17:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.054 | 0.070\n",
      "2023-12-30 18:17:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.073 | 0.072\n",
      "2023-12-30 18:17:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.054 | 0.068\n",
      "2023-12-30 18:17:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.056 | 0.065\n",
      "2023-12-30 18:17:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.040 | 0.067\n",
      "2023-12-30 18:17:13 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.040 | 0.068\n",
      "2023-12-30 18:17:14 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.055 | 0.071\n",
      "2023-12-30 18:17:14 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.054 | 0.068\n",
      "2023-12-30 18:17:14 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.059 | 0.077\n",
      "2023-12-30 18:17:15 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.051 | 0.070\n",
      "2023-12-30 18:17:15 - INFO     | Early stopping: no decrease (0.076 vs 0.075); counter: 1 out of 3\n",
      " 15%|█▌        | 3/20 [00:30<02:55, 10.33s/it]2023-12-30 18:17:15 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 18:17:15 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.016 | 0.073\n",
      "2023-12-30 18:17:16 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.036 | 0.069\n",
      "2023-12-30 18:17:16 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.050 | 0.066\n",
      "2023-12-30 18:17:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.033 | 0.065\n",
      "2023-12-30 18:17:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.033 | 0.065\n",
      "2023-12-30 18:17:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.045 | 0.071\n",
      "2023-12-30 18:17:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.033 | 0.068\n",
      "2023-12-30 18:17:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.032 | 0.067\n",
      "2023-12-30 18:17:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.048 | 0.063\n",
      "2023-12-30 18:17:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.046 | 0.068\n",
      "2023-12-30 18:17:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.070 | 0.067\n",
      "2023-12-30 18:17:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.045 | 0.066\n",
      "2023-12-30 18:17:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.042 | 0.065\n",
      "2023-12-30 18:17:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.047 | 0.075\n",
      "2023-12-30 18:17:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.040 | 0.070\n",
      "2023-12-30 18:17:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.039 | 0.067\n",
      "2023-12-30 18:17:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.046 | 0.069\n",
      "2023-12-30 18:17:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.030 | 0.066\n",
      "2023-12-30 18:17:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.060 | 0.083\n",
      "2023-12-30 18:17:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.043 | 0.065\n",
      "2023-12-30 18:17:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.041 | 0.063\n",
      "2023-12-30 18:17:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.041 | 0.066\n",
      "2023-12-30 18:17:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.046 | 0.065\n",
      "2023-12-30 18:17:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.048 | 0.065\n",
      "2023-12-30 18:17:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.038 | 0.064\n",
      "2023-12-30 18:17:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.040 | 0.080\n",
      "2023-12-30 18:17:24 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.042 | 0.065\n",
      "2023-12-30 18:17:25 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.027 | 0.065\n",
      "2023-12-30 18:17:25 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.059 | 0.069\n",
      "2023-12-30 18:17:25 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.038 | 0.065\n",
      "2023-12-30 18:17:26 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.038 | 0.063\n",
      "2023-12-30 18:17:26 - INFO     | Early stopping: loss decreased (0.076 -> 0.069; -9.3%). Caching model state.\n",
      " 20%|██        | 4/20 [00:41<02:47, 10.48s/it]2023-12-30 18:17:26 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 18:17:26 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.027 | 0.065\n",
      "2023-12-30 18:17:26 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.025 | 0.065\n",
      "2023-12-30 18:17:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.031 | 0.065\n",
      "2023-12-30 18:17:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.028 | 0.061\n",
      "2023-12-30 18:17:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.052 | 0.062\n",
      "2023-12-30 18:17:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.044 | 0.065\n",
      "2023-12-30 18:17:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.027 | 0.062\n",
      "2023-12-30 18:17:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.037 | 0.062\n",
      "2023-12-30 18:17:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.020 | 0.077\n",
      "2023-12-30 18:17:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.043 | 0.060\n",
      "2023-12-30 18:17:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.022 | 0.062\n",
      "2023-12-30 18:17:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.036 | 0.059\n",
      "2023-12-30 18:17:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.029 | 0.061\n",
      "2023-12-30 18:17:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.038 | 0.060\n",
      "2023-12-30 18:17:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.039 | 0.063\n",
      "2023-12-30 18:17:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.035 | 0.060\n",
      "2023-12-30 18:17:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.038 | 0.062\n",
      "2023-12-30 18:17:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.029 | 0.060\n",
      "2023-12-30 18:17:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.046 | 0.059\n",
      "2023-12-30 18:17:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.034 | 0.061\n",
      "2023-12-30 18:17:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.027 | 0.061\n",
      "2023-12-30 18:17:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.034 | 0.065\n",
      "2023-12-30 18:17:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.034 | 0.062\n",
      "2023-12-30 18:17:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.032 | 0.060\n",
      "2023-12-30 18:17:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.027 | 0.063\n",
      "2023-12-30 18:17:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.056 | 0.059\n",
      "2023-12-30 18:17:34 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.034 | 0.057\n",
      "2023-12-30 18:17:35 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.049 | 0.060\n",
      "2023-12-30 18:17:35 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.034 | 0.058\n",
      "2023-12-30 18:17:35 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.040 | 0.057\n",
      "2023-12-30 18:17:36 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.034 | 0.061\n",
      "2023-12-30 18:17:36 - INFO     | Early stopping: loss decreased (0.069 -> 0.061; -11.4%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:51<02:35, 10.39s/it]2023-12-30 18:17:36 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 18:17:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.006 | 0.061\n",
      "2023-12-30 18:17:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.031 | 0.063\n",
      "2023-12-30 18:17:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.024 | 0.056\n",
      "2023-12-30 18:17:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.022 | 0.055\n",
      "2023-12-30 18:17:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.021 | 0.057\n",
      "2023-12-30 18:17:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.029 | 0.058\n",
      "2023-12-30 18:17:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.030 | 0.062\n",
      "2023-12-30 18:17:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.027 | 0.057\n",
      "2023-12-30 18:17:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.037 | 0.059\n",
      "2023-12-30 18:17:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.029 | 0.061\n",
      "2023-12-30 18:17:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.032 | 0.061\n",
      "2023-12-30 18:17:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.029 | 0.061\n",
      "2023-12-30 18:17:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.032 | 0.062\n",
      "2023-12-30 18:17:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.030 | 0.057\n",
      "2023-12-30 18:17:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.042 | 0.063\n",
      "2023-12-30 18:17:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.024 | 0.065\n",
      "2023-12-30 18:17:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.036 | 0.057\n",
      "2023-12-30 18:17:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.033 | 0.066\n",
      "2023-12-30 18:17:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.024 | 0.058\n",
      "2023-12-30 18:17:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.031 | 0.063\n",
      "2023-12-30 18:17:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.032 | 0.059\n",
      "2023-12-30 18:17:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.032 | 0.059\n",
      "2023-12-30 18:17:44 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.035 | 0.058\n",
      "2023-12-30 18:17:44 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.035 | 0.061\n",
      "2023-12-30 18:17:44 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.040 | 0.061\n",
      "2023-12-30 18:17:45 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.030 | 0.056\n",
      "2023-12-30 18:17:45 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.030 | 0.056\n",
      "2023-12-30 18:17:45 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.033 | 0.056\n",
      "2023-12-30 18:17:46 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.033 | 0.056\n",
      "2023-12-30 18:17:46 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.028 | 0.059\n",
      "2023-12-30 18:17:46 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.025 | 0.056\n",
      "2023-12-30 18:17:46 - INFO     | Early stopping: loss decreased (0.061 -> 0.056; -8.7%). Caching model state.\n",
      " 30%|███       | 6/20 [01:02<02:25, 10.40s/it]2023-12-30 18:17:46 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 18:17:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.018 | 0.055\n",
      "2023-12-30 18:17:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.026 | 0.055\n",
      "2023-12-30 18:17:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.023 | 0.057\n",
      "2023-12-30 18:17:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.028 | 0.055\n",
      "2023-12-30 18:17:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.025 | 0.058\n",
      "2023-12-30 18:17:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.028 | 0.055\n",
      "2023-12-30 18:17:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.023 | 0.056\n",
      "2023-12-30 18:17:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.023 | 0.063\n",
      "2023-12-30 18:17:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.024 | 0.060\n",
      "2023-12-30 18:17:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.032 | 0.058\n",
      "2023-12-30 18:17:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.027 | 0.057\n",
      "2023-12-30 18:17:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.025 | 0.055\n",
      "2023-12-30 18:17:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.019 | 0.055\n",
      "2023-12-30 18:17:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.028 | 0.056\n",
      "2023-12-30 18:17:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.024 | 0.056\n",
      "2023-12-30 18:17:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.030 | 0.056\n",
      "2023-12-30 18:17:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.028 | 0.060\n",
      "2023-12-30 18:17:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.033 | 0.056\n",
      "2023-12-30 18:17:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.026 | 0.057\n",
      "2023-12-30 18:17:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.027 | 0.056\n",
      "2023-12-30 18:17:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.025 | 0.054\n",
      "2023-12-30 18:17:54 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.021 | 0.056\n",
      "2023-12-30 18:17:54 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.028 | 0.053\n",
      "2023-12-30 18:17:54 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.023 | 0.056\n",
      "2023-12-30 18:17:55 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.031 | 0.052\n",
      "2023-12-30 18:17:55 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.020 | 0.055\n",
      "2023-12-30 18:17:55 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.033 | 0.061\n",
      "2023-12-30 18:17:56 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.024 | 0.053\n",
      "2023-12-30 18:17:56 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.026 | 0.059\n",
      "2023-12-30 18:17:56 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.032 | 0.056\n",
      "2023-12-30 18:17:57 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.023 | 0.057\n",
      "2023-12-30 18:17:57 - INFO     | Early stopping: no decrease (0.056 vs 0.056); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:12<02:14, 10.37s/it]2023-12-30 18:17:57 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 18:17:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.012 | 0.055\n",
      "2023-12-30 18:17:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.027 | 0.057\n",
      "2023-12-30 18:17:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.018 | 0.055\n",
      "2023-12-30 18:17:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.018 | 0.056\n",
      "2023-12-30 18:17:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.024 | 0.055\n",
      "2023-12-30 18:17:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.032 | 0.057\n",
      "2023-12-30 18:17:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.024 | 0.055\n",
      "2023-12-30 18:18:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.021 | 0.056\n",
      "2023-12-30 18:18:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.015 | 0.057\n",
      "2023-12-30 18:18:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.017 | 0.055\n",
      "2023-12-30 18:18:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.026 | 0.056\n",
      "2023-12-30 18:18:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.049 | 0.054\n",
      "2023-12-30 18:18:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.025 | 0.054\n",
      "2023-12-30 18:18:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.026 | 0.052\n",
      "2023-12-30 18:18:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.013 | 0.053\n",
      "2023-12-30 18:18:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.022 | 0.057\n",
      "2023-12-30 18:18:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.018 | 0.054\n",
      "2023-12-30 18:18:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.023 | 0.059\n",
      "2023-12-30 18:18:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.021 | 0.055\n",
      "2023-12-30 18:18:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.022 | 0.053\n",
      "2023-12-30 18:18:04 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.034 | 0.053\n",
      "2023-12-30 18:18:04 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.015 | 0.052\n",
      "2023-12-30 18:18:04 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.023 | 0.054\n",
      "2023-12-30 18:18:05 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.032 | 0.060\n",
      "2023-12-30 18:18:05 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.029 | 0.053\n",
      "2023-12-30 18:18:06 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.023 | 0.060\n",
      "2023-12-30 18:18:06 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.018 | 0.053\n",
      "2023-12-30 18:18:06 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.016 | 0.052\n",
      "2023-12-30 18:18:07 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.031 | 0.053\n",
      "2023-12-30 18:18:07 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.021 | 0.053\n",
      "2023-12-30 18:18:07 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.017 | 0.052\n",
      "2023-12-30 18:18:08 - INFO     | Early stopping: no decrease (0.056 vs 0.054); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:23<02:06, 10.50s/it]2023-12-30 18:18:08 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 18:18:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.004 | 0.054\n",
      "2023-12-30 18:18:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.019 | 0.053\n",
      "2023-12-30 18:18:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.019 | 0.052\n",
      "2023-12-30 18:18:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.016 | 0.053\n",
      "2023-12-30 18:18:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.018 | 0.052\n",
      "2023-12-30 18:18:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.020 | 0.053\n",
      "2023-12-30 18:18:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.018 | 0.052\n",
      "2023-12-30 18:18:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.019 | 0.051\n",
      "2023-12-30 18:18:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.013 | 0.052\n",
      "2023-12-30 18:18:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.032 | 0.055\n",
      "2023-12-30 18:18:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.011 | 0.054\n",
      "2023-12-30 18:18:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.020 | 0.057\n",
      "2023-12-30 18:18:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.024 | 0.055\n",
      "2023-12-30 18:18:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.021 | 0.053\n",
      "2023-12-30 18:18:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.017 | 0.054\n",
      "2023-12-30 18:18:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.022 | 0.054\n",
      "2023-12-30 18:18:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.016 | 0.054\n",
      "2023-12-30 18:18:14 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.027 | 0.053\n",
      "2023-12-30 18:18:14 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.021 | 0.054\n",
      "2023-12-30 18:18:14 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.023 | 0.061\n",
      "2023-12-30 18:18:15 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.027 | 0.053\n",
      "2023-12-30 18:18:15 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.020 | 0.054\n",
      "2023-12-30 18:18:15 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.025 | 0.056\n",
      "2023-12-30 18:18:16 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.012 | 0.053\n",
      "2023-12-30 18:18:16 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.016 | 0.054\n",
      "2023-12-30 18:18:16 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.030 | 0.053\n",
      "2023-12-30 18:18:17 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.027 | 0.054\n",
      "2023-12-30 18:18:17 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.013 | 0.052\n",
      "2023-12-30 18:18:17 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.021 | 0.056\n",
      "2023-12-30 18:18:18 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.021 | 0.052\n",
      "2023-12-30 18:18:18 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.024 | 0.054\n",
      "2023-12-30 18:18:18 - INFO     | Early stopping: loss decreased (0.056 -> 0.051; -9.7%). Caching model state.\n",
      " 45%|████▌     | 9/20 [01:34<01:55, 10.52s/it]2023-12-30 18:18:18 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 18:18:18 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.015 | 0.052\n",
      "2023-12-30 18:18:19 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.016 | 0.051\n",
      "2023-12-30 18:18:19 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.013 | 0.053\n",
      "2023-12-30 18:18:19 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.018 | 0.054\n",
      "2023-12-30 18:18:20 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.018 | 0.053\n",
      "2023-12-30 18:18:20 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.012 | 0.050\n",
      "2023-12-30 18:18:20 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.012 | 0.054\n",
      "2023-12-30 18:18:21 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.010 | 0.056\n",
      "2023-12-30 18:18:21 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.012 | 0.056\n",
      "2023-12-30 18:18:21 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.018 | 0.054\n",
      "2023-12-30 18:18:22 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.017 | 0.053\n",
      "2023-12-30 18:18:22 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.031 | 0.056\n",
      "2023-12-30 18:18:22 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.016 | 0.057\n",
      "2023-12-30 18:18:23 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.022 | 0.055\n",
      "2023-12-30 18:18:23 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.020 | 0.057\n",
      "2023-12-30 18:18:23 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.013 | 0.055\n",
      "2023-12-30 18:18:24 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.015 | 0.056\n",
      "2023-12-30 18:18:24 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.014 | 0.053\n",
      "2023-12-30 18:18:24 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.016 | 0.054\n",
      "2023-12-30 18:18:25 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.017 | 0.051\n",
      "2023-12-30 18:18:25 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.017 | 0.052\n",
      "2023-12-30 18:18:25 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.015 | 0.054\n",
      "2023-12-30 18:18:26 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.017 | 0.052\n",
      "2023-12-30 18:18:26 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.013 | 0.052\n",
      "2023-12-30 18:18:26 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.035 | 0.055\n",
      "2023-12-30 18:18:26 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.015 | 0.052\n",
      "2023-12-30 18:18:27 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.015 | 0.053\n",
      "2023-12-30 18:18:27 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.036 | 0.050\n",
      "2023-12-30 18:18:27 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.024 | 0.054\n",
      "2023-12-30 18:18:28 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.016 | 0.049\n",
      "2023-12-30 18:18:28 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.027 | 0.051\n",
      "2023-12-30 18:18:28 - INFO     | Early stopping: no decrease (0.051 vs 0.051); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:44<01:43, 10.40s/it]2023-12-30 18:18:28 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 18:18:29 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.035 | 0.051\n",
      "2023-12-30 18:18:29 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.012 | 0.049\n",
      "2023-12-30 18:18:29 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.015 | 0.050\n",
      "2023-12-30 18:18:29 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.012 | 0.053\n",
      "2023-12-30 18:18:30 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:30 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.014 | 0.050\n",
      "2023-12-30 18:18:30 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.015 | 0.050\n",
      "2023-12-30 18:18:31 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.014 | 0.053\n",
      "2023-12-30 18:18:31 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.013 | 0.051\n",
      "2023-12-30 18:18:31 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.017 | 0.052\n",
      "2023-12-30 18:18:32 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.021 | 0.053\n",
      "2023-12-30 18:18:32 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.013 | 0.054\n",
      "2023-12-30 18:18:32 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.015 | 0.053\n",
      "2023-12-30 18:18:33 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.013 | 0.051\n",
      "2023-12-30 18:18:33 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.018 | 0.052\n",
      "2023-12-30 18:18:33 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.012 | 0.054\n",
      "2023-12-30 18:18:34 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.011 | 0.058\n",
      "2023-12-30 18:18:34 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.011 | 0.050\n",
      "2023-12-30 18:18:34 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.014 | 0.049\n",
      "2023-12-30 18:18:35 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.018 | 0.050\n",
      "2023-12-30 18:18:35 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.014 | 0.050\n",
      "2023-12-30 18:18:35 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.019 | 0.058\n",
      "2023-12-30 18:18:35 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.026 | 0.053\n",
      "2023-12-30 18:18:36 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.033 | 0.057\n",
      "2023-12-30 18:18:36 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.021 | 0.049\n",
      "2023-12-30 18:18:36 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.017 | 0.051\n",
      "2023-12-30 18:18:37 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.017 | 0.055\n",
      "2023-12-30 18:18:37 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.017 | 0.052\n",
      "2023-12-30 18:18:37 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.020 | 0.065\n",
      "2023-12-30 18:18:38 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.019 | 0.050\n",
      "2023-12-30 18:18:38 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.015 | 0.052\n",
      "2023-12-30 18:18:38 - INFO     | Early stopping: no decrease (0.051 vs 0.053); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:54<01:32, 10.29s/it]2023-12-30 18:18:38 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 18:18:39 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.004 | 0.053\n",
      "2023-12-30 18:18:39 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.012 | 0.054\n",
      "2023-12-30 18:18:39 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.015 | 0.054\n",
      "2023-12-30 18:18:40 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.019 | 0.050\n",
      "2023-12-30 18:18:40 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.011 | 0.051\n",
      "2023-12-30 18:18:40 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.009 | 0.052\n",
      "2023-12-30 18:18:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.015 | 0.052\n",
      "2023-12-30 18:18:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.020 | 0.054\n",
      "2023-12-30 18:18:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.013 | 0.053\n",
      "2023-12-30 18:18:41 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.017 | 0.053\n",
      "2023-12-30 18:18:42 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.016 | 0.054\n",
      "2023-12-30 18:18:42 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.014 | 0.052\n",
      "2023-12-30 18:18:42 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.019 | 0.050\n",
      "2023-12-30 18:18:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.021 | 0.051\n",
      "2023-12-30 18:18:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.023 | 0.055\n",
      "2023-12-30 18:18:43 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.013 | 0.051\n",
      "2023-12-30 18:18:44 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.015 | 0.052\n",
      "2023-12-30 18:18:44 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.013 | 0.051\n",
      "2023-12-30 18:18:44 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.010 | 0.054\n",
      "2023-12-30 18:18:45 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.014 | 0.051\n",
      "2023-12-30 18:18:45 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.009 | 0.050\n",
      "2023-12-30 18:18:45 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.012 | 0.052\n",
      "2023-12-30 18:18:46 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.016 | 0.051\n",
      "2023-12-30 18:18:46 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.012 | 0.054\n",
      "2023-12-30 18:18:46 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.013 | 0.053\n",
      "2023-12-30 18:18:47 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.017 | 0.049\n",
      "2023-12-30 18:18:47 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.012 | 0.051\n",
      "2023-12-30 18:18:47 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.009 | 0.050\n",
      "2023-12-30 18:18:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.013 | 0.053\n",
      "2023-12-30 18:18:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.013 | 0.050\n",
      "2023-12-30 18:18:48 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.012 | 0.052\n",
      "2023-12-30 18:18:48 - INFO     | Early stopping: no decrease (0.051 vs 0.051); counter: 3 out of 3\n",
      "2023-12-30 18:18:48 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:18:48 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 60%|██████    | 12/20 [02:04<01:21, 10.24s/it]2023-12-30 18:18:48 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 18:18:49 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.004 | 0.051\n",
      "2023-12-30 18:18:49 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.010 | 0.050\n",
      "2023-12-30 18:18:49 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.010 | 0.050\n",
      "2023-12-30 18:18:50 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.007 | 0.050\n",
      "2023-12-30 18:18:50 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.011 | 0.051\n",
      "2023-12-30 18:18:50 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.008 | 0.051\n",
      "2023-12-30 18:18:51 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.009 | 0.051\n",
      "2023-12-30 18:18:51 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:51 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.013 | 0.051\n",
      "2023-12-30 18:18:52 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.013 | 0.051\n",
      "2023-12-30 18:18:53 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.029 | 0.051\n",
      "2023-12-30 18:18:53 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.007 | 0.049\n",
      "2023-12-30 18:18:53 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.012 | 0.054\n",
      "2023-12-30 18:18:54 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.011 | 0.051\n",
      "2023-12-30 18:18:54 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.006 | 0.051\n",
      "2023-12-30 18:18:54 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:55 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.011 | 0.051\n",
      "2023-12-30 18:18:55 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.012 | 0.050\n",
      "2023-12-30 18:18:55 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.013 | 0.050\n",
      "2023-12-30 18:18:55 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.010 | 0.049\n",
      "2023-12-30 18:18:56 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:56 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.008 | 0.050\n",
      "2023-12-30 18:18:56 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.009 | 0.049\n",
      "2023-12-30 18:18:57 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.007 | 0.049\n",
      "2023-12-30 18:18:57 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.010 | 0.049\n",
      "2023-12-30 18:18:57 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.010 | 0.050\n",
      "2023-12-30 18:18:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.014 | 0.049\n",
      "2023-12-30 18:18:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.012 | 0.049\n",
      "2023-12-30 18:18:58 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.022 | 0.048\n",
      "2023-12-30 18:18:59 - INFO     | Early stopping: loss decreased (0.051 -> 0.047; -7.4%). Caching model state.\n",
      " 65%|██████▌   | 13/20 [02:14<01:11, 10.24s/it]2023-12-30 18:18:59 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 18:18:59 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.004 | 0.047\n",
      "2023-12-30 18:18:59 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.009 | 0.047\n",
      "2023-12-30 18:19:00 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.012 | 0.048\n",
      "2023-12-30 18:19:00 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:00 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.008 | 0.047\n",
      "2023-12-30 18:19:01 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.007 | 0.048\n",
      "2023-12-30 18:19:01 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:01 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:02 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.008 | 0.050\n",
      "2023-12-30 18:19:02 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:02 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.024 | 0.050\n",
      "2023-12-30 18:19:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.007 | 0.050\n",
      "2023-12-30 18:19:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.013 | 0.050\n",
      "2023-12-30 18:19:03 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.012 | 0.051\n",
      "2023-12-30 18:19:04 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.011 | 0.052\n",
      "2023-12-30 18:19:04 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.012 | 0.049\n",
      "2023-12-30 18:19:04 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.007 | 0.050\n",
      "2023-12-30 18:19:05 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.011 | 0.048\n",
      "2023-12-30 18:19:05 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.010 | 0.048\n",
      "2023-12-30 18:19:05 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.013 | 0.048\n",
      "2023-12-30 18:19:06 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.009 | 0.050\n",
      "2023-12-30 18:19:06 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:06 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.011 | 0.051\n",
      "2023-12-30 18:19:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.014 | 0.049\n",
      "2023-12-30 18:19:07 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.010 | 0.051\n",
      "2023-12-30 18:19:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.008 | 0.050\n",
      "2023-12-30 18:19:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.006 | 0.051\n",
      "2023-12-30 18:19:08 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.011 | 0.051\n",
      "2023-12-30 18:19:09 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.008 | 0.050\n",
      "2023-12-30 18:19:09 - INFO     | Early stopping: no decrease (0.047 vs 0.050); counter: 1 out of 3\n",
      " 70%|███████   | 14/20 [02:24<01:01, 10.21s/it]2023-12-30 18:19:09 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:19:09 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.001 | 0.050\n",
      "2023-12-30 18:19:09 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.013 | 0.050\n",
      "2023-12-30 18:19:10 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.010 | 0.053\n",
      "2023-12-30 18:19:10 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.009 | 0.051\n",
      "2023-12-30 18:19:10 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.011 | 0.050\n",
      "2023-12-30 18:19:11 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.008 | 0.051\n",
      "2023-12-30 18:19:11 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.009 | 0.052\n",
      "2023-12-30 18:19:11 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.009 | 0.052\n",
      "2023-12-30 18:19:12 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.007 | 0.051\n",
      "2023-12-30 18:19:12 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.007 | 0.050\n",
      "2023-12-30 18:19:12 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.020 | 0.052\n",
      "2023-12-30 18:19:13 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.011 | 0.049\n",
      "2023-12-30 18:19:13 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:13 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.014 | 0.050\n",
      "2023-12-30 18:19:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.013 | 0.050\n",
      "2023-12-30 18:19:14 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:15 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.009 | 0.050\n",
      "2023-12-30 18:19:15 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.013 | 0.050\n",
      "2023-12-30 18:19:15 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.012 | 0.050\n",
      "2023-12-30 18:19:16 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:16 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:16 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.010 | 0.048\n",
      "2023-12-30 18:19:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:17 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.007 | 0.048\n",
      "2023-12-30 18:19:18 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:19 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.009 | 0.050\n",
      "2023-12-30 18:19:19 - INFO     | Early stopping: no decrease (0.047 vs 0.050); counter: 2 out of 3\n",
      " 75%|███████▌  | 15/20 [02:34<00:50, 10.17s/it]2023-12-30 18:19:19 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:19:19 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.006 | 0.050\n",
      "2023-12-30 18:19:20 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.012 | 0.049\n",
      "2023-12-30 18:19:20 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:20 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.011 | 0.050\n",
      "2023-12-30 18:19:21 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:21 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.012 | 0.049\n",
      "2023-12-30 18:19:21 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.021 | 0.050\n",
      "2023-12-30 18:19:21 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:22 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.007 | 0.050\n",
      "2023-12-30 18:19:22 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.010 | 0.050\n",
      "2023-12-30 18:19:22 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.010 | 0.050\n",
      "2023-12-30 18:19:23 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:23 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.006 | 0.050\n",
      "2023-12-30 18:19:23 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:24 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:24 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.011 | 0.049\n",
      "2023-12-30 18:19:24 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.007 | 0.048\n",
      "2023-12-30 18:19:25 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:25 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:25 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.011 | 0.050\n",
      "2023-12-30 18:19:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.010 | 0.050\n",
      "2023-12-30 18:19:26 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.013 | 0.049\n",
      "2023-12-30 18:19:27 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:28 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.006 | 0.050\n",
      "2023-12-30 18:19:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:29 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:29 - INFO     | Early stopping: no decrease (0.047 vs 0.049); counter: 3 out of 3\n",
      "2023-12-30 18:19:29 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:19:29 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 80%|████████  | 16/20 [02:45<00:40, 10.24s/it]2023-12-30 18:19:29 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 18:19:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.011 | 0.049\n",
      "2023-12-30 18:19:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.013 | 0.049\n",
      "2023-12-30 18:19:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.010 | 0.048\n",
      "2023-12-30 18:19:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.005 | 0.048\n",
      "2023-12-30 18:19:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.021 | 0.049\n",
      "2023-12-30 18:19:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.005 | 0.048\n",
      "2023-12-30 18:19:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.006 | 0.050\n",
      "2023-12-30 18:19:36 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.006 | 0.050\n",
      "2023-12-30 18:19:37 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:37 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:37 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:38 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:38 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.011 | 0.048\n",
      "2023-12-30 18:19:38 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:39 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.010 | 0.048\n",
      "2023-12-30 18:19:39 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:39 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.013 | 0.048\n",
      "2023-12-30 18:19:40 - INFO     | Early stopping: no decrease (0.047 vs 0.048); counter: 1 out of 3\n",
      " 85%|████████▌ | 17/20 [02:55<00:30, 10.30s/it]2023-12-30 18:19:40 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 18:19:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.002 | 0.048\n",
      "2023-12-30 18:19:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.009 | 0.048\n",
      "2023-12-30 18:19:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.004 | 0.048\n",
      "2023-12-30 18:19:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.012 | 0.049\n",
      "2023-12-30 18:19:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.007 | 0.048\n",
      "2023-12-30 18:19:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.013 | 0.049\n",
      "2023-12-30 18:19:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.006 | 0.050\n",
      "2023-12-30 18:19:46 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.004 | 0.049\n",
      "2023-12-30 18:19:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.012 | 0.050\n",
      "2023-12-30 18:19:47 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.009 | 0.050\n",
      "2023-12-30 18:19:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.011 | 0.049\n",
      "2023-12-30 18:19:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:48 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.017 | 0.048\n",
      "2023-12-30 18:19:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.009 | 0.048\n",
      "2023-12-30 18:19:49 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.009 | 0.048\n",
      "2023-12-30 18:19:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:50 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:50 - INFO     | Early stopping: no decrease (0.047 vs 0.048); counter: 2 out of 3\n",
      " 90%|█████████ | 18/20 [03:05<00:20, 10.32s/it]2023-12-30 18:19:50 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 18:19:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.002 | 0.048\n",
      "2023-12-30 18:19:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.004 | 0.048\n",
      "2023-12-30 18:19:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.007 | 0.048\n",
      "2023-12-30 18:19:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.017 | 0.048\n",
      "2023-12-30 18:19:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.011 | 0.048\n",
      "2023-12-30 18:19:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.012 | 0.049\n",
      "2023-12-30 18:19:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.007 | 0.048\n",
      "2023-12-30 18:19:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.009 | 0.049\n",
      "2023-12-30 18:19:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.010 | 0.049\n",
      "2023-12-30 18:19:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.009 | 0.048\n",
      "2023-12-30 18:19:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.006 | 0.049\n",
      "2023-12-30 18:19:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:56 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.008 | 0.049\n",
      "2023-12-30 18:19:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.005 | 0.049\n",
      "2023-12-30 18:19:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:57 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.012 | 0.048\n",
      "2023-12-30 18:19:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.004 | 0.048\n",
      "2023-12-30 18:19:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.008 | 0.048\n",
      "2023-12-30 18:19:58 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.006 | 0.048\n",
      "2023-12-30 18:19:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.005 | 0.048\n",
      "2023-12-30 18:19:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.007 | 0.049\n",
      "2023-12-30 18:19:59 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.009 | 0.048\n",
      "2023-12-30 18:20:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.010 | 0.049\n",
      "2023-12-30 18:20:00 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.006 | 0.049\n",
      "2023-12-30 18:20:00 - INFO     | Early stopping: no decrease (0.047 vs 0.049); counter: 3 out of 3\n",
      "2023-12-30 18:20:00 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:20:00 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      " 95%|█████████▌| 19/20 [03:16<00:10, 10.34s/it]2023-12-30 18:20:00 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 18:20:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.008 | 0.049\n",
      "2023-12-30 18:20:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.006 | 0.049\n",
      "2023-12-30 18:20:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.005 | 0.049\n",
      "2023-12-30 18:20:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.008 | 0.049\n",
      "2023-12-30 18:20:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.007 | 0.049\n",
      "2023-12-30 18:20:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.004 | 0.049\n",
      "2023-12-30 18:20:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.008 | 0.049\n",
      "2023-12-30 18:20:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.008 | 0.049\n",
      "2023-12-30 18:20:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.007 | 0.049\n",
      "2023-12-30 18:20:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.006 | 0.049\n",
      "2023-12-30 18:20:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.006 | 0.049\n",
      "2023-12-30 18:20:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.008 | 0.049\n",
      "2023-12-30 18:20:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.006 | 0.048\n",
      "2023-12-30 18:20:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.005 | 0.048\n",
      "2023-12-30 18:20:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.007 | 0.048\n",
      "2023-12-30 18:20:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.006 | 0.048\n",
      "2023-12-30 18:20:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.006 | 0.049\n",
      "2023-12-30 18:20:06 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.007 | 0.049\n",
      "2023-12-30 18:20:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.007 | 0.049\n",
      "2023-12-30 18:20:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.005 | 0.048\n",
      "2023-12-30 18:20:07 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.006 | 0.048\n",
      "2023-12-30 18:20:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.008 | 0.048\n",
      "2023-12-30 18:20:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.016 | 0.048\n",
      "2023-12-30 18:20:08 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.005 | 0.048\n",
      "2023-12-30 18:20:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.006 | 0.048\n",
      "2023-12-30 18:20:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.006 | 0.048\n",
      "2023-12-30 18:20:09 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.010 | 0.048\n",
      "2023-12-30 18:20:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.006 | 0.048\n",
      "2023-12-30 18:20:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.008 | 0.048\n",
      "2023-12-30 18:20:10 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.012 | 0.048\n",
      "2023-12-30 18:20:11 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.011 | 0.048\n",
      "2023-12-30 18:20:11 - INFO     | Early stopping: no decrease (0.047 vs 0.048); counter: 1 out of 3\n",
      "100%|██████████| 20/20 [03:26<00:00, 10.34s/it]\n",
      "2023-12-30 18:20:11 - INFO     | Best validation loss: 0.047\n",
      "2023-12-30 18:20:11 - INFO     | Best early stopping index/epoch: 12\n",
      "2023-12-30 18:20:11 - INFO     | Average Loss on test set: 0.043\n",
      "2023-12-30 18:20:13 - INFO     | Weighted Precision: 0.987, Recall: 0.987, F1: 0.987\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>████████████▄▄▄▄▂▂▂▁</td></tr><tr><td>step_learning_rate</td><td>████████████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁</td></tr><tr><td>step_training_loss</td><td>█▅▄▄▃▂▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▅▃▃▂▂▂▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_validation_loss</td><td>0.04694</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00013</td></tr><tr><td>step_learning_rate</td><td>0.00013</td></tr><tr><td>step_training_loss</td><td>0.0107</td></tr><tr><td>step_validation_loss</td><td>0.04836</td></tr><tr><td>test_loss</td><td>0.04311</td></tr><tr><td>weighted_f1</td><td>0.98743</td></tr><tr><td>weighted_precision</td><td>0.98744</td></tr><tr><td>weighted_recall</td><td>0.98743</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-18</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/47batujz' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/47batujz</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_181644-47batujz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uhys5slm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_182023-uhys5slm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/uhys5slm' target=\"_blank\">vivid-sweep-19</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/uhys5slm' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/uhys5slm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 64], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:20:23 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 18:20:24 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 17.045 | 588.018\n",
      "2023-12-30 18:20:24 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 27.223 | 1.006\n",
      "2023-12-30 18:20:24 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 0.704 | 0.493\n",
      "2023-12-30 18:20:25 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 0.485 | 0.359\n",
      "2023-12-30 18:20:25 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 0.419 | 0.314\n",
      "2023-12-30 18:20:25 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 0.281 | 0.306\n",
      "2023-12-30 18:20:26 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 0.298 | 0.233\n",
      "2023-12-30 18:20:26 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 0.287 | 0.240\n",
      "2023-12-30 18:20:26 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 0.256 | 0.248\n",
      "2023-12-30 18:20:26 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 0.253 | 0.239\n",
      "2023-12-30 18:20:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 0.232 | 0.221\n",
      "2023-12-30 18:20:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 0.277 | 0.224\n",
      "2023-12-30 18:20:27 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 0.243 | 0.212\n",
      "2023-12-30 18:20:28 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.287 | 0.292\n",
      "2023-12-30 18:20:28 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.255 | 0.234\n",
      "2023-12-30 18:20:28 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.253 | 0.225\n",
      "2023-12-30 18:20:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.203 | 0.207\n",
      "2023-12-30 18:20:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.208 | 0.209\n",
      "2023-12-30 18:20:29 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.216 | 0.281\n",
      "2023-12-30 18:20:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.233 | 0.187\n",
      "2023-12-30 18:20:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.200 | 0.206\n",
      "2023-12-30 18:20:30 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.226 | 0.220\n",
      "2023-12-30 18:20:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.212 | 0.222\n",
      "2023-12-30 18:20:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.242 | 0.193\n",
      "2023-12-30 18:20:31 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.213 | 0.228\n",
      "2023-12-30 18:20:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.197 | 0.252\n",
      "2023-12-30 18:20:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.202 | 0.212\n",
      "2023-12-30 18:20:32 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.190 | 0.200\n",
      "2023-12-30 18:20:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.214 | 0.202\n",
      "2023-12-30 18:20:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.236 | 0.224\n",
      "2023-12-30 18:20:33 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.187 | 0.192\n",
      "2023-12-30 18:20:33 - INFO     | Early stopping: loss decreased (inf -> 0.187; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:14, 10.26s/it]2023-12-30 18:20:33 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 18:20:34 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.489 | 0.184\n",
      "2023-12-30 18:20:34 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.193 | 0.181\n",
      "2023-12-30 18:20:34 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.226 | 0.199\n",
      "2023-12-30 18:20:35 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.183 | 0.203\n",
      "2023-12-30 18:20:35 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.250 | 0.229\n",
      "2023-12-30 18:20:35 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.227 | 0.251\n",
      "2023-12-30 18:20:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.222 | 0.194\n",
      "2023-12-30 18:20:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.195 | 0.219\n",
      "2023-12-30 18:20:36 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.188 | 0.205\n",
      "2023-12-30 18:20:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.177 | 0.179\n",
      "2023-12-30 18:20:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.217 | 0.198\n",
      "2023-12-30 18:20:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.157 | 0.222\n",
      "2023-12-30 18:20:37 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.184 | 0.181\n",
      "2023-12-30 18:20:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.197 | 0.216\n",
      "2023-12-30 18:20:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.201 | 0.194\n",
      "2023-12-30 18:20:38 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.225 | 0.217\n",
      "2023-12-30 18:20:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.170 | 0.213\n",
      "2023-12-30 18:20:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.188 | 0.216\n",
      "2023-12-30 18:20:39 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.196 | 0.228\n",
      "2023-12-30 18:20:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.217 | 0.212\n",
      "2023-12-30 18:20:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.215 | 0.225\n",
      "2023-12-30 18:20:40 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.205 | 0.204\n",
      "2023-12-30 18:20:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.182 | 0.210\n",
      "2023-12-30 18:20:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.173 | 0.277\n",
      "2023-12-30 18:20:41 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.276 | 0.274\n",
      "2023-12-30 18:20:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.183 | 0.216\n",
      "2023-12-30 18:20:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.236 | 0.195\n",
      "2023-12-30 18:20:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.233 | 0.258\n",
      "2023-12-30 18:20:42 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.270 | 0.206\n",
      "2023-12-30 18:20:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.214 | 0.207\n",
      "2023-12-30 18:20:43 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.170 | 0.208\n",
      "2023-12-30 18:20:43 - INFO     | Early stopping: no decrease (0.187 vs 0.198); counter: 1 out of 3\n",
      " 10%|█         | 2/20 [00:20<03:01, 10.07s/it]2023-12-30 18:20:43 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 18:20:44 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.247 | 0.198\n",
      "2023-12-30 18:20:44 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.171 | 0.178\n",
      "2023-12-30 18:20:44 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.188 | 0.208\n",
      "2023-12-30 18:20:45 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.200 | 0.158\n",
      "2023-12-30 18:20:45 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.164 | 0.222\n",
      "2023-12-30 18:20:45 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.175 | 0.159\n",
      "2023-12-30 18:20:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.192 | 0.214\n",
      "2023-12-30 18:20:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.189 | 0.224\n",
      "2023-12-30 18:20:46 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.233 | 0.217\n",
      "2023-12-30 18:20:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.228 | 0.229\n",
      "2023-12-30 18:20:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.198 | 0.184\n",
      "2023-12-30 18:20:47 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.176 | 0.239\n",
      "2023-12-30 18:20:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.214 | 0.314\n",
      "2023-12-30 18:20:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.231 | 0.220\n",
      "2023-12-30 18:20:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.231 | 0.202\n",
      "2023-12-30 18:20:48 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.219 | 0.194\n",
      "2023-12-30 18:20:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.222 | 0.219\n",
      "2023-12-30 18:20:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.202 | 0.182\n",
      "2023-12-30 18:20:49 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.133 | 0.175\n",
      "2023-12-30 18:20:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.210 | 0.194\n",
      "2023-12-30 18:20:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.231 | 0.215\n",
      "2023-12-30 18:20:50 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.244 | 0.303\n",
      "2023-12-30 18:20:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.245 | 0.221\n",
      "2023-12-30 18:20:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.199 | 0.279\n",
      "2023-12-30 18:20:51 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.215 | 0.196\n",
      "2023-12-30 18:20:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.202 | 0.286\n",
      "2023-12-30 18:20:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.227 | 0.258\n",
      "2023-12-30 18:20:52 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.212 | 0.205\n",
      "2023-12-30 18:20:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.195 | 0.207\n",
      "2023-12-30 18:20:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.214 | 0.201\n",
      "2023-12-30 18:20:53 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.196 | 0.181\n",
      "2023-12-30 18:20:53 - INFO     | Early stopping: loss decreased (0.187 -> 0.166; -10.9%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:50, 10.05s/it]2023-12-30 18:20:53 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 18:20:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.155 | 0.169\n",
      "2023-12-30 18:20:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.178 | 0.212\n",
      "2023-12-30 18:20:54 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.162 | 0.198\n",
      "2023-12-30 18:20:55 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.184 | 0.203\n",
      "2023-12-30 18:20:55 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.187 | 0.189\n",
      "2023-12-30 18:20:55 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.216 | 0.220\n",
      "2023-12-30 18:20:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.215 | 0.232\n",
      "2023-12-30 18:20:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.185 | 0.216\n",
      "2023-12-30 18:20:56 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.225 | 0.196\n",
      "2023-12-30 18:20:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.187 | 0.226\n",
      "2023-12-30 18:20:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.253 | 0.239\n",
      "2023-12-30 18:20:57 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.278 | 0.210\n",
      "2023-12-30 18:20:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.229 | 0.205\n",
      "2023-12-30 18:20:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.195 | 0.209\n",
      "2023-12-30 18:20:58 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.209 | 0.187\n",
      "2023-12-30 18:20:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.170 | 0.187\n",
      "2023-12-30 18:20:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.190 | 0.188\n",
      "2023-12-30 18:20:59 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.205 | 0.234\n",
      "2023-12-30 18:21:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.175 | 0.209\n",
      "2023-12-30 18:21:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.205 | 0.185\n",
      "2023-12-30 18:21:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.245 | 0.256\n",
      "2023-12-30 18:21:00 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.167 | 0.192\n",
      "2023-12-30 18:21:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.157 | 0.219\n",
      "2023-12-30 18:21:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.235 | 0.244\n",
      "2023-12-30 18:21:01 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.190 | 0.171\n",
      "2023-12-30 18:21:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.214 | 0.229\n",
      "2023-12-30 18:21:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.184 | 0.203\n",
      "2023-12-30 18:21:02 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.192 | 0.164\n",
      "2023-12-30 18:21:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.216 | 0.222\n",
      "2023-12-30 18:21:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.194 | 0.204\n",
      "2023-12-30 18:21:03 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.246 | 0.227\n",
      "2023-12-30 18:21:04 - INFO     | Early stopping: no decrease (0.166 vs 0.210); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:41, 10.08s/it]2023-12-30 18:21:04 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 18:21:04 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.266 | 0.203\n",
      "2023-12-30 18:21:04 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.239 | 0.273\n",
      "2023-12-30 18:21:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.271 | 0.368\n",
      "2023-12-30 18:21:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.315 | 0.277\n",
      "2023-12-30 18:21:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.208 | 0.250\n",
      "2023-12-30 18:21:05 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.254 | 0.207\n",
      "2023-12-30 18:21:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.178 | 0.180\n",
      "2023-12-30 18:21:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.177 | 0.204\n",
      "2023-12-30 18:21:06 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.226 | 0.198\n",
      "2023-12-30 18:21:07 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.174 | 0.188\n",
      "2023-12-30 18:21:07 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.167 | 0.210\n",
      "2023-12-30 18:21:07 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.178 | 0.188\n",
      "2023-12-30 18:21:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.196 | 0.204\n",
      "2023-12-30 18:21:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.232 | 0.196\n",
      "2023-12-30 18:21:08 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.203 | 0.226\n",
      "2023-12-30 18:21:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.205 | 0.281\n",
      "2023-12-30 18:21:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.237 | 0.193\n",
      "2023-12-30 18:21:09 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.154 | 0.176\n",
      "2023-12-30 18:21:10 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.201 | 0.218\n",
      "2023-12-30 18:21:10 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.196 | 0.212\n",
      "2023-12-30 18:21:10 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.228 | 0.199\n",
      "2023-12-30 18:21:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.197 | 0.220\n",
      "2023-12-30 18:21:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.189 | 0.229\n",
      "2023-12-30 18:21:11 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.187 | 0.207\n",
      "2023-12-30 18:21:12 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.171 | 0.211\n",
      "2023-12-30 18:21:12 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.196 | 0.179\n",
      "2023-12-30 18:21:12 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.175 | 0.199\n",
      "2023-12-30 18:21:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.204 | 0.190\n",
      "2023-12-30 18:21:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.215 | 0.197\n",
      "2023-12-30 18:21:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.222 | 0.175\n",
      "2023-12-30 18:21:13 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.182 | 0.223\n",
      "2023-12-30 18:21:14 - INFO     | Early stopping: no decrease (0.166 vs 0.260); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:50<02:31, 10.09s/it]2023-12-30 18:21:14 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 18:21:14 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.250 | 0.245\n",
      "2023-12-30 18:21:14 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.207 | 0.243\n",
      "2023-12-30 18:21:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.173 | 0.222\n",
      "2023-12-30 18:21:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.246 | 0.229\n",
      "2023-12-30 18:21:15 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.192 | 0.200\n",
      "2023-12-30 18:21:16 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.180 | 0.209\n",
      "2023-12-30 18:21:16 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.205 | 0.229\n",
      "2023-12-30 18:21:16 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.193 | 0.199\n",
      "2023-12-30 18:21:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.189 | 0.250\n",
      "2023-12-30 18:21:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.252 | 0.318\n",
      "2023-12-30 18:21:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.225 | 0.231\n",
      "2023-12-30 18:21:17 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.180 | 0.198\n",
      "2023-12-30 18:21:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.197 | 0.255\n",
      "2023-12-30 18:21:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.208 | 0.240\n",
      "2023-12-30 18:21:18 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.313 | 0.266\n",
      "2023-12-30 18:21:19 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.311 | 0.291\n",
      "2023-12-30 18:21:19 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.228 | 0.224\n",
      "2023-12-30 18:21:19 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.225 | 0.230\n",
      "2023-12-30 18:21:20 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.194 | 0.277\n",
      "2023-12-30 18:21:20 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.192 | 0.186\n",
      "2023-12-30 18:21:20 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.192 | 0.222\n",
      "2023-12-30 18:21:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.196 | 0.187\n",
      "2023-12-30 18:21:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.172 | 0.192\n",
      "2023-12-30 18:21:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.179 | 0.203\n",
      "2023-12-30 18:21:21 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.182 | 0.224\n",
      "2023-12-30 18:21:22 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.228 | 0.210\n",
      "2023-12-30 18:21:22 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.213 | 0.196\n",
      "2023-12-30 18:21:22 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.155 | 0.189\n",
      "2023-12-30 18:21:23 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.187 | 0.209\n",
      "2023-12-30 18:21:23 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.202 | 0.187\n",
      "2023-12-30 18:21:23 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.194 | 0.247\n",
      "2023-12-30 18:21:24 - INFO     | Early stopping: no decrease (0.166 vs 0.235); counter: 3 out of 3\n",
      "2023-12-30 18:21:24 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:21:24 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 30%|███       | 6/20 [01:00<02:20, 10.05s/it]2023-12-30 18:21:24 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 18:21:24 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.112 | 0.214\n",
      "2023-12-30 18:21:24 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.193 | 0.164\n",
      "2023-12-30 18:21:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.147 | 0.161\n",
      "2023-12-30 18:21:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.165 | 0.158\n",
      "2023-12-30 18:21:25 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.124 | 0.164\n",
      "2023-12-30 18:21:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.196 | 0.202\n",
      "2023-12-30 18:21:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.164 | 0.142\n",
      "2023-12-30 18:21:26 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.141 | 0.165\n",
      "2023-12-30 18:21:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.135 | 0.142\n",
      "2023-12-30 18:21:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.148 | 0.190\n",
      "2023-12-30 18:21:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.148 | 0.136\n",
      "2023-12-30 18:21:27 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.146 | 0.129\n",
      "2023-12-30 18:21:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.139 | 0.139\n",
      "2023-12-30 18:21:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.132 | 0.160\n",
      "2023-12-30 18:21:28 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.140 | 0.145\n",
      "2023-12-30 18:21:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.130 | 0.180\n",
      "2023-12-30 18:21:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.147 | 0.149\n",
      "2023-12-30 18:21:29 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.125 | 0.149\n",
      "2023-12-30 18:21:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.145 | 0.137\n",
      "2023-12-30 18:21:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.138 | 0.157\n",
      "2023-12-30 18:21:30 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.145 | 0.165\n",
      "2023-12-30 18:21:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.158 | 0.149\n",
      "2023-12-30 18:21:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.136 | 0.136\n",
      "2023-12-30 18:21:31 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.133 | 0.139\n",
      "2023-12-30 18:21:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.143 | 0.148\n",
      "2023-12-30 18:21:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.143 | 0.146\n",
      "2023-12-30 18:21:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.132 | 0.143\n",
      "2023-12-30 18:21:32 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.143 | 0.141\n",
      "2023-12-30 18:21:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.148 | 0.142\n",
      "2023-12-30 18:21:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.149 | 0.162\n",
      "2023-12-30 18:21:33 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.126 | 0.139\n",
      "2023-12-30 18:21:34 - INFO     | Early stopping: loss decreased (0.166 -> 0.153; -7.7%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:10<02:10, 10.06s/it]2023-12-30 18:21:34 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 18:21:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.071 | 0.157\n",
      "2023-12-30 18:21:34 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.104 | 0.140\n",
      "2023-12-30 18:21:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.099 | 0.138\n",
      "2023-12-30 18:21:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.166 | 0.150\n",
      "2023-12-30 18:21:35 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.123 | 0.133\n",
      "2023-12-30 18:21:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.112 | 0.135\n",
      "2023-12-30 18:21:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.135 | 0.135\n",
      "2023-12-30 18:21:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.136 | 0.158\n",
      "2023-12-30 18:21:36 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.123 | 0.138\n",
      "2023-12-30 18:21:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.135 | 0.134\n",
      "2023-12-30 18:21:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.108 | 0.161\n",
      "2023-12-30 18:21:37 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.171 | 0.172\n",
      "2023-12-30 18:21:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.131 | 0.138\n",
      "2023-12-30 18:21:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.124 | 0.165\n",
      "2023-12-30 18:21:38 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.134 | 0.146\n",
      "2023-12-30 18:21:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.134 | 0.141\n",
      "2023-12-30 18:21:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.119 | 0.134\n",
      "2023-12-30 18:21:39 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.137 | 0.134\n",
      "2023-12-30 18:21:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.125 | 0.142\n",
      "2023-12-30 18:21:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.096 | 0.130\n",
      "2023-12-30 18:21:40 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.092 | 0.152\n",
      "2023-12-30 18:21:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.154 | 0.165\n",
      "2023-12-30 18:21:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.153 | 0.140\n",
      "2023-12-30 18:21:41 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.143 | 0.155\n",
      "2023-12-30 18:21:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.149 | 0.153\n",
      "2023-12-30 18:21:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.150 | 0.169\n",
      "2023-12-30 18:21:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.146 | 0.157\n",
      "2023-12-30 18:21:42 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.138 | 0.145\n",
      "2023-12-30 18:21:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.156 | 0.139\n",
      "2023-12-30 18:21:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.112 | 0.139\n",
      "2023-12-30 18:21:43 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.139 | 0.149\n",
      "2023-12-30 18:21:44 - INFO     | Early stopping: loss decreased (0.153 -> 0.138; -10.2%). Caching model state.\n",
      " 40%|████      | 8/20 [01:20<02:00, 10.01s/it]2023-12-30 18:21:44 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 18:21:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.012 | 0.139\n",
      "2023-12-30 18:21:44 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.128 | 0.175\n",
      "2023-12-30 18:21:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.093 | 0.185\n",
      "2023-12-30 18:21:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.133 | 0.170\n",
      "2023-12-30 18:21:45 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.150 | 0.133\n",
      "2023-12-30 18:21:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.101 | 0.136\n",
      "2023-12-30 18:21:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.108 | 0.135\n",
      "2023-12-30 18:21:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.092 | 0.132\n",
      "2023-12-30 18:21:46 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.104 | 0.164\n",
      "2023-12-30 18:21:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.147 | 0.138\n",
      "2023-12-30 18:21:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.103 | 0.158\n",
      "2023-12-30 18:21:47 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.226 | 0.194\n",
      "2023-12-30 18:21:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.118 | 0.172\n",
      "2023-12-30 18:21:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.145 | 0.145\n",
      "2023-12-30 18:21:48 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.120 | 0.148\n",
      "2023-12-30 18:21:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.115 | 0.144\n",
      "2023-12-30 18:21:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.155 | 0.181\n",
      "2023-12-30 18:21:49 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.137 | 0.154\n",
      "2023-12-30 18:21:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.142 | 0.138\n",
      "2023-12-30 18:21:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.161 | 0.137\n",
      "2023-12-30 18:21:50 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.144 | 0.126\n",
      "2023-12-30 18:21:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.126 | 0.130\n",
      "2023-12-30 18:21:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.127 | 0.140\n",
      "2023-12-30 18:21:51 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.160 | 0.149\n",
      "2023-12-30 18:21:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.096 | 0.146\n",
      "2023-12-30 18:21:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.113 | 0.135\n",
      "2023-12-30 18:21:52 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.113 | 0.148\n",
      "2023-12-30 18:21:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.119 | 0.171\n",
      "2023-12-30 18:21:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.100 | 0.146\n",
      "2023-12-30 18:21:53 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.127 | 0.150\n",
      "2023-12-30 18:21:54 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.131 | 0.144\n",
      "2023-12-30 18:21:54 - INFO     | Early stopping: no decrease (0.138 vs 0.134); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:30<01:50, 10.06s/it]2023-12-30 18:21:54 - INFO     | Epoch: 9 | Learning Rate: 0.005\n",
      "2023-12-30 18:21:54 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 504064 examples: 0.102 | 0.132\n",
      "2023-12-30 18:21:54 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 505920 examples: 0.089 | 0.146\n",
      "2023-12-30 18:21:55 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 507776 examples: 0.113 | 0.135\n",
      "2023-12-30 18:21:55 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 509632 examples: 0.140 | 0.146\n",
      "2023-12-30 18:21:55 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 511488 examples: 0.098 | 0.126\n",
      "2023-12-30 18:21:56 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 513344 examples: 0.095 | 0.142\n",
      "2023-12-30 18:21:56 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 515200 examples: 0.113 | 0.159\n",
      "2023-12-30 18:21:56 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 517056 examples: 0.141 | 0.139\n",
      "2023-12-30 18:21:57 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 518912 examples: 0.142 | 0.151\n",
      "2023-12-30 18:21:57 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 520768 examples: 0.172 | 0.137\n",
      "2023-12-30 18:21:57 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 522624 examples: 0.117 | 0.132\n",
      "2023-12-30 18:21:58 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 524480 examples: 0.119 | 0.128\n",
      "2023-12-30 18:21:58 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 526336 examples: 0.115 | 0.129\n",
      "2023-12-30 18:21:58 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 528192 examples: 0.133 | 0.118\n",
      "2023-12-30 18:21:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 530048 examples: 0.094 | 0.128\n",
      "2023-12-30 18:21:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 531904 examples: 0.105 | 0.125\n",
      "2023-12-30 18:21:59 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 533760 examples: 0.114 | 0.159\n",
      "2023-12-30 18:22:00 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 535616 examples: 0.102 | 0.171\n",
      "2023-12-30 18:22:00 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 537472 examples: 0.198 | 0.166\n",
      "2023-12-30 18:22:00 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 539328 examples: 0.155 | 0.169\n",
      "2023-12-30 18:22:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 541184 examples: 0.145 | 0.143\n",
      "2023-12-30 18:22:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 543040 examples: 0.099 | 0.137\n",
      "2023-12-30 18:22:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 544896 examples: 0.129 | 0.167\n",
      "2023-12-30 18:22:01 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 546752 examples: 0.132 | 0.140\n",
      "2023-12-30 18:22:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 548608 examples: 0.129 | 0.132\n",
      "2023-12-30 18:22:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 550464 examples: 0.128 | 0.125\n",
      "2023-12-30 18:22:02 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 552320 examples: 0.124 | 0.132\n",
      "2023-12-30 18:22:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 554176 examples: 0.127 | 0.146\n",
      "2023-12-30 18:22:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 556032 examples: 0.163 | 0.150\n",
      "2023-12-30 18:22:03 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 557888 examples: 0.102 | 0.125\n",
      "2023-12-30 18:22:04 - INFO     | Epoch: 9 | Learning Rate: 0.005: Training/Validation Loss after 559744 examples: 0.121 | 0.132\n",
      "2023-12-30 18:22:04 - INFO     | Early stopping: no decrease (0.138 vs 0.147); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:40<01:40, 10.09s/it]2023-12-30 18:22:04 - INFO     | Epoch: 10 | Learning Rate: 0.005\n",
      "2023-12-30 18:22:04 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 560064 examples: 0.223 | 0.155\n",
      "2023-12-30 18:22:05 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 561920 examples: 0.103 | 0.123\n",
      "2023-12-30 18:22:05 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 563776 examples: 0.100 | 0.139\n",
      "2023-12-30 18:22:05 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 565632 examples: 0.087 | 0.174\n",
      "2023-12-30 18:22:06 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 567488 examples: 0.132 | 0.145\n",
      "2023-12-30 18:22:06 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 569344 examples: 0.131 | 0.123\n",
      "2023-12-30 18:22:06 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 571200 examples: 0.116 | 0.134\n",
      "2023-12-30 18:22:07 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 573056 examples: 0.133 | 0.140\n",
      "2023-12-30 18:22:07 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 574912 examples: 0.104 | 0.142\n",
      "2023-12-30 18:22:07 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 576768 examples: 0.118 | 0.150\n",
      "2023-12-30 18:22:07 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 578624 examples: 0.119 | 0.144\n",
      "2023-12-30 18:22:08 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 580480 examples: 0.138 | 0.139\n",
      "2023-12-30 18:22:08 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 582336 examples: 0.133 | 0.141\n",
      "2023-12-30 18:22:08 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 584192 examples: 0.109 | 0.195\n",
      "2023-12-30 18:22:09 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 586048 examples: 0.141 | 0.142\n",
      "2023-12-30 18:22:09 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 587904 examples: 0.126 | 0.175\n",
      "2023-12-30 18:22:10 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 589760 examples: 0.135 | 0.162\n",
      "2023-12-30 18:22:10 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 591616 examples: 0.133 | 0.161\n",
      "2023-12-30 18:22:10 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 593472 examples: 0.123 | 0.150\n",
      "2023-12-30 18:22:11 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 595328 examples: 0.143 | 0.147\n",
      "2023-12-30 18:22:11 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 597184 examples: 0.123 | 0.156\n",
      "2023-12-30 18:22:11 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 599040 examples: 0.095 | 0.130\n",
      "2023-12-30 18:22:11 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 600896 examples: 0.138 | 0.163\n",
      "2023-12-30 18:22:12 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 602752 examples: 0.123 | 0.127\n",
      "2023-12-30 18:22:12 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 604608 examples: 0.106 | 0.157\n",
      "2023-12-30 18:22:12 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 606464 examples: 0.125 | 0.170\n",
      "2023-12-30 18:22:13 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 608320 examples: 0.129 | 0.148\n",
      "2023-12-30 18:22:13 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 610176 examples: 0.113 | 0.148\n",
      "2023-12-30 18:22:13 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 612032 examples: 0.123 | 0.146\n",
      "2023-12-30 18:22:14 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 613888 examples: 0.142 | 0.154\n",
      "2023-12-30 18:22:14 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 615744 examples: 0.129 | 0.168\n",
      "2023-12-30 18:22:14 - INFO     | Early stopping: no decrease (0.138 vs 0.152); counter: 3 out of 3\n",
      "2023-12-30 18:22:14 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:22:14 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 55%|█████▌    | 11/20 [01:51<01:31, 10.15s/it]2023-12-30 18:22:14 - INFO     | Epoch: 11 | Learning Rate: 0.003\n",
      "2023-12-30 18:22:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 616064 examples: 0.018 | 0.157\n",
      "2023-12-30 18:22:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 617920 examples: 0.094 | 0.129\n",
      "2023-12-30 18:22:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 619776 examples: 0.098 | 0.130\n",
      "2023-12-30 18:22:15 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 621632 examples: 0.094 | 0.119\n",
      "2023-12-30 18:22:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 623488 examples: 0.105 | 0.126\n",
      "2023-12-30 18:22:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 625344 examples: 0.108 | 0.121\n",
      "2023-12-30 18:22:16 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 627200 examples: 0.100 | 0.142\n",
      "2023-12-30 18:22:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 629056 examples: 0.091 | 0.115\n",
      "2023-12-30 18:22:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 630912 examples: 0.104 | 0.122\n",
      "2023-12-30 18:22:17 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 632768 examples: 0.087 | 0.127\n",
      "2023-12-30 18:22:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 634624 examples: 0.081 | 0.117\n",
      "2023-12-30 18:22:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 636480 examples: 0.079 | 0.111\n",
      "2023-12-30 18:22:18 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 638336 examples: 0.080 | 0.109\n",
      "2023-12-30 18:22:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 640192 examples: 0.082 | 0.123\n",
      "2023-12-30 18:22:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 642048 examples: 0.076 | 0.117\n",
      "2023-12-30 18:22:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 643904 examples: 0.076 | 0.127\n",
      "2023-12-30 18:22:19 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 645760 examples: 0.068 | 0.120\n",
      "2023-12-30 18:22:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 647616 examples: 0.101 | 0.121\n",
      "2023-12-30 18:22:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 649472 examples: 0.083 | 0.127\n",
      "2023-12-30 18:22:20 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 651328 examples: 0.078 | 0.116\n",
      "2023-12-30 18:22:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 653184 examples: 0.089 | 0.130\n",
      "2023-12-30 18:22:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 655040 examples: 0.079 | 0.123\n",
      "2023-12-30 18:22:21 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 656896 examples: 0.108 | 0.141\n",
      "2023-12-30 18:22:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 658752 examples: 0.109 | 0.175\n",
      "2023-12-30 18:22:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 660608 examples: 0.105 | 0.125\n",
      "2023-12-30 18:22:22 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 662464 examples: 0.087 | 0.111\n",
      "2023-12-30 18:22:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 664320 examples: 0.061 | 0.128\n",
      "2023-12-30 18:22:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 666176 examples: 0.063 | 0.141\n",
      "2023-12-30 18:22:23 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 668032 examples: 0.108 | 0.133\n",
      "2023-12-30 18:22:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 669888 examples: 0.104 | 0.130\n",
      "2023-12-30 18:22:24 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 671744 examples: 0.078 | 0.135\n",
      "2023-12-30 18:22:24 - INFO     | Early stopping: no decrease (0.138 vs 0.139); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [02:00<01:20, 10.07s/it]2023-12-30 18:22:24 - INFO     | Epoch: 12 | Learning Rate: 0.003\n",
      "2023-12-30 18:22:24 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 672064 examples: 0.114 | 0.138\n",
      "2023-12-30 18:22:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 673920 examples: 0.075 | 0.121\n",
      "2023-12-30 18:22:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 675776 examples: 0.074 | 0.120\n",
      "2023-12-30 18:22:25 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 677632 examples: 0.071 | 0.128\n",
      "2023-12-30 18:22:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 679488 examples: 0.095 | 0.135\n",
      "2023-12-30 18:22:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 681344 examples: 0.076 | 0.119\n",
      "2023-12-30 18:22:26 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 683200 examples: 0.053 | 0.117\n",
      "2023-12-30 18:22:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 685056 examples: 0.082 | 0.121\n",
      "2023-12-30 18:22:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 686912 examples: 0.080 | 0.130\n",
      "2023-12-30 18:22:27 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 688768 examples: 0.082 | 0.139\n",
      "2023-12-30 18:22:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 690624 examples: 0.082 | 0.114\n",
      "2023-12-30 18:22:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 692480 examples: 0.075 | 0.135\n",
      "2023-12-30 18:22:28 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 694336 examples: 0.057 | 0.133\n",
      "2023-12-30 18:22:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 696192 examples: 0.056 | 0.119\n",
      "2023-12-30 18:22:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 698048 examples: 0.112 | 0.123\n",
      "2023-12-30 18:22:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 699904 examples: 0.088 | 0.120\n",
      "2023-12-30 18:22:29 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 701760 examples: 0.070 | 0.115\n",
      "2023-12-30 18:22:30 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 703616 examples: 0.099 | 0.112\n",
      "2023-12-30 18:22:30 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 705472 examples: 0.066 | 0.109\n",
      "2023-12-30 18:22:30 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 707328 examples: 0.088 | 0.106\n",
      "2023-12-30 18:22:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 709184 examples: 0.107 | 0.124\n",
      "2023-12-30 18:22:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 711040 examples: 0.099 | 0.109\n",
      "2023-12-30 18:22:31 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 712896 examples: 0.069 | 0.108\n",
      "2023-12-30 18:22:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 714752 examples: 0.080 | 0.108\n",
      "2023-12-30 18:22:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 716608 examples: 0.084 | 0.138\n",
      "2023-12-30 18:22:32 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 718464 examples: 0.091 | 0.120\n",
      "2023-12-30 18:22:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 720320 examples: 0.101 | 0.128\n",
      "2023-12-30 18:22:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 722176 examples: 0.080 | 0.126\n",
      "2023-12-30 18:22:33 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 724032 examples: 0.093 | 0.108\n",
      "2023-12-30 18:22:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 725888 examples: 0.090 | 0.124\n",
      "2023-12-30 18:22:34 - INFO     | Epoch: 12 | Learning Rate: 0.003: Training/Validation Loss after 727744 examples: 0.082 | 0.104\n",
      "2023-12-30 18:22:34 - INFO     | Early stopping: loss decreased (0.138 -> 0.105; -24.1%). Caching model state.\n",
      " 65%|██████▌   | 13/20 [02:10<01:10, 10.03s/it]2023-12-30 18:22:34 - INFO     | Epoch: 13 | Learning Rate: 0.003\n",
      "2023-12-30 18:22:34 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 728064 examples: 0.090 | 0.105\n",
      "2023-12-30 18:22:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 729920 examples: 0.055 | 0.113\n",
      "2023-12-30 18:22:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 731776 examples: 0.040 | 0.119\n",
      "2023-12-30 18:22:35 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 733632 examples: 0.063 | 0.113\n",
      "2023-12-30 18:22:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 735488 examples: 0.051 | 0.104\n",
      "2023-12-30 18:22:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 737344 examples: 0.058 | 0.107\n",
      "2023-12-30 18:22:36 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 739200 examples: 0.074 | 0.116\n",
      "2023-12-30 18:22:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 741056 examples: 0.067 | 0.127\n",
      "2023-12-30 18:22:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 742912 examples: 0.085 | 0.116\n",
      "2023-12-30 18:22:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 744768 examples: 0.067 | 0.118\n",
      "2023-12-30 18:22:37 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 746624 examples: 0.078 | 0.114\n",
      "2023-12-30 18:22:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 748480 examples: 0.066 | 0.116\n",
      "2023-12-30 18:22:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 750336 examples: 0.063 | 0.106\n",
      "2023-12-30 18:22:38 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 752192 examples: 0.078 | 0.104\n",
      "2023-12-30 18:22:39 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 754048 examples: 0.064 | 0.125\n",
      "2023-12-30 18:22:39 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 755904 examples: 0.112 | 0.131\n",
      "2023-12-30 18:22:39 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 757760 examples: 0.096 | 0.122\n",
      "2023-12-30 18:22:40 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 759616 examples: 0.078 | 0.122\n",
      "2023-12-30 18:22:40 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 761472 examples: 0.067 | 0.104\n",
      "2023-12-30 18:22:40 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 763328 examples: 0.075 | 0.107\n",
      "2023-12-30 18:22:41 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 765184 examples: 0.062 | 0.116\n",
      "2023-12-30 18:22:41 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 767040 examples: 0.063 | 0.109\n",
      "2023-12-30 18:22:41 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 768896 examples: 0.045 | 0.110\n",
      "2023-12-30 18:22:41 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 770752 examples: 0.099 | 0.149\n",
      "2023-12-30 18:22:42 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 772608 examples: 0.128 | 0.133\n",
      "2023-12-30 18:22:42 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 774464 examples: 0.109 | 0.128\n",
      "2023-12-30 18:22:42 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 776320 examples: 0.086 | 0.125\n",
      "2023-12-30 18:22:43 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 778176 examples: 0.091 | 0.116\n",
      "2023-12-30 18:22:43 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 780032 examples: 0.087 | 0.107\n",
      "2023-12-30 18:22:43 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 781888 examples: 0.082 | 0.114\n",
      "2023-12-30 18:22:44 - INFO     | Epoch: 13 | Learning Rate: 0.003: Training/Validation Loss after 783744 examples: 0.071 | 0.115\n",
      "2023-12-30 18:22:44 - INFO     | Early stopping: no decrease (0.105 vs 0.112); counter: 1 out of 3\n",
      " 70%|███████   | 14/20 [02:20<00:59,  9.95s/it]2023-12-30 18:22:44 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 18:22:44 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.032 | 0.112\n",
      "2023-12-30 18:22:44 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.045 | 0.118\n",
      "2023-12-30 18:22:45 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.064 | 0.112\n",
      "2023-12-30 18:22:45 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.070 | 0.105\n",
      "2023-12-30 18:22:45 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.070 | 0.116\n",
      "2023-12-30 18:22:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.067 | 0.115\n",
      "2023-12-30 18:22:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.054 | 0.111\n",
      "2023-12-30 18:22:46 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.081 | 0.112\n",
      "2023-12-30 18:22:47 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.053 | 0.110\n",
      "2023-12-30 18:22:47 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.065 | 0.117\n",
      "2023-12-30 18:22:47 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.059 | 0.124\n",
      "2023-12-30 18:22:48 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.070 | 0.136\n",
      "2023-12-30 18:22:48 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.083 | 0.125\n",
      "2023-12-30 18:22:48 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.066 | 0.108\n",
      "2023-12-30 18:22:48 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.070 | 0.111\n",
      "2023-12-30 18:22:49 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.069 | 0.113\n",
      "2023-12-30 18:22:49 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.049 | 0.112\n",
      "2023-12-30 18:22:49 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.076 | 0.122\n",
      "2023-12-30 18:22:50 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.063 | 0.129\n",
      "2023-12-30 18:22:50 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.097 | 0.117\n",
      "2023-12-30 18:22:50 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.067 | 0.117\n",
      "2023-12-30 18:22:51 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.073 | 0.113\n",
      "2023-12-30 18:22:51 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.074 | 0.121\n",
      "2023-12-30 18:22:51 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.068 | 0.126\n",
      "2023-12-30 18:22:52 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.070 | 0.118\n",
      "2023-12-30 18:22:52 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.101 | 0.127\n",
      "2023-12-30 18:22:52 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.071 | 0.130\n",
      "2023-12-30 18:22:52 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.092 | 0.118\n",
      "2023-12-30 18:22:53 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.096 | 0.118\n",
      "2023-12-30 18:22:53 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.061 | 0.106\n",
      "2023-12-30 18:22:53 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.077 | 0.139\n",
      "2023-12-30 18:22:54 - INFO     | Early stopping: no decrease (0.105 vs 0.117); counter: 2 out of 3\n",
      " 75%|███████▌  | 15/20 [02:30<00:49,  9.91s/it]2023-12-30 18:22:54 - INFO     | Epoch: 15 | Learning Rate: 0.003\n",
      "2023-12-30 18:22:54 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 840064 examples: 0.073 | 0.114\n",
      "2023-12-30 18:22:54 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 841920 examples: 0.043 | 0.104\n",
      "2023-12-30 18:22:55 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 843776 examples: 0.050 | 0.135\n",
      "2023-12-30 18:22:55 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 845632 examples: 0.076 | 0.114\n",
      "2023-12-30 18:22:55 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 847488 examples: 0.070 | 0.118\n",
      "2023-12-30 18:22:56 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 849344 examples: 0.067 | 0.122\n",
      "2023-12-30 18:22:56 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 851200 examples: 0.053 | 0.113\n",
      "2023-12-30 18:22:56 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 853056 examples: 0.051 | 0.115\n",
      "2023-12-30 18:22:56 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 854912 examples: 0.088 | 0.131\n",
      "2023-12-30 18:22:57 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 856768 examples: 0.075 | 0.112\n",
      "2023-12-30 18:22:57 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 858624 examples: 0.059 | 0.115\n",
      "2023-12-30 18:22:57 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 860480 examples: 0.055 | 0.111\n",
      "2023-12-30 18:22:58 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 862336 examples: 0.084 | 0.121\n",
      "2023-12-30 18:22:58 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 864192 examples: 0.083 | 0.125\n",
      "2023-12-30 18:22:58 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 866048 examples: 0.067 | 0.117\n",
      "2023-12-30 18:22:59 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 867904 examples: 0.082 | 0.116\n",
      "2023-12-30 18:22:59 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 869760 examples: 0.050 | 0.113\n",
      "2023-12-30 18:22:59 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 871616 examples: 0.047 | 0.120\n",
      "2023-12-30 18:23:00 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 873472 examples: 0.064 | 0.116\n",
      "2023-12-30 18:23:00 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 875328 examples: 0.065 | 0.108\n",
      "2023-12-30 18:23:00 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 877184 examples: 0.059 | 0.123\n",
      "2023-12-30 18:23:01 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 879040 examples: 0.111 | 0.122\n",
      "2023-12-30 18:23:01 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 880896 examples: 0.080 | 0.128\n",
      "2023-12-30 18:23:01 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 882752 examples: 0.082 | 0.125\n",
      "2023-12-30 18:23:02 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 884608 examples: 0.079 | 0.119\n",
      "2023-12-30 18:23:02 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 886464 examples: 0.082 | 0.122\n",
      "2023-12-30 18:23:02 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 888320 examples: 0.085 | 0.113\n",
      "2023-12-30 18:23:02 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 890176 examples: 0.062 | 0.125\n",
      "2023-12-30 18:23:03 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 892032 examples: 0.070 | 0.120\n",
      "2023-12-30 18:23:03 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 893888 examples: 0.104 | 0.125\n",
      "2023-12-30 18:23:03 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 895744 examples: 0.057 | 0.129\n",
      "2023-12-30 18:23:04 - INFO     | Early stopping: no decrease (0.105 vs 0.125); counter: 3 out of 3\n",
      "2023-12-30 18:23:04 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:23:04 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 80%|████████  | 16/20 [02:40<00:39,  9.94s/it]2023-12-30 18:23:04 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:23:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.023 | 0.125\n",
      "2023-12-30 18:23:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.048 | 0.119\n",
      "2023-12-30 18:23:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.046 | 0.115\n",
      "2023-12-30 18:23:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.052 | 0.119\n",
      "2023-12-30 18:23:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.040 | 0.123\n",
      "2023-12-30 18:23:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.054 | 0.118\n",
      "2023-12-30 18:23:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.052 | 0.119\n",
      "2023-12-30 18:23:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.062 | 0.113\n",
      "2023-12-30 18:23:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.050 | 0.120\n",
      "2023-12-30 18:23:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.050 | 0.117\n",
      "2023-12-30 18:23:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.050 | 0.116\n",
      "2023-12-30 18:23:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.053 | 0.119\n",
      "2023-12-30 18:23:08 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.041 | 0.114\n",
      "2023-12-30 18:23:08 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.043 | 0.115\n",
      "2023-12-30 18:23:08 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.052 | 0.118\n",
      "2023-12-30 18:23:09 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.063 | 0.124\n",
      "2023-12-30 18:23:09 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.056 | 0.113\n",
      "2023-12-30 18:23:09 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.036 | 0.112\n",
      "2023-12-30 18:23:10 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.047 | 0.115\n",
      "2023-12-30 18:23:10 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.054 | 0.115\n",
      "2023-12-30 18:23:10 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.070 | 0.118\n",
      "2023-12-30 18:23:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.054 | 0.117\n",
      "2023-12-30 18:23:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.059 | 0.117\n",
      "2023-12-30 18:23:11 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.056 | 0.114\n",
      "2023-12-30 18:23:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.055 | 0.113\n",
      "2023-12-30 18:23:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.056 | 0.111\n",
      "2023-12-30 18:23:12 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.049 | 0.111\n",
      "2023-12-30 18:23:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.046 | 0.116\n",
      "2023-12-30 18:23:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.035 | 0.117\n",
      "2023-12-30 18:23:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.068 | 0.117\n",
      "2023-12-30 18:23:13 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.058 | 0.119\n",
      "2023-12-30 18:23:14 - INFO     | Early stopping: no decrease (0.105 vs 0.117); counter: 1 out of 3\n",
      " 85%|████████▌ | 17/20 [02:50<00:29,  9.96s/it]2023-12-30 18:23:14 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:23:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.087 | 0.117\n",
      "2023-12-30 18:23:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.043 | 0.111\n",
      "2023-12-30 18:23:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.034 | 0.118\n",
      "2023-12-30 18:23:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.034 | 0.114\n",
      "2023-12-30 18:23:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.037 | 0.112\n",
      "2023-12-30 18:23:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.034 | 0.119\n",
      "2023-12-30 18:23:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.040 | 0.111\n",
      "2023-12-30 18:23:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.037 | 0.109\n",
      "2023-12-30 18:23:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.035 | 0.113\n",
      "2023-12-30 18:23:17 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.061 | 0.120\n",
      "2023-12-30 18:23:17 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.061 | 0.114\n",
      "2023-12-30 18:23:17 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.052 | 0.119\n",
      "2023-12-30 18:23:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.048 | 0.122\n",
      "2023-12-30 18:23:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.042 | 0.119\n",
      "2023-12-30 18:23:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.043 | 0.117\n",
      "2023-12-30 18:23:19 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.041 | 0.117\n",
      "2023-12-30 18:23:19 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.045 | 0.116\n",
      "2023-12-30 18:23:19 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.038 | 0.114\n",
      "2023-12-30 18:23:20 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.044 | 0.107\n",
      "2023-12-30 18:23:20 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.040 | 0.109\n",
      "2023-12-30 18:23:20 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.047 | 0.115\n",
      "2023-12-30 18:23:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.068 | 0.112\n",
      "2023-12-30 18:23:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.048 | 0.110\n",
      "2023-12-30 18:23:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.046 | 0.110\n",
      "2023-12-30 18:23:21 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.036 | 0.110\n",
      "2023-12-30 18:23:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.061 | 0.109\n",
      "2023-12-30 18:23:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.051 | 0.107\n",
      "2023-12-30 18:23:22 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.042 | 0.110\n",
      "2023-12-30 18:23:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.053 | 0.112\n",
      "2023-12-30 18:23:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.046 | 0.114\n",
      "2023-12-30 18:23:23 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.039 | 0.110\n",
      "2023-12-30 18:23:24 - INFO     | Early stopping: no decrease (0.105 vs 0.111); counter: 2 out of 3\n",
      " 90%|█████████ | 18/20 [03:00<00:19,  9.97s/it]2023-12-30 18:23:24 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 18:23:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.009 | 0.111\n",
      "2023-12-30 18:23:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.041 | 0.106\n",
      "2023-12-30 18:23:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.048 | 0.105\n",
      "2023-12-30 18:23:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.037 | 0.107\n",
      "2023-12-30 18:23:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.028 | 0.109\n",
      "2023-12-30 18:23:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.039 | 0.110\n",
      "2023-12-30 18:23:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.034 | 0.122\n",
      "2023-12-30 18:23:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.039 | 0.106\n",
      "2023-12-30 18:23:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.051 | 0.108\n",
      "2023-12-30 18:23:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.034 | 0.114\n",
      "2023-12-30 18:23:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.046 | 0.116\n",
      "2023-12-30 18:23:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.031 | 0.115\n",
      "2023-12-30 18:23:28 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.045 | 0.110\n",
      "2023-12-30 18:23:28 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.036 | 0.110\n",
      "2023-12-30 18:23:28 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.035 | 0.109\n",
      "2023-12-30 18:23:29 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.038 | 0.112\n",
      "2023-12-30 18:23:29 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.033 | 0.114\n",
      "2023-12-30 18:23:29 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.047 | 0.119\n",
      "2023-12-30 18:23:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.054 | 0.118\n",
      "2023-12-30 18:23:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.049 | 0.112\n",
      "2023-12-30 18:23:30 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.046 | 0.110\n",
      "2023-12-30 18:23:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.047 | 0.115\n",
      "2023-12-30 18:23:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.062 | 0.126\n",
      "2023-12-30 18:23:31 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.051 | 0.113\n",
      "2023-12-30 18:23:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.050 | 0.111\n",
      "2023-12-30 18:23:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.036 | 0.109\n",
      "2023-12-30 18:23:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.034 | 0.112\n",
      "2023-12-30 18:23:32 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.031 | 0.117\n",
      "2023-12-30 18:23:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.051 | 0.116\n",
      "2023-12-30 18:23:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.046 | 0.119\n",
      "2023-12-30 18:23:33 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.036 | 0.107\n",
      "2023-12-30 18:23:34 - INFO     | Early stopping: no decrease (0.105 vs 0.107); counter: 3 out of 3\n",
      "2023-12-30 18:23:34 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:23:34 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 95%|█████████▌| 19/20 [03:10<00:09,  9.97s/it]2023-12-30 18:23:34 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 18:23:34 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.021 | 0.106\n",
      "2023-12-30 18:23:34 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.028 | 0.110\n",
      "2023-12-30 18:23:35 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.032 | 0.109\n",
      "2023-12-30 18:23:35 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.028 | 0.108\n",
      "2023-12-30 18:23:35 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.032 | 0.110\n",
      "2023-12-30 18:23:35 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.024 | 0.110\n",
      "2023-12-30 18:23:36 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.025 | 0.111\n",
      "2023-12-30 18:23:36 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.051 | 0.118\n",
      "2023-12-30 18:23:36 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.023 | 0.111\n",
      "2023-12-30 18:23:37 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.031 | 0.117\n",
      "2023-12-30 18:23:37 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.036 | 0.111\n",
      "2023-12-30 18:23:37 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.035 | 0.108\n",
      "2023-12-30 18:23:38 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.020 | 0.108\n",
      "2023-12-30 18:23:38 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.026 | 0.107\n",
      "2023-12-30 18:23:38 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.026 | 0.107\n",
      "2023-12-30 18:23:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.029 | 0.112\n",
      "2023-12-30 18:23:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.026 | 0.113\n",
      "2023-12-30 18:23:39 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.031 | 0.109\n",
      "2023-12-30 18:23:40 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.034 | 0.110\n",
      "2023-12-30 18:23:40 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.042 | 0.112\n",
      "2023-12-30 18:23:40 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.040 | 0.111\n",
      "2023-12-30 18:23:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.042 | 0.109\n",
      "2023-12-30 18:23:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.030 | 0.112\n",
      "2023-12-30 18:23:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.032 | 0.116\n",
      "2023-12-30 18:23:41 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.035 | 0.118\n",
      "2023-12-30 18:23:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.038 | 0.114\n",
      "2023-12-30 18:23:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.021 | 0.115\n",
      "2023-12-30 18:23:42 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.038 | 0.118\n",
      "2023-12-30 18:23:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.040 | 0.135\n",
      "2023-12-30 18:23:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.031 | 0.125\n",
      "2023-12-30 18:23:43 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.037 | 0.117\n",
      "2023-12-30 18:23:44 - INFO     | Early stopping: no decrease (0.105 vs 0.116); counter: 1 out of 3\n",
      "100%|██████████| 20/20 [03:20<00:00, 10.02s/it]\n",
      "2023-12-30 18:23:44 - INFO     | Best validation loss: 0.105\n",
      "2023-12-30 18:23:44 - INFO     | Best early stopping index/epoch: 12\n",
      "2023-12-30 18:23:44 - INFO     | Average Loss on test set: 0.118\n",
      "2023-12-30 18:23:47 - INFO     | Weighted Precision: 0.970, Recall: 0.970, F1: 0.970\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▂▂▂▂▂▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>▅▄█▃▃▄▃▃▅▄▄▄▄▃▃▃▂▃▃▃▂▃▂▂▂▂▁▂▂▂▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>▇▆▄▅▄▄▅▅█▅▅▆▅▃▂▂▂▃▃▂▂▃▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_validation_loss</td><td>0.10452</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00063</td></tr><tr><td>step_learning_rate</td><td>0.00063</td></tr><tr><td>step_training_loss</td><td>0.03704</td></tr><tr><td>step_validation_loss</td><td>0.11678</td></tr><tr><td>test_loss</td><td>0.11784</td></tr><tr><td>weighted_f1</td><td>0.97014</td></tr><tr><td>weighted_precision</td><td>0.97021</td></tr><tr><td>weighted_recall</td><td>0.97014</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-19</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/uhys5slm' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/uhys5slm</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_182023-uhys5slm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dkgi6uwl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_182357-dkgi6uwl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/dkgi6uwl' target=\"_blank\">smooth-sweep-20</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/dkgi6uwl' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/dkgi6uwl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 64], 'learning_rate': 0.01, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:23:57 - INFO     | Epoch: 0 | Learning Rate: 0.010\n",
      "2023-12-30 18:23:58 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 00064 examples: 32.373 | 2596.794\n",
      "2023-12-30 18:23:58 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 01920 examples: 69.769 | 2.287\n",
      "2023-12-30 18:23:58 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 03776 examples: 2.312 | 2.293\n",
      "2023-12-30 18:23:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 05632 examples: 2.287 | 2.288\n",
      "2023-12-30 18:23:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 07488 examples: 2.281 | 2.288\n",
      "2023-12-30 18:23:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 09344 examples: 2.293 | 2.275\n",
      "2023-12-30 18:23:59 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 11200 examples: 2.278 | 2.245\n",
      "2023-12-30 18:24:00 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 13056 examples: 2.245 | 2.207\n",
      "2023-12-30 18:24:00 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 14912 examples: 2.192 | 2.213\n",
      "2023-12-30 18:24:00 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 16768 examples: 2.052 | 1.965\n",
      "2023-12-30 18:24:01 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 18624 examples: 1.870 | 1.745\n",
      "2023-12-30 18:24:01 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 20480 examples: 1.525 | 1.227\n",
      "2023-12-30 18:24:01 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 22336 examples: 1.193 | 0.993\n",
      "2023-12-30 18:24:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 24192 examples: 0.965 | 1.013\n",
      "2023-12-30 18:24:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 26048 examples: 0.891 | 0.815\n",
      "2023-12-30 18:24:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 27904 examples: 0.723 | 0.611\n",
      "2023-12-30 18:24:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 29760 examples: 0.619 | 0.522\n",
      "2023-12-30 18:24:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 31616 examples: 0.593 | 0.612\n",
      "2023-12-30 18:24:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 33472 examples: 0.529 | 0.506\n",
      "2023-12-30 18:24:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 35328 examples: 0.475 | 0.457\n",
      "2023-12-30 18:24:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 37184 examples: 0.488 | 0.400\n",
      "2023-12-30 18:24:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 39040 examples: 0.440 | 0.465\n",
      "2023-12-30 18:24:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 40896 examples: 0.434 | 0.471\n",
      "2023-12-30 18:24:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 42752 examples: 0.406 | 0.359\n",
      "2023-12-30 18:24:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 44608 examples: 0.356 | 0.317\n",
      "2023-12-30 18:24:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 46464 examples: 0.346 | 0.331\n",
      "2023-12-30 18:24:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 48320 examples: 0.306 | 0.341\n",
      "2023-12-30 18:24:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 50176 examples: 0.353 | 0.274\n",
      "2023-12-30 18:24:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 52032 examples: 0.282 | 0.246\n",
      "2023-12-30 18:24:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 53888 examples: 0.256 | 0.238\n",
      "2023-12-30 18:24:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Training/Validation Loss after 55744 examples: 0.253 | 0.231\n",
      "2023-12-30 18:24:07 - INFO     | Early stopping: loss decreased (inf -> 0.301; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:10, 10.03s/it]2023-12-30 18:24:07 - INFO     | Epoch: 1 | Learning Rate: 0.010\n",
      "2023-12-30 18:24:08 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 56064 examples: 0.273 | 0.286\n",
      "2023-12-30 18:24:08 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 57920 examples: 0.239 | 0.231\n",
      "2023-12-30 18:24:08 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 59776 examples: 0.295 | 0.265\n",
      "2023-12-30 18:24:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 61632 examples: 0.223 | 0.220\n",
      "2023-12-30 18:24:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 63488 examples: 0.207 | 0.239\n",
      "2023-12-30 18:24:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 65344 examples: 0.195 | 0.192\n",
      "2023-12-30 18:24:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 67200 examples: 0.205 | 0.204\n",
      "2023-12-30 18:24:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 69056 examples: 0.196 | 0.180\n",
      "2023-12-30 18:24:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 70912 examples: 0.178 | 0.177\n",
      "2023-12-30 18:24:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 72768 examples: 0.147 | 0.234\n",
      "2023-12-30 18:24:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 74624 examples: 0.208 | 0.174\n",
      "2023-12-30 18:24:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 76480 examples: 0.154 | 0.169\n",
      "2023-12-30 18:24:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 78336 examples: 0.179 | 0.222\n",
      "2023-12-30 18:24:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 80192 examples: 0.207 | 0.190\n",
      "2023-12-30 18:24:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 82048 examples: 0.136 | 0.155\n",
      "2023-12-30 18:24:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 83904 examples: 0.161 | 0.157\n",
      "2023-12-30 18:24:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 85760 examples: 0.123 | 0.150\n",
      "2023-12-30 18:24:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 87616 examples: 0.145 | 0.138\n",
      "2023-12-30 18:24:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 89472 examples: 0.159 | 0.178\n",
      "2023-12-30 18:24:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 91328 examples: 0.143 | 0.135\n",
      "2023-12-30 18:24:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 93184 examples: 0.144 | 0.128\n",
      "2023-12-30 18:24:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 95040 examples: 0.130 | 0.129\n",
      "2023-12-30 18:24:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 96896 examples: 0.137 | 0.120\n",
      "2023-12-30 18:24:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 98752 examples: 0.135 | 0.134\n",
      "2023-12-30 18:24:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 100608 examples: 0.139 | 0.133\n",
      "2023-12-30 18:24:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 102464 examples: 0.115 | 0.244\n",
      "2023-12-30 18:24:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 104320 examples: 0.118 | 0.126\n",
      "2023-12-30 18:24:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 106176 examples: 0.125 | 0.149\n",
      "2023-12-30 18:24:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 108032 examples: 0.112 | 0.123\n",
      "2023-12-30 18:24:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 109888 examples: 0.115 | 0.109\n",
      "2023-12-30 18:24:17 - INFO     | Epoch: 1 | Learning Rate: 0.010: Training/Validation Loss after 111744 examples: 0.103 | 0.153\n",
      "2023-12-30 18:24:17 - INFO     | Early stopping: loss decreased (0.301 -> 0.134; -55.6%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:02, 10.12s/it]2023-12-30 18:24:17 - INFO     | Epoch: 2 | Learning Rate: 0.010\n",
      "2023-12-30 18:24:18 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 112064 examples: 0.118 | 0.119\n",
      "2023-12-30 18:24:18 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 113920 examples: 0.090 | 0.122\n",
      "2023-12-30 18:24:18 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 115776 examples: 0.096 | 0.133\n",
      "2023-12-30 18:24:19 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 117632 examples: 0.105 | 0.114\n",
      "2023-12-30 18:24:19 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 119488 examples: 0.119 | 0.149\n",
      "2023-12-30 18:24:19 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 121344 examples: 0.124 | 0.111\n",
      "2023-12-30 18:24:20 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 123200 examples: 0.096 | 0.127\n",
      "2023-12-30 18:24:20 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 125056 examples: 0.136 | 0.117\n",
      "2023-12-30 18:24:20 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 126912 examples: 0.078 | 0.111\n",
      "2023-12-30 18:24:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 128768 examples: 0.070 | 0.107\n",
      "2023-12-30 18:24:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 130624 examples: 0.135 | 0.121\n",
      "2023-12-30 18:24:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 132480 examples: 0.096 | 0.103\n",
      "2023-12-30 18:24:21 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 134336 examples: 0.109 | 0.125\n",
      "2023-12-30 18:24:22 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 136192 examples: 0.089 | 0.117\n",
      "2023-12-30 18:24:22 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 138048 examples: 0.101 | 0.107\n",
      "2023-12-30 18:24:22 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 139904 examples: 0.104 | 0.123\n",
      "2023-12-30 18:24:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 141760 examples: 0.090 | 0.101\n",
      "2023-12-30 18:24:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 143616 examples: 0.114 | 0.141\n",
      "2023-12-30 18:24:23 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 145472 examples: 0.087 | 0.094\n",
      "2023-12-30 18:24:24 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 147328 examples: 0.095 | 0.106\n",
      "2023-12-30 18:24:24 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 149184 examples: 0.105 | 0.105\n",
      "2023-12-30 18:24:24 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 151040 examples: 0.121 | 0.109\n",
      "2023-12-30 18:24:25 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 152896 examples: 0.102 | 0.097\n",
      "2023-12-30 18:24:25 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 154752 examples: 0.094 | 0.089\n",
      "2023-12-30 18:24:25 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 156608 examples: 0.064 | 0.110\n",
      "2023-12-30 18:24:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 158464 examples: 0.075 | 0.107\n",
      "2023-12-30 18:24:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 160320 examples: 0.077 | 0.095\n",
      "2023-12-30 18:24:26 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 162176 examples: 0.095 | 0.096\n",
      "2023-12-30 18:24:27 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 164032 examples: 0.108 | 0.100\n",
      "2023-12-30 18:24:27 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 165888 examples: 0.103 | 0.125\n",
      "2023-12-30 18:24:27 - INFO     | Epoch: 2 | Learning Rate: 0.010: Training/Validation Loss after 167744 examples: 0.118 | 0.094\n",
      "2023-12-30 18:24:27 - INFO     | Early stopping: loss decreased (0.134 -> 0.099; -25.9%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:50, 10.06s/it]2023-12-30 18:24:27 - INFO     | Epoch: 3 | Learning Rate: 0.010\n",
      "2023-12-30 18:24:28 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 168064 examples: 0.104 | 0.092\n",
      "2023-12-30 18:24:28 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 169920 examples: 0.060 | 0.094\n",
      "2023-12-30 18:24:28 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 171776 examples: 0.079 | 0.095\n",
      "2023-12-30 18:24:29 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 173632 examples: 0.067 | 0.106\n",
      "2023-12-30 18:24:29 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 175488 examples: 0.061 | 0.089\n",
      "2023-12-30 18:24:29 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 177344 examples: 0.098 | 0.098\n",
      "2023-12-30 18:24:30 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 179200 examples: 0.084 | 0.107\n",
      "2023-12-30 18:24:30 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 181056 examples: 0.080 | 0.094\n",
      "2023-12-30 18:24:30 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 182912 examples: 0.098 | 0.111\n",
      "2023-12-30 18:24:31 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 184768 examples: 0.079 | 0.114\n",
      "2023-12-30 18:24:31 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 186624 examples: 0.095 | 0.103\n",
      "2023-12-30 18:24:31 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 188480 examples: 0.082 | 0.092\n",
      "2023-12-30 18:24:32 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 190336 examples: 0.074 | 0.101\n",
      "2023-12-30 18:24:32 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 192192 examples: 0.085 | 0.100\n",
      "2023-12-30 18:24:32 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 194048 examples: 0.086 | 0.094\n",
      "2023-12-30 18:24:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 195904 examples: 0.073 | 0.099\n",
      "2023-12-30 18:24:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 197760 examples: 0.093 | 0.088\n",
      "2023-12-30 18:24:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 199616 examples: 0.107 | 0.105\n",
      "2023-12-30 18:24:33 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 201472 examples: 0.093 | 0.091\n",
      "2023-12-30 18:24:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 203328 examples: 0.078 | 0.101\n",
      "2023-12-30 18:24:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 205184 examples: 0.076 | 0.112\n",
      "2023-12-30 18:24:34 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 207040 examples: 0.089 | 0.093\n",
      "2023-12-30 18:24:35 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 208896 examples: 0.071 | 0.095\n",
      "2023-12-30 18:24:35 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 210752 examples: 0.078 | 0.097\n",
      "2023-12-30 18:24:35 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 212608 examples: 0.077 | 0.095\n",
      "2023-12-30 18:24:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 214464 examples: 0.105 | 0.091\n",
      "2023-12-30 18:24:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 216320 examples: 0.082 | 0.080\n",
      "2023-12-30 18:24:36 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 218176 examples: 0.075 | 0.081\n",
      "2023-12-30 18:24:37 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 220032 examples: 0.090 | 0.088\n",
      "2023-12-30 18:24:37 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 221888 examples: 0.079 | 0.102\n",
      "2023-12-30 18:24:37 - INFO     | Epoch: 3 | Learning Rate: 0.010: Training/Validation Loss after 223744 examples: 0.069 | 0.093\n",
      "2023-12-30 18:24:38 - INFO     | Early stopping: no decrease (0.099 vs 0.099); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:41, 10.10s/it]2023-12-30 18:24:38 - INFO     | Epoch: 4 | Learning Rate: 0.010\n",
      "2023-12-30 18:24:38 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 224064 examples: 0.012 | 0.096\n",
      "2023-12-30 18:24:38 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 225920 examples: 0.069 | 0.080\n",
      "2023-12-30 18:24:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 227776 examples: 0.059 | 0.099\n",
      "2023-12-30 18:24:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 229632 examples: 0.079 | 0.087\n",
      "2023-12-30 18:24:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 231488 examples: 0.057 | 0.129\n",
      "2023-12-30 18:24:39 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 233344 examples: 0.073 | 0.095\n",
      "2023-12-30 18:24:40 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 235200 examples: 0.077 | 0.085\n",
      "2023-12-30 18:24:40 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 237056 examples: 0.065 | 0.084\n",
      "2023-12-30 18:24:40 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 238912 examples: 0.064 | 0.081\n",
      "2023-12-30 18:24:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 240768 examples: 0.085 | 0.082\n",
      "2023-12-30 18:24:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 242624 examples: 0.061 | 0.088\n",
      "2023-12-30 18:24:41 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 244480 examples: 0.082 | 0.095\n",
      "2023-12-30 18:24:42 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 246336 examples: 0.072 | 0.099\n",
      "2023-12-30 18:24:42 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 248192 examples: 0.084 | 0.098\n",
      "2023-12-30 18:24:42 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 250048 examples: 0.067 | 0.092\n",
      "2023-12-30 18:24:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 251904 examples: 0.067 | 0.093\n",
      "2023-12-30 18:24:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 253760 examples: 0.075 | 0.082\n",
      "2023-12-30 18:24:43 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 255616 examples: 0.058 | 0.112\n",
      "2023-12-30 18:24:44 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 257472 examples: 0.054 | 0.086\n",
      "2023-12-30 18:24:44 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 259328 examples: 0.093 | 0.102\n",
      "2023-12-30 18:24:44 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 261184 examples: 0.074 | 0.098\n",
      "2023-12-30 18:24:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 263040 examples: 0.073 | 0.087\n",
      "2023-12-30 18:24:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 264896 examples: 0.076 | 0.105\n",
      "2023-12-30 18:24:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 266752 examples: 0.068 | 0.079\n",
      "2023-12-30 18:24:45 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 268608 examples: 0.114 | 0.090\n",
      "2023-12-30 18:24:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 270464 examples: 0.073 | 0.074\n",
      "2023-12-30 18:24:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 272320 examples: 0.056 | 0.096\n",
      "2023-12-30 18:24:46 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 274176 examples: 0.096 | 0.094\n",
      "2023-12-30 18:24:47 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 276032 examples: 0.076 | 0.093\n",
      "2023-12-30 18:24:47 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 277888 examples: 0.068 | 0.080\n",
      "2023-12-30 18:24:47 - INFO     | Epoch: 4 | Learning Rate: 0.010: Training/Validation Loss after 279744 examples: 0.075 | 0.076\n",
      "2023-12-30 18:24:48 - INFO     | Early stopping: no decrease (0.099 vs 0.101); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:50<02:31, 10.08s/it]2023-12-30 18:24:48 - INFO     | Epoch: 5 | Learning Rate: 0.010\n",
      "2023-12-30 18:24:48 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 280064 examples: 0.192 | 0.101\n",
      "2023-12-30 18:24:48 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 281920 examples: 0.060 | 0.083\n",
      "2023-12-30 18:24:49 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 283776 examples: 0.070 | 0.094\n",
      "2023-12-30 18:24:49 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 285632 examples: 0.065 | 0.082\n",
      "2023-12-30 18:24:49 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 287488 examples: 0.063 | 0.107\n",
      "2023-12-30 18:24:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 289344 examples: 0.065 | 0.087\n",
      "2023-12-30 18:24:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 291200 examples: 0.055 | 0.098\n",
      "2023-12-30 18:24:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 293056 examples: 0.066 | 0.086\n",
      "2023-12-30 18:24:50 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 294912 examples: 0.098 | 0.087\n",
      "2023-12-30 18:24:51 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 296768 examples: 0.071 | 0.081\n",
      "2023-12-30 18:24:51 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 298624 examples: 0.053 | 0.088\n",
      "2023-12-30 18:24:51 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 300480 examples: 0.048 | 0.082\n",
      "2023-12-30 18:24:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 302336 examples: 0.077 | 0.089\n",
      "2023-12-30 18:24:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 304192 examples: 0.059 | 0.074\n",
      "2023-12-30 18:24:52 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 306048 examples: 0.076 | 0.082\n",
      "2023-12-30 18:24:53 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 307904 examples: 0.082 | 0.102\n",
      "2023-12-30 18:24:53 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 309760 examples: 0.059 | 0.107\n",
      "2023-12-30 18:24:53 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 311616 examples: 0.075 | 0.131\n",
      "2023-12-30 18:24:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 313472 examples: 0.061 | 0.080\n",
      "2023-12-30 18:24:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 315328 examples: 0.068 | 0.116\n",
      "2023-12-30 18:24:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 317184 examples: 0.068 | 0.079\n",
      "2023-12-30 18:24:54 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 319040 examples: 0.074 | 0.074\n",
      "2023-12-30 18:24:55 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 320896 examples: 0.068 | 0.078\n",
      "2023-12-30 18:24:55 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 322752 examples: 0.055 | 0.080\n",
      "2023-12-30 18:24:55 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 324608 examples: 0.079 | 0.100\n",
      "2023-12-30 18:24:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 326464 examples: 0.081 | 0.084\n",
      "2023-12-30 18:24:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 328320 examples: 0.077 | 0.084\n",
      "2023-12-30 18:24:56 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 330176 examples: 0.045 | 0.081\n",
      "2023-12-30 18:24:57 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 332032 examples: 0.064 | 0.091\n",
      "2023-12-30 18:24:57 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 333888 examples: 0.070 | 0.075\n",
      "2023-12-30 18:24:57 - INFO     | Epoch: 5 | Learning Rate: 0.010: Training/Validation Loss after 335744 examples: 0.051 | 0.084\n",
      "2023-12-30 18:24:58 - INFO     | Early stopping: loss decreased (0.099 -> 0.077; -21.9%). Caching model state.\n",
      " 30%|███       | 6/20 [01:00<02:20, 10.01s/it]2023-12-30 18:24:58 - INFO     | Epoch: 6 | Learning Rate: 0.010\n",
      "2023-12-30 18:24:58 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 336064 examples: 0.131 | 0.075\n",
      "2023-12-30 18:24:58 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 337920 examples: 0.061 | 0.078\n",
      "2023-12-30 18:24:58 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 339776 examples: 0.053 | 0.087\n",
      "2023-12-30 18:24:59 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 341632 examples: 0.050 | 0.090\n",
      "2023-12-30 18:24:59 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 343488 examples: 0.058 | 0.085\n",
      "2023-12-30 18:24:59 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 345344 examples: 0.084 | 0.082\n",
      "2023-12-30 18:25:00 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 347200 examples: 0.074 | 0.085\n",
      "2023-12-30 18:25:00 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 349056 examples: 0.060 | 0.071\n",
      "2023-12-30 18:25:00 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 350912 examples: 0.056 | 0.135\n",
      "2023-12-30 18:25:01 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 352768 examples: 0.061 | 0.084\n",
      "2023-12-30 18:25:01 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 354624 examples: 0.056 | 0.089\n",
      "2023-12-30 18:25:01 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 356480 examples: 0.063 | 0.075\n",
      "2023-12-30 18:25:02 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 358336 examples: 0.052 | 0.086\n",
      "2023-12-30 18:25:02 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 360192 examples: 0.061 | 0.080\n",
      "2023-12-30 18:25:02 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 362048 examples: 0.068 | 0.075\n",
      "2023-12-30 18:25:03 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 363904 examples: 0.073 | 0.080\n",
      "2023-12-30 18:25:03 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 365760 examples: 0.056 | 0.075\n",
      "2023-12-30 18:25:03 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 367616 examples: 0.081 | 0.081\n",
      "2023-12-30 18:25:03 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 369472 examples: 0.053 | 0.074\n",
      "2023-12-30 18:25:04 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 371328 examples: 0.049 | 0.073\n",
      "2023-12-30 18:25:04 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 373184 examples: 0.066 | 0.076\n",
      "2023-12-30 18:25:04 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 375040 examples: 0.051 | 0.088\n",
      "2023-12-30 18:25:05 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 376896 examples: 0.069 | 0.087\n",
      "2023-12-30 18:25:05 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 378752 examples: 0.079 | 0.092\n",
      "2023-12-30 18:25:05 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 380608 examples: 0.064 | 0.077\n",
      "2023-12-30 18:25:06 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 382464 examples: 0.088 | 0.078\n",
      "2023-12-30 18:25:06 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 384320 examples: 0.059 | 0.115\n",
      "2023-12-30 18:25:06 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 386176 examples: 0.062 | 0.097\n",
      "2023-12-30 18:25:07 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 388032 examples: 0.054 | 0.068\n",
      "2023-12-30 18:25:07 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 389888 examples: 0.060 | 0.074\n",
      "2023-12-30 18:25:07 - INFO     | Epoch: 6 | Learning Rate: 0.010: Training/Validation Loss after 391744 examples: 0.063 | 0.069\n",
      "2023-12-30 18:25:08 - INFO     | Early stopping: loss decreased (0.077 -> 0.071; -7.6%). Caching model state.\n",
      " 35%|███▌      | 7/20 [01:10<02:10, 10.03s/it]2023-12-30 18:25:08 - INFO     | Epoch: 7 | Learning Rate: 0.010\n",
      "2023-12-30 18:25:08 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 392064 examples: 0.009 | 0.071\n",
      "2023-12-30 18:25:08 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 393920 examples: 0.043 | 0.074\n",
      "2023-12-30 18:25:09 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 395776 examples: 0.078 | 0.090\n",
      "2023-12-30 18:25:09 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 397632 examples: 0.061 | 0.078\n",
      "2023-12-30 18:25:09 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 399488 examples: 0.069 | 0.079\n",
      "2023-12-30 18:25:09 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 401344 examples: 0.064 | 0.128\n",
      "2023-12-30 18:25:10 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 403200 examples: 0.053 | 0.079\n",
      "2023-12-30 18:25:10 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 405056 examples: 0.059 | 0.084\n",
      "2023-12-30 18:25:10 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 406912 examples: 0.042 | 0.084\n",
      "2023-12-30 18:25:11 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 408768 examples: 0.041 | 0.075\n",
      "2023-12-30 18:25:11 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 410624 examples: 0.044 | 0.076\n",
      "2023-12-30 18:25:11 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 412480 examples: 0.050 | 0.082\n",
      "2023-12-30 18:25:12 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 414336 examples: 0.046 | 0.079\n",
      "2023-12-30 18:25:12 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 416192 examples: 0.072 | 0.074\n",
      "2023-12-30 18:25:12 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 418048 examples: 0.042 | 0.076\n",
      "2023-12-30 18:25:13 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 419904 examples: 0.061 | 0.082\n",
      "2023-12-30 18:25:13 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 421760 examples: 0.057 | 0.100\n",
      "2023-12-30 18:25:13 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 423616 examples: 0.094 | 0.077\n",
      "2023-12-30 18:25:14 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 425472 examples: 0.057 | 0.073\n",
      "2023-12-30 18:25:14 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 427328 examples: 0.045 | 0.080\n",
      "2023-12-30 18:25:14 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 429184 examples: 0.056 | 0.079\n",
      "2023-12-30 18:25:14 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 431040 examples: 0.048 | 0.093\n",
      "2023-12-30 18:25:15 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 432896 examples: 0.062 | 0.072\n",
      "2023-12-30 18:25:15 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 434752 examples: 0.057 | 0.094\n",
      "2023-12-30 18:25:15 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 436608 examples: 0.060 | 0.088\n",
      "2023-12-30 18:25:16 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 438464 examples: 0.064 | 0.119\n",
      "2023-12-30 18:25:16 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 440320 examples: 0.066 | 0.082\n",
      "2023-12-30 18:25:16 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 442176 examples: 0.061 | 0.094\n",
      "2023-12-30 18:25:17 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 444032 examples: 0.047 | 0.090\n",
      "2023-12-30 18:25:17 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 445888 examples: 0.096 | 0.130\n",
      "2023-12-30 18:25:17 - INFO     | Epoch: 7 | Learning Rate: 0.010: Training/Validation Loss after 447744 examples: 0.085 | 0.085\n",
      "2023-12-30 18:25:18 - INFO     | Early stopping: no decrease (0.071 vs 0.087); counter: 1 out of 3\n",
      " 40%|████      | 8/20 [01:20<02:00, 10.01s/it]2023-12-30 18:25:18 - INFO     | Epoch: 8 | Learning Rate: 0.010\n",
      "2023-12-30 18:25:18 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 448064 examples: 0.008 | 0.084\n",
      "2023-12-30 18:25:18 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 449920 examples: 0.054 | 0.074\n",
      "2023-12-30 18:25:18 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 451776 examples: 0.046 | 0.079\n",
      "2023-12-30 18:25:19 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 453632 examples: 0.056 | 0.076\n",
      "2023-12-30 18:25:19 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 455488 examples: 0.045 | 0.076\n",
      "2023-12-30 18:25:19 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 457344 examples: 0.051 | 0.076\n",
      "2023-12-30 18:25:20 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 459200 examples: 0.049 | 0.086\n",
      "2023-12-30 18:25:20 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 461056 examples: 0.062 | 0.085\n",
      "2023-12-30 18:25:20 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 462912 examples: 0.056 | 0.089\n",
      "2023-12-30 18:25:21 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 464768 examples: 0.062 | 0.079\n",
      "2023-12-30 18:25:21 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 466624 examples: 0.050 | 0.078\n",
      "2023-12-30 18:25:21 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 468480 examples: 0.063 | 0.076\n",
      "2023-12-30 18:25:22 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 470336 examples: 0.080 | 0.072\n",
      "2023-12-30 18:25:22 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 472192 examples: 0.043 | 0.074\n",
      "2023-12-30 18:25:22 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 474048 examples: 0.074 | 0.079\n",
      "2023-12-30 18:25:23 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 475904 examples: 0.046 | 0.069\n",
      "2023-12-30 18:25:23 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 477760 examples: 0.059 | 0.127\n",
      "2023-12-30 18:25:23 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 479616 examples: 0.066 | 0.101\n",
      "2023-12-30 18:25:24 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 481472 examples: 0.044 | 0.085\n",
      "2023-12-30 18:25:24 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 483328 examples: 0.051 | 0.082\n",
      "2023-12-30 18:25:24 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 485184 examples: 0.055 | 0.071\n",
      "2023-12-30 18:25:24 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 487040 examples: 0.050 | 0.073\n",
      "2023-12-30 18:25:25 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 488896 examples: 0.053 | 0.106\n",
      "2023-12-30 18:25:25 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 490752 examples: 0.056 | 0.086\n",
      "2023-12-30 18:25:25 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 492608 examples: 0.069 | 0.074\n",
      "2023-12-30 18:25:26 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 494464 examples: 0.075 | 0.086\n",
      "2023-12-30 18:25:26 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 496320 examples: 0.057 | 0.086\n",
      "2023-12-30 18:25:26 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 498176 examples: 0.051 | 0.081\n",
      "2023-12-30 18:25:27 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 500032 examples: 0.065 | 0.091\n",
      "2023-12-30 18:25:27 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 501888 examples: 0.055 | 0.080\n",
      "2023-12-30 18:25:27 - INFO     | Epoch: 8 | Learning Rate: 0.010: Training/Validation Loss after 503744 examples: 0.047 | 0.120\n",
      "2023-12-30 18:25:28 - INFO     | Early stopping: no decrease (0.071 vs 0.081); counter: 2 out of 3\n",
      " 45%|████▌     | 9/20 [01:30<01:50, 10.00s/it]2023-12-30 18:25:28 - INFO     | Epoch: 9 | Learning Rate: 0.010\n",
      "2023-12-30 18:25:28 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 504064 examples: 0.128 | 0.091\n",
      "2023-12-30 18:25:28 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 505920 examples: 0.046 | 0.087\n",
      "2023-12-30 18:25:28 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 507776 examples: 0.056 | 0.074\n",
      "2023-12-30 18:25:29 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 509632 examples: 0.052 | 0.074\n",
      "2023-12-30 18:25:29 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 511488 examples: 0.046 | 0.090\n",
      "2023-12-30 18:25:29 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 513344 examples: 0.049 | 0.092\n",
      "2023-12-30 18:25:30 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 515200 examples: 0.059 | 0.107\n",
      "2023-12-30 18:25:30 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 517056 examples: 0.077 | 0.074\n",
      "2023-12-30 18:25:30 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 518912 examples: 0.047 | 0.077\n",
      "2023-12-30 18:25:31 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 520768 examples: 0.049 | 0.084\n",
      "2023-12-30 18:25:31 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 522624 examples: 0.066 | 0.073\n",
      "2023-12-30 18:25:31 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 524480 examples: 0.057 | 0.111\n",
      "2023-12-30 18:25:32 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 526336 examples: 0.059 | 0.103\n",
      "2023-12-30 18:25:32 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 528192 examples: 0.069 | 0.092\n",
      "2023-12-30 18:25:32 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 530048 examples: 0.058 | 0.094\n",
      "2023-12-30 18:25:32 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 531904 examples: 0.048 | 0.089\n",
      "2023-12-30 18:25:33 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 533760 examples: 0.069 | 0.076\n",
      "2023-12-30 18:25:33 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 535616 examples: 0.051 | 0.077\n",
      "2023-12-30 18:25:33 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 537472 examples: 0.059 | 0.086\n",
      "2023-12-30 18:25:34 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 539328 examples: 0.052 | 0.086\n",
      "2023-12-30 18:25:34 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 541184 examples: 0.053 | 0.069\n",
      "2023-12-30 18:25:34 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 543040 examples: 0.058 | 0.080\n",
      "2023-12-30 18:25:35 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 544896 examples: 0.056 | 0.077\n",
      "2023-12-30 18:25:35 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 546752 examples: 0.060 | 0.113\n",
      "2023-12-30 18:25:35 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 548608 examples: 0.045 | 0.070\n",
      "2023-12-30 18:25:36 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 550464 examples: 0.052 | 0.097\n",
      "2023-12-30 18:25:36 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 552320 examples: 0.062 | 0.074\n",
      "2023-12-30 18:25:36 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 554176 examples: 0.087 | 0.085\n",
      "2023-12-30 18:25:37 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 556032 examples: 0.050 | 0.085\n",
      "2023-12-30 18:25:37 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 557888 examples: 0.048 | 0.083\n",
      "2023-12-30 18:25:37 - INFO     | Epoch: 9 | Learning Rate: 0.010: Training/Validation Loss after 559744 examples: 0.048 | 0.085\n",
      "2023-12-30 18:25:37 - INFO     | Early stopping: no decrease (0.071 vs 0.090); counter: 3 out of 3\n",
      "2023-12-30 18:25:37 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:25:37 - INFO     | Reducing learning rate: 0.01 -> 0.005\n",
      " 50%|█████     | 10/20 [01:40<01:39,  9.97s/it]2023-12-30 18:25:37 - INFO     | Epoch: 10 | Learning Rate: 0.005\n",
      "2023-12-30 18:25:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 560064 examples: 0.050 | 0.088\n",
      "2023-12-30 18:25:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 561920 examples: 0.040 | 0.066\n",
      "2023-12-30 18:25:38 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 563776 examples: 0.037 | 0.064\n",
      "2023-12-30 18:25:39 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 565632 examples: 0.039 | 0.078\n",
      "2023-12-30 18:25:39 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 567488 examples: 0.057 | 0.072\n",
      "2023-12-30 18:25:39 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 569344 examples: 0.029 | 0.079\n",
      "2023-12-30 18:25:40 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 571200 examples: 0.028 | 0.065\n",
      "2023-12-30 18:25:40 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 573056 examples: 0.028 | 0.065\n",
      "2023-12-30 18:25:40 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 574912 examples: 0.035 | 0.067\n",
      "2023-12-30 18:25:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 576768 examples: 0.024 | 0.068\n",
      "2023-12-30 18:25:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 578624 examples: 0.029 | 0.068\n",
      "2023-12-30 18:25:41 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 580480 examples: 0.033 | 0.067\n",
      "2023-12-30 18:25:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 582336 examples: 0.043 | 0.074\n",
      "2023-12-30 18:25:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 584192 examples: 0.052 | 0.064\n",
      "2023-12-30 18:25:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 586048 examples: 0.027 | 0.067\n",
      "2023-12-30 18:25:42 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 587904 examples: 0.030 | 0.069\n",
      "2023-12-30 18:25:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 589760 examples: 0.027 | 0.070\n",
      "2023-12-30 18:25:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 591616 examples: 0.027 | 0.066\n",
      "2023-12-30 18:25:43 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 593472 examples: 0.048 | 0.067\n",
      "2023-12-30 18:25:44 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 595328 examples: 0.040 | 0.067\n",
      "2023-12-30 18:25:44 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 597184 examples: 0.038 | 0.068\n",
      "2023-12-30 18:25:44 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 599040 examples: 0.044 | 0.063\n",
      "2023-12-30 18:25:45 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 600896 examples: 0.021 | 0.064\n",
      "2023-12-30 18:25:45 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 602752 examples: 0.050 | 0.066\n",
      "2023-12-30 18:25:45 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 604608 examples: 0.024 | 0.069\n",
      "2023-12-30 18:25:46 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 606464 examples: 0.055 | 0.080\n",
      "2023-12-30 18:25:46 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 608320 examples: 0.032 | 0.064\n",
      "2023-12-30 18:25:46 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 610176 examples: 0.035 | 0.067\n",
      "2023-12-30 18:25:47 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 612032 examples: 0.049 | 0.072\n",
      "2023-12-30 18:25:47 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 613888 examples: 0.034 | 0.069\n",
      "2023-12-30 18:25:47 - INFO     | Epoch: 10 | Learning Rate: 0.005: Training/Validation Loss after 615744 examples: 0.023 | 0.064\n",
      "2023-12-30 18:25:48 - INFO     | Early stopping: loss decreased (0.071 -> 0.066; -7.1%). Caching model state.\n",
      " 55%|█████▌    | 11/20 [01:50<01:30, 10.00s/it]2023-12-30 18:25:48 - INFO     | Epoch: 11 | Learning Rate: 0.005\n",
      "2023-12-30 18:25:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 616064 examples: 0.003 | 0.067\n",
      "2023-12-30 18:25:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 617920 examples: 0.028 | 0.070\n",
      "2023-12-30 18:25:48 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 619776 examples: 0.037 | 0.067\n",
      "2023-12-30 18:25:49 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 621632 examples: 0.024 | 0.066\n",
      "2023-12-30 18:25:49 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 623488 examples: 0.039 | 0.067\n",
      "2023-12-30 18:25:49 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 625344 examples: 0.031 | 0.069\n",
      "2023-12-30 18:25:50 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 627200 examples: 0.034 | 0.075\n",
      "2023-12-30 18:25:50 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 629056 examples: 0.038 | 0.073\n",
      "2023-12-30 18:25:50 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 630912 examples: 0.030 | 0.069\n",
      "2023-12-30 18:25:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 632768 examples: 0.026 | 0.069\n",
      "2023-12-30 18:25:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 634624 examples: 0.034 | 0.072\n",
      "2023-12-30 18:25:51 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 636480 examples: 0.030 | 0.063\n",
      "2023-12-30 18:25:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 638336 examples: 0.031 | 0.072\n",
      "2023-12-30 18:25:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 640192 examples: 0.042 | 0.068\n",
      "2023-12-30 18:25:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 642048 examples: 0.030 | 0.066\n",
      "2023-12-30 18:25:52 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 643904 examples: 0.031 | 0.066\n",
      "2023-12-30 18:25:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 645760 examples: 0.035 | 0.069\n",
      "2023-12-30 18:25:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 647616 examples: 0.035 | 0.077\n",
      "2023-12-30 18:25:53 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 649472 examples: 0.040 | 0.069\n",
      "2023-12-30 18:25:54 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 651328 examples: 0.031 | 0.072\n",
      "2023-12-30 18:25:54 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 653184 examples: 0.030 | 0.068\n",
      "2023-12-30 18:25:54 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 655040 examples: 0.034 | 0.069\n",
      "2023-12-30 18:25:55 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 656896 examples: 0.035 | 0.068\n",
      "2023-12-30 18:25:55 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 658752 examples: 0.044 | 0.069\n",
      "2023-12-30 18:25:55 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 660608 examples: 0.031 | 0.066\n",
      "2023-12-30 18:25:56 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 662464 examples: 0.033 | 0.067\n",
      "2023-12-30 18:25:56 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 664320 examples: 0.031 | 0.070\n",
      "2023-12-30 18:25:56 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 666176 examples: 0.033 | 0.065\n",
      "2023-12-30 18:25:57 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 668032 examples: 0.030 | 0.066\n",
      "2023-12-30 18:25:57 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 669888 examples: 0.029 | 0.063\n",
      "2023-12-30 18:25:57 - INFO     | Epoch: 11 | Learning Rate: 0.005: Training/Validation Loss after 671744 examples: 0.026 | 0.071\n",
      "2023-12-30 18:25:58 - INFO     | Early stopping: no decrease (0.066 vs 0.071); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [02:00<01:20, 10.00s/it]2023-12-30 18:25:58 - INFO     | Epoch: 12 | Learning Rate: 0.005\n",
      "2023-12-30 18:25:58 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 672064 examples: 0.024 | 0.067\n",
      "2023-12-30 18:25:58 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 673920 examples: 0.021 | 0.074\n",
      "2023-12-30 18:25:58 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 675776 examples: 0.035 | 0.071\n",
      "2023-12-30 18:25:59 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 677632 examples: 0.037 | 0.068\n",
      "2023-12-30 18:25:59 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 679488 examples: 0.023 | 0.069\n",
      "2023-12-30 18:25:59 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 681344 examples: 0.042 | 0.071\n",
      "2023-12-30 18:26:00 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 683200 examples: 0.032 | 0.082\n",
      "2023-12-30 18:26:00 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 685056 examples: 0.023 | 0.069\n",
      "2023-12-30 18:26:00 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 686912 examples: 0.031 | 0.081\n",
      "2023-12-30 18:26:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 688768 examples: 0.047 | 0.065\n",
      "2023-12-30 18:26:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 690624 examples: 0.031 | 0.070\n",
      "2023-12-30 18:26:01 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 692480 examples: 0.023 | 0.076\n",
      "2023-12-30 18:26:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 694336 examples: 0.034 | 0.083\n",
      "2023-12-30 18:26:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 696192 examples: 0.038 | 0.075\n",
      "2023-12-30 18:26:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 698048 examples: 0.018 | 0.071\n",
      "2023-12-30 18:26:02 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 699904 examples: 0.047 | 0.083\n",
      "2023-12-30 18:26:03 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 701760 examples: 0.024 | 0.068\n",
      "2023-12-30 18:26:03 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 703616 examples: 0.030 | 0.074\n",
      "2023-12-30 18:26:03 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 705472 examples: 0.031 | 0.074\n",
      "2023-12-30 18:26:04 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 707328 examples: 0.028 | 0.069\n",
      "2023-12-30 18:26:04 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 709184 examples: 0.035 | 0.070\n",
      "2023-12-30 18:26:04 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 711040 examples: 0.028 | 0.070\n",
      "2023-12-30 18:26:05 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 712896 examples: 0.033 | 0.070\n",
      "2023-12-30 18:26:05 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 714752 examples: 0.039 | 0.070\n",
      "2023-12-30 18:26:05 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 716608 examples: 0.029 | 0.071\n",
      "2023-12-30 18:26:06 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 718464 examples: 0.035 | 0.072\n",
      "2023-12-30 18:26:06 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 720320 examples: 0.045 | 0.072\n",
      "2023-12-30 18:26:06 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 722176 examples: 0.031 | 0.070\n",
      "2023-12-30 18:26:07 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 724032 examples: 0.031 | 0.070\n",
      "2023-12-30 18:26:07 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 725888 examples: 0.044 | 0.069\n",
      "2023-12-30 18:26:07 - INFO     | Epoch: 12 | Learning Rate: 0.005: Training/Validation Loss after 727744 examples: 0.031 | 0.072\n",
      "2023-12-30 18:26:08 - INFO     | Early stopping: no decrease (0.066 vs 0.072); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:10<01:10, 10.02s/it]2023-12-30 18:26:08 - INFO     | Epoch: 13 | Learning Rate: 0.005\n",
      "2023-12-30 18:26:08 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 728064 examples: 0.013 | 0.073\n",
      "2023-12-30 18:26:08 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 729920 examples: 0.022 | 0.071\n",
      "2023-12-30 18:26:08 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 731776 examples: 0.029 | 0.083\n",
      "2023-12-30 18:26:09 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 733632 examples: 0.025 | 0.065\n",
      "2023-12-30 18:26:09 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 735488 examples: 0.031 | 0.068\n",
      "2023-12-30 18:26:09 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 737344 examples: 0.040 | 0.072\n",
      "2023-12-30 18:26:10 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 739200 examples: 0.023 | 0.072\n",
      "2023-12-30 18:26:10 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 741056 examples: 0.040 | 0.080\n",
      "2023-12-30 18:26:10 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 742912 examples: 0.020 | 0.072\n",
      "2023-12-30 18:26:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 744768 examples: 0.032 | 0.079\n",
      "2023-12-30 18:26:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 746624 examples: 0.032 | 0.073\n",
      "2023-12-30 18:26:11 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 748480 examples: 0.020 | 0.077\n",
      "2023-12-30 18:26:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 750336 examples: 0.038 | 0.071\n",
      "2023-12-30 18:26:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 752192 examples: 0.032 | 0.075\n",
      "2023-12-30 18:26:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 754048 examples: 0.037 | 0.071\n",
      "2023-12-30 18:26:12 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 755904 examples: 0.024 | 0.070\n",
      "2023-12-30 18:26:13 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 757760 examples: 0.030 | 0.071\n",
      "2023-12-30 18:26:13 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 759616 examples: 0.031 | 0.071\n",
      "2023-12-30 18:26:13 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 761472 examples: 0.036 | 0.074\n",
      "2023-12-30 18:26:14 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 763328 examples: 0.035 | 0.074\n",
      "2023-12-30 18:26:14 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 765184 examples: 0.037 | 0.080\n",
      "2023-12-30 18:26:14 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 767040 examples: 0.027 | 0.070\n",
      "2023-12-30 18:26:15 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 768896 examples: 0.026 | 0.070\n",
      "2023-12-30 18:26:15 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 770752 examples: 0.025 | 0.072\n",
      "2023-12-30 18:26:15 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 772608 examples: 0.031 | 0.076\n",
      "2023-12-30 18:26:16 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 774464 examples: 0.024 | 0.076\n",
      "2023-12-30 18:26:16 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 776320 examples: 0.026 | 0.099\n",
      "2023-12-30 18:26:16 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 778176 examples: 0.045 | 0.074\n",
      "2023-12-30 18:26:16 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 780032 examples: 0.048 | 0.080\n",
      "2023-12-30 18:26:17 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 781888 examples: 0.027 | 0.072\n",
      "2023-12-30 18:26:17 - INFO     | Epoch: 13 | Learning Rate: 0.005: Training/Validation Loss after 783744 examples: 0.027 | 0.076\n",
      "2023-12-30 18:26:17 - INFO     | Early stopping: no decrease (0.066 vs 0.070); counter: 3 out of 3\n",
      "2023-12-30 18:26:17 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:26:17 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 70%|███████   | 14/20 [02:20<00:59,  9.93s/it]2023-12-30 18:26:17 - INFO     | Epoch: 14 | Learning Rate: 0.003\n",
      "2023-12-30 18:26:18 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 784064 examples: 0.001 | 0.070\n",
      "2023-12-30 18:26:18 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 785920 examples: 0.025 | 0.070\n",
      "2023-12-30 18:26:18 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 787776 examples: 0.027 | 0.068\n",
      "2023-12-30 18:26:19 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 789632 examples: 0.018 | 0.070\n",
      "2023-12-30 18:26:19 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 791488 examples: 0.025 | 0.067\n",
      "2023-12-30 18:26:19 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 793344 examples: 0.019 | 0.069\n",
      "2023-12-30 18:26:20 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 795200 examples: 0.024 | 0.066\n",
      "2023-12-30 18:26:20 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 797056 examples: 0.031 | 0.068\n",
      "2023-12-30 18:26:20 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 798912 examples: 0.019 | 0.068\n",
      "2023-12-30 18:26:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 800768 examples: 0.036 | 0.067\n",
      "2023-12-30 18:26:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 802624 examples: 0.015 | 0.068\n",
      "2023-12-30 18:26:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 804480 examples: 0.030 | 0.066\n",
      "2023-12-30 18:26:21 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 806336 examples: 0.024 | 0.067\n",
      "2023-12-30 18:26:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 808192 examples: 0.025 | 0.066\n",
      "2023-12-30 18:26:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 810048 examples: 0.029 | 0.068\n",
      "2023-12-30 18:26:22 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 811904 examples: 0.015 | 0.066\n",
      "2023-12-30 18:26:23 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 813760 examples: 0.019 | 0.065\n",
      "2023-12-30 18:26:23 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 815616 examples: 0.023 | 0.072\n",
      "2023-12-30 18:26:23 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 817472 examples: 0.013 | 0.067\n",
      "2023-12-30 18:26:24 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 819328 examples: 0.019 | 0.065\n",
      "2023-12-30 18:26:24 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 821184 examples: 0.023 | 0.066\n",
      "2023-12-30 18:26:24 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 823040 examples: 0.018 | 0.069\n",
      "2023-12-30 18:26:25 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 824896 examples: 0.034 | 0.075\n",
      "2023-12-30 18:26:25 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 826752 examples: 0.023 | 0.071\n",
      "2023-12-30 18:26:25 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 828608 examples: 0.020 | 0.069\n",
      "2023-12-30 18:26:25 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 830464 examples: 0.025 | 0.069\n",
      "2023-12-30 18:26:26 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 832320 examples: 0.025 | 0.070\n",
      "2023-12-30 18:26:26 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 834176 examples: 0.029 | 0.065\n",
      "2023-12-30 18:26:26 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 836032 examples: 0.031 | 0.066\n",
      "2023-12-30 18:26:27 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 837888 examples: 0.027 | 0.069\n",
      "2023-12-30 18:26:27 - INFO     | Epoch: 14 | Learning Rate: 0.003: Training/Validation Loss after 839744 examples: 0.020 | 0.071\n",
      "2023-12-30 18:26:27 - INFO     | Early stopping: no decrease (0.066 vs 0.070); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:30<00:49,  9.96s/it]2023-12-30 18:26:27 - INFO     | Epoch: 15 | Learning Rate: 0.003\n",
      "2023-12-30 18:26:28 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 840064 examples: 0.084 | 0.068\n",
      "2023-12-30 18:26:28 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 841920 examples: 0.014 | 0.066\n",
      "2023-12-30 18:26:28 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 843776 examples: 0.018 | 0.070\n",
      "2023-12-30 18:26:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 845632 examples: 0.021 | 0.068\n",
      "2023-12-30 18:26:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 847488 examples: 0.021 | 0.067\n",
      "2023-12-30 18:26:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 849344 examples: 0.022 | 0.072\n",
      "2023-12-30 18:26:29 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 851200 examples: 0.021 | 0.069\n",
      "2023-12-30 18:26:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 853056 examples: 0.022 | 0.072\n",
      "2023-12-30 18:26:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 854912 examples: 0.030 | 0.075\n",
      "2023-12-30 18:26:30 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 856768 examples: 0.021 | 0.070\n",
      "2023-12-30 18:26:31 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 858624 examples: 0.023 | 0.068\n",
      "2023-12-30 18:26:31 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 860480 examples: 0.023 | 0.070\n",
      "2023-12-30 18:26:31 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 862336 examples: 0.021 | 0.070\n",
      "2023-12-30 18:26:32 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 864192 examples: 0.023 | 0.068\n",
      "2023-12-30 18:26:32 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 866048 examples: 0.016 | 0.069\n",
      "2023-12-30 18:26:32 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 867904 examples: 0.015 | 0.066\n",
      "2023-12-30 18:26:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 869760 examples: 0.026 | 0.069\n",
      "2023-12-30 18:26:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 871616 examples: 0.023 | 0.066\n",
      "2023-12-30 18:26:33 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 873472 examples: 0.019 | 0.068\n",
      "2023-12-30 18:26:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 875328 examples: 0.011 | 0.072\n",
      "2023-12-30 18:26:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 877184 examples: 0.023 | 0.070\n",
      "2023-12-30 18:26:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 879040 examples: 0.021 | 0.069\n",
      "2023-12-30 18:26:34 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 880896 examples: 0.035 | 0.075\n",
      "2023-12-30 18:26:35 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 882752 examples: 0.017 | 0.073\n",
      "2023-12-30 18:26:35 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 884608 examples: 0.020 | 0.072\n",
      "2023-12-30 18:26:35 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 886464 examples: 0.026 | 0.076\n",
      "2023-12-30 18:26:36 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 888320 examples: 0.023 | 0.070\n",
      "2023-12-30 18:26:36 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 890176 examples: 0.043 | 0.067\n",
      "2023-12-30 18:26:36 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 892032 examples: 0.018 | 0.067\n",
      "2023-12-30 18:26:37 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 893888 examples: 0.027 | 0.065\n",
      "2023-12-30 18:26:37 - INFO     | Epoch: 15 | Learning Rate: 0.003: Training/Validation Loss after 895744 examples: 0.027 | 0.071\n",
      "2023-12-30 18:26:37 - INFO     | Early stopping: no decrease (0.066 vs 0.074); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:40<00:39,  9.96s/it]2023-12-30 18:26:37 - INFO     | Epoch: 16 | Learning Rate: 0.003\n",
      "2023-12-30 18:26:38 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 896064 examples: 0.028 | 0.074\n",
      "2023-12-30 18:26:38 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 897920 examples: 0.014 | 0.070\n",
      "2023-12-30 18:26:38 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 899776 examples: 0.020 | 0.071\n",
      "2023-12-30 18:26:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 901632 examples: 0.020 | 0.069\n",
      "2023-12-30 18:26:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 903488 examples: 0.021 | 0.072\n",
      "2023-12-30 18:26:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 905344 examples: 0.015 | 0.069\n",
      "2023-12-30 18:26:39 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 907200 examples: 0.038 | 0.072\n",
      "2023-12-30 18:26:40 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 909056 examples: 0.018 | 0.072\n",
      "2023-12-30 18:26:40 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 910912 examples: 0.027 | 0.071\n",
      "2023-12-30 18:26:40 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 912768 examples: 0.021 | 0.070\n",
      "2023-12-30 18:26:41 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 914624 examples: 0.015 | 0.070\n",
      "2023-12-30 18:26:41 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 916480 examples: 0.018 | 0.071\n",
      "2023-12-30 18:26:41 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 918336 examples: 0.019 | 0.070\n",
      "2023-12-30 18:26:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 920192 examples: 0.020 | 0.069\n",
      "2023-12-30 18:26:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 922048 examples: 0.025 | 0.070\n",
      "2023-12-30 18:26:42 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 923904 examples: 0.041 | 0.070\n",
      "2023-12-30 18:26:43 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 925760 examples: 0.028 | 0.073\n",
      "2023-12-30 18:26:43 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 927616 examples: 0.019 | 0.071\n",
      "2023-12-30 18:26:43 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 929472 examples: 0.028 | 0.074\n",
      "2023-12-30 18:26:44 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 931328 examples: 0.024 | 0.070\n",
      "2023-12-30 18:26:44 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 933184 examples: 0.014 | 0.069\n",
      "2023-12-30 18:26:44 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 935040 examples: 0.017 | 0.071\n",
      "2023-12-30 18:26:45 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 936896 examples: 0.011 | 0.069\n",
      "2023-12-30 18:26:45 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 938752 examples: 0.021 | 0.071\n",
      "2023-12-30 18:26:45 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 940608 examples: 0.024 | 0.070\n",
      "2023-12-30 18:26:45 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 942464 examples: 0.013 | 0.074\n",
      "2023-12-30 18:26:46 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 944320 examples: 0.026 | 0.072\n",
      "2023-12-30 18:26:46 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 946176 examples: 0.015 | 0.076\n",
      "2023-12-30 18:26:46 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 948032 examples: 0.024 | 0.068\n",
      "2023-12-30 18:26:47 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 949888 examples: 0.029 | 0.069\n",
      "2023-12-30 18:26:47 - INFO     | Epoch: 16 | Learning Rate: 0.003: Training/Validation Loss after 951744 examples: 0.023 | 0.075\n",
      "2023-12-30 18:26:47 - INFO     | Early stopping: no decrease (0.066 vs 0.077); counter: 3 out of 3\n",
      "2023-12-30 18:26:47 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:26:47 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 85%|████████▌ | 17/20 [02:50<00:29,  9.98s/it]2023-12-30 18:26:47 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:26:48 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.043 | 0.077\n",
      "2023-12-30 18:26:48 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.016 | 0.072\n",
      "2023-12-30 18:26:48 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.015 | 0.069\n",
      "2023-12-30 18:26:49 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.012 | 0.069\n",
      "2023-12-30 18:26:49 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.011 | 0.069\n",
      "2023-12-30 18:26:49 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.021 | 0.070\n",
      "2023-12-30 18:26:50 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.018 | 0.069\n",
      "2023-12-30 18:26:50 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.019 | 0.069\n",
      "2023-12-30 18:26:50 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.020 | 0.070\n",
      "2023-12-30 18:26:50 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.020 | 0.071\n",
      "2023-12-30 18:26:51 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.017 | 0.071\n",
      "2023-12-30 18:26:51 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.019 | 0.071\n",
      "2023-12-30 18:26:51 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.013 | 0.072\n",
      "2023-12-30 18:26:52 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.013 | 0.072\n",
      "2023-12-30 18:26:52 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.020 | 0.072\n",
      "2023-12-30 18:26:52 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.018 | 0.071\n",
      "2023-12-30 18:26:53 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.016 | 0.071\n",
      "2023-12-30 18:26:53 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.019 | 0.070\n",
      "2023-12-30 18:26:53 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.020 | 0.069\n",
      "2023-12-30 18:26:54 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.018 | 0.069\n",
      "2023-12-30 18:26:54 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.018 | 0.069\n",
      "2023-12-30 18:26:54 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.020 | 0.069\n",
      "2023-12-30 18:26:55 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.028 | 0.068\n",
      "2023-12-30 18:26:55 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.021 | 0.069\n",
      "2023-12-30 18:26:55 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.018 | 0.069\n",
      "2023-12-30 18:26:56 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.014 | 0.070\n",
      "2023-12-30 18:26:56 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.028 | 0.069\n",
      "2023-12-30 18:26:56 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.015 | 0.069\n",
      "2023-12-30 18:26:56 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.022 | 0.070\n",
      "2023-12-30 18:26:57 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.013 | 0.070\n",
      "2023-12-30 18:26:57 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.023 | 0.071\n",
      "2023-12-30 18:26:57 - INFO     | Early stopping: no decrease (0.066 vs 0.072); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [03:00<00:20, 10.02s/it]2023-12-30 18:26:57 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 18:26:58 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.006 | 0.072\n",
      "2023-12-30 18:26:58 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.018 | 0.072\n",
      "2023-12-30 18:26:58 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.014 | 0.071\n",
      "2023-12-30 18:26:59 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.011 | 0.073\n",
      "2023-12-30 18:26:59 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.023 | 0.071\n",
      "2023-12-30 18:26:59 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.016 | 0.072\n",
      "2023-12-30 18:27:00 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.015 | 0.071\n",
      "2023-12-30 18:27:00 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.017 | 0.070\n",
      "2023-12-30 18:27:00 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.014 | 0.071\n",
      "2023-12-30 18:27:01 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.022 | 0.073\n",
      "2023-12-30 18:27:01 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.015 | 0.072\n",
      "2023-12-30 18:27:01 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.015 | 0.072\n",
      "2023-12-30 18:27:02 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.022 | 0.070\n",
      "2023-12-30 18:27:02 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.018 | 0.072\n",
      "2023-12-30 18:27:02 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.012 | 0.072\n",
      "2023-12-30 18:27:03 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.024 | 0.074\n",
      "2023-12-30 18:27:03 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.022 | 0.071\n",
      "2023-12-30 18:27:03 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.022 | 0.071\n",
      "2023-12-30 18:27:03 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.016 | 0.070\n",
      "2023-12-30 18:27:04 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.018 | 0.072\n",
      "2023-12-30 18:27:04 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.019 | 0.071\n",
      "2023-12-30 18:27:04 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.016 | 0.069\n",
      "2023-12-30 18:27:05 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.019 | 0.072\n",
      "2023-12-30 18:27:05 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.023 | 0.070\n",
      "2023-12-30 18:27:05 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.014 | 0.071\n",
      "2023-12-30 18:27:06 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1054464 examples: 0.016 | 0.071\n",
      "2023-12-30 18:27:06 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1056320 examples: 0.035 | 0.070\n",
      "2023-12-30 18:27:06 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1058176 examples: 0.016 | 0.071\n",
      "2023-12-30 18:27:07 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1060032 examples: 0.015 | 0.069\n",
      "2023-12-30 18:27:07 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1061888 examples: 0.014 | 0.072\n",
      "2023-12-30 18:27:07 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1063744 examples: 0.010 | 0.072\n",
      "2023-12-30 18:27:08 - INFO     | Early stopping: no decrease (0.066 vs 0.071); counter: 2 out of 3\n",
      " 95%|█████████▌| 19/20 [03:10<00:10, 10.05s/it]2023-12-30 18:27:08 - INFO     | Epoch: 19 | Learning Rate: 0.001\n",
      "2023-12-30 18:27:08 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1064064 examples: 0.002 | 0.071\n",
      "2023-12-30 18:27:08 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1065920 examples: 0.013 | 0.072\n",
      "2023-12-30 18:27:08 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1067776 examples: 0.018 | 0.070\n",
      "2023-12-30 18:27:09 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1069632 examples: 0.022 | 0.072\n",
      "2023-12-30 18:27:09 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1071488 examples: 0.019 | 0.072\n",
      "2023-12-30 18:27:09 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1073344 examples: 0.010 | 0.071\n",
      "2023-12-30 18:27:10 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1075200 examples: 0.012 | 0.071\n",
      "2023-12-30 18:27:10 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1077056 examples: 0.010 | 0.071\n",
      "2023-12-30 18:27:10 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1078912 examples: 0.011 | 0.071\n",
      "2023-12-30 18:27:11 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1080768 examples: 0.019 | 0.071\n",
      "2023-12-30 18:27:11 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1082624 examples: 0.023 | 0.071\n",
      "2023-12-30 18:27:11 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1084480 examples: 0.013 | 0.072\n",
      "2023-12-30 18:27:12 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1086336 examples: 0.016 | 0.073\n",
      "2023-12-30 18:27:12 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1088192 examples: 0.014 | 0.071\n",
      "2023-12-30 18:27:12 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1090048 examples: 0.015 | 0.072\n",
      "2023-12-30 18:27:13 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1091904 examples: 0.018 | 0.073\n",
      "2023-12-30 18:27:13 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1093760 examples: 0.021 | 0.077\n",
      "2023-12-30 18:27:13 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1095616 examples: 0.028 | 0.073\n",
      "2023-12-30 18:27:13 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1097472 examples: 0.015 | 0.073\n",
      "2023-12-30 18:27:14 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1099328 examples: 0.016 | 0.071\n",
      "2023-12-30 18:27:14 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1101184 examples: 0.038 | 0.073\n",
      "2023-12-30 18:27:14 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1103040 examples: 0.015 | 0.071\n",
      "2023-12-30 18:27:15 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1104896 examples: 0.010 | 0.072\n",
      "2023-12-30 18:27:15 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1106752 examples: 0.008 | 0.072\n",
      "2023-12-30 18:27:15 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1108608 examples: 0.021 | 0.072\n",
      "2023-12-30 18:27:16 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1110464 examples: 0.010 | 0.072\n",
      "2023-12-30 18:27:16 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1112320 examples: 0.013 | 0.073\n",
      "2023-12-30 18:27:16 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1114176 examples: 0.022 | 0.072\n",
      "2023-12-30 18:27:17 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1116032 examples: 0.017 | 0.073\n",
      "2023-12-30 18:27:17 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1117888 examples: 0.021 | 0.072\n",
      "2023-12-30 18:27:17 - INFO     | Epoch: 19 | Learning Rate: 0.001: Training/Validation Loss after 1119744 examples: 0.013 | 0.071\n",
      "2023-12-30 18:27:18 - INFO     | Early stopping: no decrease (0.066 vs 0.071); counter: 3 out of 3\n",
      "2023-12-30 18:27:18 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:27:18 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      "100%|██████████| 20/20 [03:20<00:00, 10.02s/it]\n",
      "2023-12-30 18:27:18 - INFO     | Best validation loss: 0.066\n",
      "2023-12-30 18:27:18 - INFO     | Best early stopping index/epoch: 10\n",
      "2023-12-30 18:27:18 - INFO     | Average Loss on test set: 0.066\n",
      "2023-12-30 18:27:20 - INFO     | Weighted Precision: 0.983, Recall: 0.983, F1: 0.983\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████████▄▄▄▄▂▂▂▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_validation_loss</td><td>0.06637</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00125</td></tr><tr><td>step_learning_rate</td><td>0.00125</td></tr><tr><td>step_training_loss</td><td>0.0133</td></tr><tr><td>step_validation_loss</td><td>0.07059</td></tr><tr><td>test_loss</td><td>0.0656</td></tr><tr><td>weighted_f1</td><td>0.98342</td></tr><tr><td>weighted_precision</td><td>0.98346</td></tr><tr><td>weighted_recall</td><td>0.98343</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-20</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/dkgi6uwl' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/dkgi6uwl</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_182357-dkgi6uwl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2vkn4mfv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_182731-2vkn4mfv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2vkn4mfv' target=\"_blank\">mild-sweep-21</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2vkn4mfv' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2vkn4mfv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 64], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:27:31 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 18:27:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 15.004 | 121.319\n",
      "2023-12-30 18:27:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 16.808 | 1.696\n",
      "2023-12-30 18:27:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 1.283 | 0.916\n",
      "2023-12-30 18:27:32 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 0.700 | 0.569\n",
      "2023-12-30 18:27:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 0.465 | 0.366\n",
      "2023-12-30 18:27:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 0.397 | 0.328\n",
      "2023-12-30 18:27:33 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 0.331 | 0.292\n",
      "2023-12-30 18:27:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 0.315 | 0.240\n",
      "2023-12-30 18:27:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 0.232 | 0.245\n",
      "2023-12-30 18:27:34 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.213 | 0.218\n",
      "2023-12-30 18:27:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.207 | 0.197\n",
      "2023-12-30 18:27:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.230 | 0.179\n",
      "2023-12-30 18:27:35 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.211 | 0.210\n",
      "2023-12-30 18:27:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.217 | 0.181\n",
      "2023-12-30 18:27:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.211 | 0.208\n",
      "2023-12-30 18:27:36 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.217 | 0.180\n",
      "2023-12-30 18:27:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.164 | 0.167\n",
      "2023-12-30 18:27:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.211 | 0.201\n",
      "2023-12-30 18:27:37 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.166 | 0.167\n",
      "2023-12-30 18:27:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.176 | 0.141\n",
      "2023-12-30 18:27:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.197 | 0.171\n",
      "2023-12-30 18:27:38 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.165 | 0.148\n",
      "2023-12-30 18:27:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.167 | 0.149\n",
      "2023-12-30 18:27:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.151 | 0.154\n",
      "2023-12-30 18:27:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.141 | 0.163\n",
      "2023-12-30 18:27:39 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.164 | 0.140\n",
      "2023-12-30 18:27:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.144 | 0.144\n",
      "2023-12-30 18:27:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.188 | 0.139\n",
      "2023-12-30 18:27:40 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.150 | 0.129\n",
      "2023-12-30 18:27:41 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.171 | 0.153\n",
      "2023-12-30 18:27:41 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.146 | 0.133\n",
      "2023-12-30 18:27:41 - INFO     | Early stopping: loss decreased (inf -> 0.142; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:11, 10.09s/it]2023-12-30 18:27:41 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 18:27:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.069 | 0.148\n",
      "2023-12-30 18:27:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.142 | 0.149\n",
      "2023-12-30 18:27:42 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.159 | 0.149\n",
      "2023-12-30 18:27:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.162 | 0.158\n",
      "2023-12-30 18:27:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.135 | 0.150\n",
      "2023-12-30 18:27:43 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.129 | 0.144\n",
      "2023-12-30 18:27:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.175 | 0.132\n",
      "2023-12-30 18:27:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.114 | 0.167\n",
      "2023-12-30 18:27:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.159 | 0.149\n",
      "2023-12-30 18:27:44 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.131 | 0.134\n",
      "2023-12-30 18:27:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.147 | 0.131\n",
      "2023-12-30 18:27:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.126 | 0.148\n",
      "2023-12-30 18:27:45 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.127 | 0.142\n",
      "2023-12-30 18:27:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.134 | 0.151\n",
      "2023-12-30 18:27:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.134 | 0.147\n",
      "2023-12-30 18:27:46 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.152 | 0.169\n",
      "2023-12-30 18:27:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.144 | 0.130\n",
      "2023-12-30 18:27:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.160 | 0.123\n",
      "2023-12-30 18:27:47 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.119 | 0.183\n",
      "2023-12-30 18:27:48 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.138 | 0.150\n",
      "2023-12-30 18:27:48 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.179 | 0.130\n",
      "2023-12-30 18:27:48 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.130 | 0.116\n",
      "2023-12-30 18:27:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.090 | 0.156\n",
      "2023-12-30 18:27:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.146 | 0.146\n",
      "2023-12-30 18:27:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.123 | 0.127\n",
      "2023-12-30 18:27:49 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.128 | 0.139\n",
      "2023-12-30 18:27:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.122 | 0.138\n",
      "2023-12-30 18:27:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.117 | 0.141\n",
      "2023-12-30 18:27:50 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.168 | 0.155\n",
      "2023-12-30 18:27:51 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.152 | 0.130\n",
      "2023-12-30 18:27:51 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.156 | 0.128\n",
      "2023-12-30 18:27:51 - INFO     | Early stopping: loss decreased (0.142 -> 0.122; -13.9%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:02, 10.15s/it]2023-12-30 18:27:51 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 18:27:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.435 | 0.124\n",
      "2023-12-30 18:27:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.097 | 0.138\n",
      "2023-12-30 18:27:52 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.112 | 0.149\n",
      "2023-12-30 18:27:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.119 | 0.156\n",
      "2023-12-30 18:27:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.149 | 0.129\n",
      "2023-12-30 18:27:53 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.119 | 0.173\n",
      "2023-12-30 18:27:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.138 | 0.163\n",
      "2023-12-30 18:27:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.105 | 0.124\n",
      "2023-12-30 18:27:54 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.121 | 0.126\n",
      "2023-12-30 18:27:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.141 | 0.161\n",
      "2023-12-30 18:27:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.117 | 0.141\n",
      "2023-12-30 18:27:55 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.118 | 0.150\n",
      "2023-12-30 18:27:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.147 | 0.135\n",
      "2023-12-30 18:27:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.100 | 0.135\n",
      "2023-12-30 18:27:56 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.121 | 0.130\n",
      "2023-12-30 18:27:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.137 | 0.127\n",
      "2023-12-30 18:27:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.167 | 0.153\n",
      "2023-12-30 18:27:57 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.135 | 0.135\n",
      "2023-12-30 18:27:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.124 | 0.119\n",
      "2023-12-30 18:27:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.131 | 0.155\n",
      "2023-12-30 18:27:58 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.137 | 0.124\n",
      "2023-12-30 18:27:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.137 | 0.113\n",
      "2023-12-30 18:27:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.094 | 0.122\n",
      "2023-12-30 18:27:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.115 | 0.164\n",
      "2023-12-30 18:27:59 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.125 | 0.170\n",
      "2023-12-30 18:28:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.108 | 0.131\n",
      "2023-12-30 18:28:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.113 | 0.141\n",
      "2023-12-30 18:28:00 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.149 | 0.155\n",
      "2023-12-30 18:28:01 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.158 | 0.173\n",
      "2023-12-30 18:28:01 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.177 | 0.128\n",
      "2023-12-30 18:28:01 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.128 | 0.146\n",
      "2023-12-30 18:28:02 - INFO     | Early stopping: no decrease (0.122 vs 0.144); counter: 1 out of 3\n",
      " 15%|█▌        | 3/20 [00:30<02:53, 10.18s/it]2023-12-30 18:28:02 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 18:28:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.009 | 0.144\n",
      "2023-12-30 18:28:02 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.102 | 0.140\n",
      "2023-12-30 18:28:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.092 | 0.152\n",
      "2023-12-30 18:28:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.125 | 0.142\n",
      "2023-12-30 18:28:03 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.100 | 0.172\n",
      "2023-12-30 18:28:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.115 | 0.173\n",
      "2023-12-30 18:28:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.133 | 0.137\n",
      "2023-12-30 18:28:04 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.086 | 0.131\n",
      "2023-12-30 18:28:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.123 | 0.153\n",
      "2023-12-30 18:28:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.157 | 0.135\n",
      "2023-12-30 18:28:05 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.118 | 0.126\n",
      "2023-12-30 18:28:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.118 | 0.137\n",
      "2023-12-30 18:28:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.175 | 0.137\n",
      "2023-12-30 18:28:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.112 | 0.144\n",
      "2023-12-30 18:28:06 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.145 | 0.126\n",
      "2023-12-30 18:28:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.085 | 0.141\n",
      "2023-12-30 18:28:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.139 | 0.216\n",
      "2023-12-30 18:28:07 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.101 | 0.151\n",
      "2023-12-30 18:28:08 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.133 | 0.159\n",
      "2023-12-30 18:28:08 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.145 | 0.124\n",
      "2023-12-30 18:28:08 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.112 | 0.148\n",
      "2023-12-30 18:28:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.131 | 0.141\n",
      "2023-12-30 18:28:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.144 | 0.122\n",
      "2023-12-30 18:28:09 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.122 | 0.143\n",
      "2023-12-30 18:28:10 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.131 | 0.121\n",
      "2023-12-30 18:28:10 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.120 | 0.142\n",
      "2023-12-30 18:28:10 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.129 | 0.245\n",
      "2023-12-30 18:28:11 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.161 | 0.131\n",
      "2023-12-30 18:28:11 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.147 | 0.128\n",
      "2023-12-30 18:28:11 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.136 | 0.116\n",
      "2023-12-30 18:28:12 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.134 | 0.134\n",
      "2023-12-30 18:28:12 - INFO     | Early stopping: no decrease (0.122 vs 0.149); counter: 2 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:43, 10.19s/it]2023-12-30 18:28:12 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 18:28:12 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.129 | 0.153\n",
      "2023-12-30 18:28:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.117 | 0.156\n",
      "2023-12-30 18:28:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.100 | 0.140\n",
      "2023-12-30 18:28:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.133 | 0.165\n",
      "2023-12-30 18:28:13 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.114 | 0.148\n",
      "2023-12-30 18:28:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.126 | 0.148\n",
      "2023-12-30 18:28:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.119 | 0.141\n",
      "2023-12-30 18:28:14 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.115 | 0.152\n",
      "2023-12-30 18:28:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.147 | 0.155\n",
      "2023-12-30 18:28:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.134 | 0.170\n",
      "2023-12-30 18:28:15 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.141 | 0.132\n",
      "2023-12-30 18:28:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.101 | 0.166\n",
      "2023-12-30 18:28:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.138 | 0.151\n",
      "2023-12-30 18:28:16 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.124 | 0.135\n",
      "2023-12-30 18:28:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.112 | 0.129\n",
      "2023-12-30 18:28:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.118 | 0.145\n",
      "2023-12-30 18:28:17 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.139 | 0.149\n",
      "2023-12-30 18:28:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.094 | 0.132\n",
      "2023-12-30 18:28:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.111 | 0.143\n",
      "2023-12-30 18:28:18 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.147 | 0.133\n",
      "2023-12-30 18:28:19 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.154 | 0.146\n",
      "2023-12-30 18:28:19 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.132 | 0.144\n",
      "2023-12-30 18:28:19 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.151 | 0.157\n",
      "2023-12-30 18:28:19 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.116 | 0.166\n",
      "2023-12-30 18:28:20 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.156 | 0.207\n",
      "2023-12-30 18:28:20 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.172 | 0.168\n",
      "2023-12-30 18:28:20 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.128 | 0.148\n",
      "2023-12-30 18:28:21 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.153 | 0.144\n",
      "2023-12-30 18:28:21 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.128 | 0.145\n",
      "2023-12-30 18:28:21 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.204 | 0.130\n",
      "2023-12-30 18:28:22 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.111 | 0.148\n",
      "2023-12-30 18:28:22 - INFO     | Early stopping: no decrease (0.122 vs 0.130); counter: 3 out of 3\n",
      "2023-12-30 18:28:22 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:28:22 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 25%|██▌       | 5/20 [00:50<02:32, 10.17s/it]2023-12-30 18:28:22 - INFO     | Epoch: 5 | Learning Rate: 0.003\n",
      "2023-12-30 18:28:22 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 280064 examples: 0.125 | 0.133\n",
      "2023-12-30 18:28:23 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 281920 examples: 0.105 | 0.113\n",
      "2023-12-30 18:28:23 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 283776 examples: 0.108 | 0.108\n",
      "2023-12-30 18:28:23 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 285632 examples: 0.064 | 0.102\n",
      "2023-12-30 18:28:24 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 287488 examples: 0.090 | 0.111\n",
      "2023-12-30 18:28:24 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 289344 examples: 0.113 | 0.104\n",
      "2023-12-30 18:28:24 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 291200 examples: 0.081 | 0.123\n",
      "2023-12-30 18:28:25 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 293056 examples: 0.106 | 0.101\n",
      "2023-12-30 18:28:25 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 294912 examples: 0.103 | 0.089\n",
      "2023-12-30 18:28:25 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 296768 examples: 0.062 | 0.112\n",
      "2023-12-30 18:28:26 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 298624 examples: 0.088 | 0.106\n",
      "2023-12-30 18:28:26 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 300480 examples: 0.091 | 0.097\n",
      "2023-12-30 18:28:26 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 302336 examples: 0.078 | 0.112\n",
      "2023-12-30 18:28:27 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 304192 examples: 0.091 | 0.124\n",
      "2023-12-30 18:28:27 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 306048 examples: 0.108 | 0.119\n",
      "2023-12-30 18:28:27 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 307904 examples: 0.082 | 0.137\n",
      "2023-12-30 18:28:28 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 309760 examples: 0.106 | 0.112\n",
      "2023-12-30 18:28:28 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 311616 examples: 0.094 | 0.112\n",
      "2023-12-30 18:28:28 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 313472 examples: 0.093 | 0.100\n",
      "2023-12-30 18:28:29 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 315328 examples: 0.090 | 0.114\n",
      "2023-12-30 18:28:29 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 317184 examples: 0.099 | 0.104\n",
      "2023-12-30 18:28:29 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 319040 examples: 0.087 | 0.114\n",
      "2023-12-30 18:28:29 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 320896 examples: 0.108 | 0.121\n",
      "2023-12-30 18:28:30 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 322752 examples: 0.110 | 0.096\n",
      "2023-12-30 18:28:30 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 324608 examples: 0.077 | 0.105\n",
      "2023-12-30 18:28:30 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 326464 examples: 0.080 | 0.101\n",
      "2023-12-30 18:28:31 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 328320 examples: 0.108 | 0.131\n",
      "2023-12-30 18:28:31 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 330176 examples: 0.107 | 0.093\n",
      "2023-12-30 18:28:31 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 332032 examples: 0.090 | 0.105\n",
      "2023-12-30 18:28:32 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 333888 examples: 0.088 | 0.094\n",
      "2023-12-30 18:28:32 - INFO     | Epoch: 5 | Learning Rate: 0.003: Training/Validation Loss after 335744 examples: 0.087 | 0.094\n",
      "2023-12-30 18:28:32 - INFO     | Early stopping: loss decreased (0.122 -> 0.090; -26.6%). Caching model state.\n",
      " 30%|███       | 6/20 [01:01<02:22, 10.21s/it]2023-12-30 18:28:32 - INFO     | Epoch: 6 | Learning Rate: 0.003\n",
      "2023-12-30 18:28:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 336064 examples: 0.025 | 0.090\n",
      "2023-12-30 18:28:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 337920 examples: 0.059 | 0.083\n",
      "2023-12-30 18:28:33 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 339776 examples: 0.039 | 0.083\n",
      "2023-12-30 18:28:34 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 341632 examples: 0.046 | 0.085\n",
      "2023-12-30 18:28:34 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 343488 examples: 0.063 | 0.111\n",
      "2023-12-30 18:28:34 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 345344 examples: 0.065 | 0.116\n",
      "2023-12-30 18:28:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 347200 examples: 0.067 | 0.098\n",
      "2023-12-30 18:28:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 349056 examples: 0.043 | 0.113\n",
      "2023-12-30 18:28:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 350912 examples: 0.099 | 0.109\n",
      "2023-12-30 18:28:35 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 352768 examples: 0.086 | 0.089\n",
      "2023-12-30 18:28:36 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 354624 examples: 0.089 | 0.126\n",
      "2023-12-30 18:28:36 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 356480 examples: 0.084 | 0.093\n",
      "2023-12-30 18:28:36 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 358336 examples: 0.078 | 0.093\n",
      "2023-12-30 18:28:37 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 360192 examples: 0.083 | 0.117\n",
      "2023-12-30 18:28:37 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 362048 examples: 0.080 | 0.087\n",
      "2023-12-30 18:28:37 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 363904 examples: 0.083 | 0.097\n",
      "2023-12-30 18:28:38 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 365760 examples: 0.079 | 0.086\n",
      "2023-12-30 18:28:38 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 367616 examples: 0.067 | 0.112\n",
      "2023-12-30 18:28:38 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 369472 examples: 0.084 | 0.110\n",
      "2023-12-30 18:28:39 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 371328 examples: 0.084 | 0.111\n",
      "2023-12-30 18:28:39 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 373184 examples: 0.096 | 0.111\n",
      "2023-12-30 18:28:39 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 375040 examples: 0.080 | 0.097\n",
      "2023-12-30 18:28:40 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 376896 examples: 0.088 | 0.109\n",
      "2023-12-30 18:28:40 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 378752 examples: 0.085 | 0.102\n",
      "2023-12-30 18:28:40 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 380608 examples: 0.111 | 0.096\n",
      "2023-12-30 18:28:40 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 382464 examples: 0.080 | 0.087\n",
      "2023-12-30 18:28:41 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 384320 examples: 0.081 | 0.086\n",
      "2023-12-30 18:28:41 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 386176 examples: 0.072 | 0.083\n",
      "2023-12-30 18:28:41 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 388032 examples: 0.069 | 0.085\n",
      "2023-12-30 18:28:42 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 389888 examples: 0.063 | 0.088\n",
      "2023-12-30 18:28:42 - INFO     | Epoch: 6 | Learning Rate: 0.003: Training/Validation Loss after 391744 examples: 0.074 | 0.093\n",
      "2023-12-30 18:28:42 - INFO     | Early stopping: no decrease (0.090 vs 0.094); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:11<02:11, 10.14s/it]2023-12-30 18:28:42 - INFO     | Epoch: 7 | Learning Rate: 0.003\n",
      "2023-12-30 18:28:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 392064 examples: 0.053 | 0.092\n",
      "2023-12-30 18:28:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 393920 examples: 0.040 | 0.113\n",
      "2023-12-30 18:28:43 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 395776 examples: 0.046 | 0.111\n",
      "2023-12-30 18:28:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 397632 examples: 0.044 | 0.111\n",
      "2023-12-30 18:28:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 399488 examples: 0.059 | 0.088\n",
      "2023-12-30 18:28:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 401344 examples: 0.050 | 0.098\n",
      "2023-12-30 18:28:44 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 403200 examples: 0.046 | 0.097\n",
      "2023-12-30 18:28:45 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 405056 examples: 0.051 | 0.084\n",
      "2023-12-30 18:28:45 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 406912 examples: 0.049 | 0.091\n",
      "2023-12-30 18:28:45 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 408768 examples: 0.068 | 0.096\n",
      "2023-12-30 18:28:46 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 410624 examples: 0.045 | 0.104\n",
      "2023-12-30 18:28:46 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 412480 examples: 0.070 | 0.104\n",
      "2023-12-30 18:28:46 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 414336 examples: 0.055 | 0.089\n",
      "2023-12-30 18:28:47 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 416192 examples: 0.072 | 0.134\n",
      "2023-12-30 18:28:47 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 418048 examples: 0.102 | 0.141\n",
      "2023-12-30 18:28:47 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 419904 examples: 0.056 | 0.138\n",
      "2023-12-30 18:28:48 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 421760 examples: 0.059 | 0.122\n",
      "2023-12-30 18:28:48 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 423616 examples: 0.069 | 0.128\n",
      "2023-12-30 18:28:48 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 425472 examples: 0.096 | 0.097\n",
      "2023-12-30 18:28:49 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 427328 examples: 0.077 | 0.099\n",
      "2023-12-30 18:28:49 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 429184 examples: 0.052 | 0.107\n",
      "2023-12-30 18:28:49 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 431040 examples: 0.097 | 0.098\n",
      "2023-12-30 18:28:49 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 432896 examples: 0.068 | 0.085\n",
      "2023-12-30 18:28:50 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 434752 examples: 0.050 | 0.084\n",
      "2023-12-30 18:28:50 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 436608 examples: 0.082 | 0.113\n",
      "2023-12-30 18:28:51 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 438464 examples: 0.096 | 0.123\n",
      "2023-12-30 18:28:51 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 440320 examples: 0.101 | 0.116\n",
      "2023-12-30 18:28:51 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 442176 examples: 0.083 | 0.103\n",
      "2023-12-30 18:28:51 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 444032 examples: 0.082 | 0.106\n",
      "2023-12-30 18:28:52 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 445888 examples: 0.099 | 0.117\n",
      "2023-12-30 18:28:52 - INFO     | Epoch: 7 | Learning Rate: 0.003: Training/Validation Loss after 447744 examples: 0.091 | 0.132\n",
      "2023-12-30 18:28:52 - INFO     | Early stopping: no decrease (0.090 vs 0.145); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:21<02:01, 10.12s/it]2023-12-30 18:28:52 - INFO     | Epoch: 8 | Learning Rate: 0.003\n",
      "2023-12-30 18:28:53 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 448064 examples: 0.020 | 0.146\n",
      "2023-12-30 18:28:53 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 449920 examples: 0.077 | 0.089\n",
      "2023-12-30 18:28:53 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 451776 examples: 0.044 | 0.091\n",
      "2023-12-30 18:28:54 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 453632 examples: 0.050 | 0.089\n",
      "2023-12-30 18:28:54 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 455488 examples: 0.056 | 0.119\n",
      "2023-12-30 18:28:54 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 457344 examples: 0.060 | 0.101\n",
      "2023-12-30 18:28:55 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 459200 examples: 0.048 | 0.089\n",
      "2023-12-30 18:28:55 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 461056 examples: 0.045 | 0.101\n",
      "2023-12-30 18:28:55 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 462912 examples: 0.051 | 0.115\n",
      "2023-12-30 18:28:56 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 464768 examples: 0.053 | 0.084\n",
      "2023-12-30 18:28:56 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 466624 examples: 0.072 | 0.092\n",
      "2023-12-30 18:28:56 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 468480 examples: 0.068 | 0.103\n",
      "2023-12-30 18:28:57 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 470336 examples: 0.064 | 0.108\n",
      "2023-12-30 18:28:57 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 472192 examples: 0.076 | 0.094\n",
      "2023-12-30 18:28:57 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 474048 examples: 0.051 | 0.113\n",
      "2023-12-30 18:28:57 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 475904 examples: 0.050 | 0.117\n",
      "2023-12-30 18:28:58 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 477760 examples: 0.032 | 0.118\n",
      "2023-12-30 18:28:58 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 479616 examples: 0.072 | 0.087\n",
      "2023-12-30 18:28:58 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 481472 examples: 0.043 | 0.095\n",
      "2023-12-30 18:28:59 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 483328 examples: 0.059 | 0.111\n",
      "2023-12-30 18:28:59 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 485184 examples: 0.067 | 0.104\n",
      "2023-12-30 18:28:59 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 487040 examples: 0.085 | 0.113\n",
      "2023-12-30 18:29:00 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 488896 examples: 0.100 | 0.126\n",
      "2023-12-30 18:29:00 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 490752 examples: 0.076 | 0.102\n",
      "2023-12-30 18:29:00 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 492608 examples: 0.043 | 0.123\n",
      "2023-12-30 18:29:01 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 494464 examples: 0.086 | 0.116\n",
      "2023-12-30 18:29:01 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 496320 examples: 0.082 | 0.112\n",
      "2023-12-30 18:29:01 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 498176 examples: 0.051 | 0.102\n",
      "2023-12-30 18:29:02 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 500032 examples: 0.091 | 0.107\n",
      "2023-12-30 18:29:02 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 501888 examples: 0.094 | 0.106\n",
      "2023-12-30 18:29:02 - INFO     | Epoch: 8 | Learning Rate: 0.003: Training/Validation Loss after 503744 examples: 0.059 | 0.092\n",
      "2023-12-30 18:29:02 - INFO     | Early stopping: no decrease (0.090 vs 0.091); counter: 3 out of 3\n",
      "2023-12-30 18:29:02 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:29:02 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 45%|████▌     | 9/20 [01:31<01:51, 10.11s/it]2023-12-30 18:29:02 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 18:29:03 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.026 | 0.088\n",
      "2023-12-30 18:29:03 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.044 | 0.080\n",
      "2023-12-30 18:29:03 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.033 | 0.085\n",
      "2023-12-30 18:29:04 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.028 | 0.080\n",
      "2023-12-30 18:29:04 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.029 | 0.082\n",
      "2023-12-30 18:29:04 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.061 | 0.084\n",
      "2023-12-30 18:29:05 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.046 | 0.076\n",
      "2023-12-30 18:29:05 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.025 | 0.078\n",
      "2023-12-30 18:29:05 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.026 | 0.070\n",
      "2023-12-30 18:29:06 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.017 | 0.074\n",
      "2023-12-30 18:29:06 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.033 | 0.080\n",
      "2023-12-30 18:29:06 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.028 | 0.084\n",
      "2023-12-30 18:29:07 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.029 | 0.088\n",
      "2023-12-30 18:29:07 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.018 | 0.095\n",
      "2023-12-30 18:29:07 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.033 | 0.085\n",
      "2023-12-30 18:29:08 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.041 | 0.080\n",
      "2023-12-30 18:29:08 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.036 | 0.086\n",
      "2023-12-30 18:29:08 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.028 | 0.089\n",
      "2023-12-30 18:29:09 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.045 | 0.088\n",
      "2023-12-30 18:29:09 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.040 | 0.082\n",
      "2023-12-30 18:29:09 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.044 | 0.088\n",
      "2023-12-30 18:29:09 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.048 | 0.081\n",
      "2023-12-30 18:29:10 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.045 | 0.079\n",
      "2023-12-30 18:29:10 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.041 | 0.077\n",
      "2023-12-30 18:29:10 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.023 | 0.085\n",
      "2023-12-30 18:29:11 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.044 | 0.095\n",
      "2023-12-30 18:29:11 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.030 | 0.082\n",
      "2023-12-30 18:29:11 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.034 | 0.079\n",
      "2023-12-30 18:29:12 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.030 | 0.080\n",
      "2023-12-30 18:29:12 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.023 | 0.079\n",
      "2023-12-30 18:29:12 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.038 | 0.108\n",
      "2023-12-30 18:29:13 - INFO     | Early stopping: no decrease (0.090 vs 0.117); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:41<01:41, 10.13s/it]2023-12-30 18:29:13 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 18:29:13 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.001 | 0.119\n",
      "2023-12-30 18:29:13 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.021 | 0.086\n",
      "2023-12-30 18:29:14 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.039 | 0.086\n",
      "2023-12-30 18:29:14 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.011 | 0.091\n",
      "2023-12-30 18:29:14 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.028 | 0.090\n",
      "2023-12-30 18:29:14 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.026 | 0.088\n",
      "2023-12-30 18:29:15 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.043 | 0.086\n",
      "2023-12-30 18:29:15 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.025 | 0.083\n",
      "2023-12-30 18:29:15 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.010 | 0.080\n",
      "2023-12-30 18:29:16 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.010 | 0.085\n",
      "2023-12-30 18:29:16 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.015 | 0.086\n",
      "2023-12-30 18:29:16 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.013 | 0.081\n",
      "2023-12-30 18:29:17 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.016 | 0.091\n",
      "2023-12-30 18:29:17 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.027 | 0.096\n",
      "2023-12-30 18:29:17 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.020 | 0.110\n",
      "2023-12-30 18:29:18 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.026 | 0.088\n",
      "2023-12-30 18:29:18 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.026 | 0.081\n",
      "2023-12-30 18:29:18 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.023 | 0.088\n",
      "2023-12-30 18:29:19 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.027 | 0.082\n",
      "2023-12-30 18:29:19 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.016 | 0.091\n",
      "2023-12-30 18:29:19 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.036 | 0.084\n",
      "2023-12-30 18:29:20 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.022 | 0.094\n",
      "2023-12-30 18:29:20 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.041 | 0.086\n",
      "2023-12-30 18:29:20 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.015 | 0.089\n",
      "2023-12-30 18:29:20 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.022 | 0.085\n",
      "2023-12-30 18:29:21 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.012 | 0.086\n",
      "2023-12-30 18:29:21 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.024 | 0.088\n",
      "2023-12-30 18:29:21 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.023 | 0.087\n",
      "2023-12-30 18:29:22 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.025 | 0.080\n",
      "2023-12-30 18:29:22 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.024 | 0.082\n",
      "2023-12-30 18:29:22 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.032 | 0.099\n",
      "2023-12-30 18:29:23 - INFO     | Early stopping: no decrease (0.090 vs 0.094); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:51<01:30, 10.09s/it]2023-12-30 18:29:23 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 18:29:23 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.000 | 0.093\n",
      "2023-12-30 18:29:23 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.018 | 0.090\n",
      "2023-12-30 18:29:24 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.024 | 0.080\n",
      "2023-12-30 18:29:24 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.009 | 0.093\n",
      "2023-12-30 18:29:24 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.019 | 0.081\n",
      "2023-12-30 18:29:25 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.011 | 0.086\n",
      "2023-12-30 18:29:25 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.016 | 0.093\n",
      "2023-12-30 18:29:25 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.014 | 0.084\n",
      "2023-12-30 18:29:25 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.010 | 0.085\n",
      "2023-12-30 18:29:26 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.018 | 0.086\n",
      "2023-12-30 18:29:26 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.011 | 0.096\n",
      "2023-12-30 18:29:26 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.012 | 0.097\n",
      "2023-12-30 18:29:27 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.015 | 0.091\n",
      "2023-12-30 18:29:27 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.007 | 0.114\n",
      "2023-12-30 18:29:27 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.023 | 0.098\n",
      "2023-12-30 18:29:28 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.025 | 0.099\n",
      "2023-12-30 18:29:28 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.008 | 0.091\n",
      "2023-12-30 18:29:28 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.031 | 0.091\n",
      "2023-12-30 18:29:29 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.019 | 0.099\n",
      "2023-12-30 18:29:29 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.024 | 0.095\n",
      "2023-12-30 18:29:29 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.012 | 0.100\n",
      "2023-12-30 18:29:29 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.021 | 0.092\n",
      "2023-12-30 18:29:30 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.022 | 0.092\n",
      "2023-12-30 18:29:30 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.044 | 0.093\n",
      "2023-12-30 18:29:30 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.034 | 0.099\n",
      "2023-12-30 18:29:31 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.025 | 0.088\n",
      "2023-12-30 18:29:31 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.017 | 0.085\n",
      "2023-12-30 18:29:31 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.019 | 0.100\n",
      "2023-12-30 18:29:32 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.010 | 0.101\n",
      "2023-12-30 18:29:32 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.015 | 0.096\n",
      "2023-12-30 18:29:32 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.019 | 0.108\n",
      "2023-12-30 18:29:33 - INFO     | Early stopping: no decrease (0.090 vs 0.112); counter: 3 out of 3\n",
      "2023-12-30 18:29:33 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:29:33 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 60%|██████    | 12/20 [02:01<01:20, 10.07s/it]2023-12-30 18:29:33 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 18:29:33 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.001 | 0.113\n",
      "2023-12-30 18:29:33 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.020 | 0.108\n",
      "2023-12-30 18:29:34 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.012 | 0.101\n",
      "2023-12-30 18:29:34 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.011 | 0.103\n",
      "2023-12-30 18:29:34 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.006 | 0.098\n",
      "2023-12-30 18:29:35 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.009 | 0.096\n",
      "2023-12-30 18:29:35 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.006 | 0.095\n",
      "2023-12-30 18:29:35 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.006 | 0.097\n",
      "2023-12-30 18:29:36 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.003 | 0.092\n",
      "2023-12-30 18:29:36 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.010 | 0.090\n",
      "2023-12-30 18:29:36 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.007 | 0.091\n",
      "2023-12-30 18:29:36 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.003 | 0.092\n",
      "2023-12-30 18:29:37 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.015 | 0.095\n",
      "2023-12-30 18:29:37 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.013 | 0.097\n",
      "2023-12-30 18:29:37 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.010 | 0.094\n",
      "2023-12-30 18:29:38 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.005 | 0.098\n",
      "2023-12-30 18:29:38 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.006 | 0.104\n",
      "2023-12-30 18:29:38 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.014 | 0.103\n",
      "2023-12-30 18:29:39 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.004 | 0.102\n",
      "2023-12-30 18:29:39 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.005 | 0.101\n",
      "2023-12-30 18:29:39 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.005 | 0.099\n",
      "2023-12-30 18:29:40 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.008 | 0.100\n",
      "2023-12-30 18:29:40 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.008 | 0.101\n",
      "2023-12-30 18:29:40 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.003 | 0.100\n",
      "2023-12-30 18:29:41 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.010 | 0.112\n",
      "2023-12-30 18:29:41 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.013 | 0.100\n",
      "2023-12-30 18:29:41 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.022 | 0.098\n",
      "2023-12-30 18:29:42 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.005 | 0.095\n",
      "2023-12-30 18:29:42 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.002 | 0.096\n",
      "2023-12-30 18:29:42 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.008 | 0.098\n",
      "2023-12-30 18:29:43 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.015 | 0.091\n",
      "2023-12-30 18:29:43 - INFO     | Early stopping: no decrease (0.090 vs 0.092); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:11<01:10, 10.12s/it]2023-12-30 18:29:43 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 18:29:43 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.000 | 0.093\n",
      "2023-12-30 18:29:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.001 | 0.093\n",
      "2023-12-30 18:29:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.001 | 0.092\n",
      "2023-12-30 18:29:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.003 | 0.097\n",
      "2023-12-30 18:29:44 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.007 | 0.099\n",
      "2023-12-30 18:29:45 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.003 | 0.099\n",
      "2023-12-30 18:29:45 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.003 | 0.095\n",
      "2023-12-30 18:29:45 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.004 | 0.095\n",
      "2023-12-30 18:29:46 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.001 | 0.099\n",
      "2023-12-30 18:29:46 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.004 | 0.097\n",
      "2023-12-30 18:29:46 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.003 | 0.100\n",
      "2023-12-30 18:29:47 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.014 | 0.100\n",
      "2023-12-30 18:29:47 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.003 | 0.100\n",
      "2023-12-30 18:29:47 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.003 | 0.097\n",
      "2023-12-30 18:29:48 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.004 | 0.095\n",
      "2023-12-30 18:29:48 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.002 | 0.099\n",
      "2023-12-30 18:29:48 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.001 | 0.101\n",
      "2023-12-30 18:29:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.002 | 0.099\n",
      "2023-12-30 18:29:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.003 | 0.093\n",
      "2023-12-30 18:29:49 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.003 | 0.094\n",
      "2023-12-30 18:29:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.002 | 0.100\n",
      "2023-12-30 18:29:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.007 | 0.101\n",
      "2023-12-30 18:29:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.005 | 0.104\n",
      "2023-12-30 18:29:50 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.004 | 0.107\n",
      "2023-12-30 18:29:51 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.005 | 0.100\n",
      "2023-12-30 18:29:51 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.003 | 0.104\n",
      "2023-12-30 18:29:51 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.002 | 0.105\n",
      "2023-12-30 18:29:52 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.003 | 0.105\n",
      "2023-12-30 18:29:52 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.003 | 0.100\n",
      "2023-12-30 18:29:52 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.005 | 0.102\n",
      "2023-12-30 18:29:53 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.002 | 0.102\n",
      "2023-12-30 18:29:53 - INFO     | Early stopping: no decrease (0.090 vs 0.102); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:21<01:00, 10.11s/it]2023-12-30 18:29:53 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:29:53 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.000 | 0.102\n",
      "2023-12-30 18:29:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.001 | 0.100\n",
      "2023-12-30 18:29:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.001 | 0.100\n",
      "2023-12-30 18:29:54 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.001 | 0.103\n",
      "2023-12-30 18:29:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.000 | 0.105\n",
      "2023-12-30 18:29:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.000 | 0.105\n",
      "2023-12-30 18:29:55 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.001 | 0.105\n",
      "2023-12-30 18:29:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.001 | 0.105\n",
      "2023-12-30 18:29:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.017 | 0.103\n",
      "2023-12-30 18:29:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.003 | 0.110\n",
      "2023-12-30 18:29:56 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.006 | 0.111\n",
      "2023-12-30 18:29:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.002 | 0.105\n",
      "2023-12-30 18:29:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.001 | 0.106\n",
      "2023-12-30 18:29:57 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.001 | 0.111\n",
      "2023-12-30 18:29:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.004 | 0.111\n",
      "2023-12-30 18:29:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.004 | 0.107\n",
      "2023-12-30 18:29:58 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.002 | 0.106\n",
      "2023-12-30 18:29:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.000 | 0.104\n",
      "2023-12-30 18:29:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.001 | 0.103\n",
      "2023-12-30 18:29:59 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.001 | 0.101\n",
      "2023-12-30 18:30:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.001 | 0.102\n",
      "2023-12-30 18:30:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.001 | 0.101\n",
      "2023-12-30 18:30:00 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.002 | 0.102\n",
      "2023-12-30 18:30:01 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.001 | 0.105\n",
      "2023-12-30 18:30:01 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.004 | 0.110\n",
      "2023-12-30 18:30:01 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.002 | 0.108\n",
      "2023-12-30 18:30:02 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.003 | 0.109\n",
      "2023-12-30 18:30:02 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.002 | 0.111\n",
      "2023-12-30 18:30:02 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.007 | 0.108\n",
      "2023-12-30 18:30:03 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.003 | 0.112\n",
      "2023-12-30 18:30:03 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.003 | 0.106\n",
      "2023-12-30 18:30:03 - INFO     | Early stopping: no decrease (0.090 vs 0.106); counter: 3 out of 3\n",
      "2023-12-30 18:30:03 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:30:03 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      " 75%|███████▌  | 15/20 [02:31<00:50, 10.13s/it]2023-12-30 18:30:03 - INFO     | Epoch: 15 | Learning Rate: 0.000\n",
      "2023-12-30 18:30:03 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 840064 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:04 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 841920 examples: 0.000 | 0.105\n",
      "2023-12-30 18:30:04 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 843776 examples: 0.001 | 0.105\n",
      "2023-12-30 18:30:04 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 845632 examples: 0.001 | 0.104\n",
      "2023-12-30 18:30:05 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 847488 examples: 0.001 | 0.105\n",
      "2023-12-30 18:30:05 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 849344 examples: 0.002 | 0.105\n",
      "2023-12-30 18:30:05 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 851200 examples: 0.003 | 0.110\n",
      "2023-12-30 18:30:06 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 853056 examples: 0.001 | 0.110\n",
      "2023-12-30 18:30:06 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 854912 examples: 0.001 | 0.110\n",
      "2023-12-30 18:30:06 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 856768 examples: 0.001 | 0.112\n",
      "2023-12-30 18:30:07 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 858624 examples: 0.001 | 0.113\n",
      "2023-12-30 18:30:07 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 860480 examples: 0.003 | 0.111\n",
      "2023-12-30 18:30:07 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 862336 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:08 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 864192 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:08 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 866048 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:08 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 867904 examples: 0.000 | 0.111\n",
      "2023-12-30 18:30:09 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 869760 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:09 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 871616 examples: 0.001 | 0.107\n",
      "2023-12-30 18:30:09 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 873472 examples: 0.001 | 0.107\n",
      "2023-12-30 18:30:10 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 875328 examples: 0.001 | 0.106\n",
      "2023-12-30 18:30:10 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 877184 examples: 0.001 | 0.109\n",
      "2023-12-30 18:30:10 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 879040 examples: 0.002 | 0.108\n",
      "2023-12-30 18:30:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 880896 examples: 0.004 | 0.104\n",
      "2023-12-30 18:30:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 882752 examples: 0.012 | 0.105\n",
      "2023-12-30 18:30:11 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 884608 examples: 0.004 | 0.106\n",
      "2023-12-30 18:30:12 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 886464 examples: 0.000 | 0.113\n",
      "2023-12-30 18:30:12 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 888320 examples: 0.000 | 0.112\n",
      "2023-12-30 18:30:12 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 890176 examples: 0.000 | 0.112\n",
      "2023-12-30 18:30:13 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 892032 examples: 0.002 | 0.112\n",
      "2023-12-30 18:30:13 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 893888 examples: 0.001 | 0.110\n",
      "2023-12-30 18:30:13 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 895744 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:14 - INFO     | Early stopping: no decrease (0.090 vs 0.107); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:42<00:40, 10.19s/it]2023-12-30 18:30:14 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 18:30:14 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.001 | 0.107\n",
      "2023-12-30 18:30:14 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:14 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:15 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.000 | 0.106\n",
      "2023-12-30 18:30:15 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:15 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:16 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:16 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:16 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:17 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.001 | 0.109\n",
      "2023-12-30 18:30:17 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:17 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:18 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:18 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:18 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:19 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:19 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.000 | 0.111\n",
      "2023-12-30 18:30:19 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.012 | 0.108\n",
      "2023-12-30 18:30:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.001 | 0.107\n",
      "2023-12-30 18:30:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.000 | 0.106\n",
      "2023-12-30 18:30:20 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:21 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:21 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:22 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:23 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:23 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:23 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:24 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:24 - INFO     | Early stopping: no decrease (0.090 vs 0.108); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:52<00:30, 10.22s/it]2023-12-30 18:30:24 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 18:30:24 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:24 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:25 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:25 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:25 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:26 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:26 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:26 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:27 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:27 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.008 | 0.110\n",
      "2023-12-30 18:30:27 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.000 | 0.111\n",
      "2023-12-30 18:30:28 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:28 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:28 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:28 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:29 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:29 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:29 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:30 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:30 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:30 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:31 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:32 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:33 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:33 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.000 | 0.106\n",
      "2023-12-30 18:30:33 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.000 | 0.106\n",
      "2023-12-30 18:30:34 - INFO     | Early stopping: no decrease (0.090 vs 0.106); counter: 3 out of 3\n",
      "2023-12-30 18:30:34 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:30:34 - INFO     | Reducing learning rate: 0.0003125 -> 0.00015625\n",
      " 90%|█████████ | 18/20 [03:02<00:20, 10.15s/it]2023-12-30 18:30:34 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 18:30:34 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.000 | 0.105\n",
      "2023-12-30 18:30:34 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:35 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.000 | 0.107\n",
      "2023-12-30 18:30:35 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.108\n",
      "2023-12-30 18:30:35 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.000 | 0.109\n",
      "2023-12-30 18:30:36 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:36 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.110\n",
      "2023-12-30 18:30:36 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.000 | 0.111\n",
      "2023-12-30 18:30:37 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.112\n",
      "2023-12-30 18:30:37 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.113\n",
      "2023-12-30 18:30:37 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.114\n",
      "2023-12-30 18:30:37 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.000 | 0.113\n",
      "2023-12-30 18:30:38 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.000 | 0.113\n",
      "2023-12-30 18:30:38 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.114\n",
      "2023-12-30 18:30:38 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.000 | 0.115\n",
      "2023-12-30 18:30:39 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.000 | 0.115\n",
      "2023-12-30 18:30:39 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.000 | 0.114\n",
      "2023-12-30 18:30:39 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.115\n",
      "2023-12-30 18:30:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.116\n",
      "2023-12-30 18:30:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:40 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.007 | 0.121\n",
      "2023-12-30 18:30:41 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.118\n",
      "2023-12-30 18:30:41 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.115\n",
      "2023-12-30 18:30:41 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.000 | 0.116\n",
      "2023-12-30 18:30:42 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.116\n",
      "2023-12-30 18:30:42 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:42 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:43 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:43 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.000 | 0.116\n",
      "2023-12-30 18:30:43 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.000 | 0.116\n",
      "2023-12-30 18:30:44 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.116\n",
      "2023-12-30 18:30:44 - INFO     | Early stopping: no decrease (0.090 vs 0.117); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [03:12<00:10, 10.16s/it]2023-12-30 18:30:44 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 18:30:44 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:45 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.000 | 0.118\n",
      "2023-12-30 18:30:45 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.004 | 0.120\n",
      "2023-12-30 18:30:45 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.120\n",
      "2023-12-30 18:30:46 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.120\n",
      "2023-12-30 18:30:46 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.120\n",
      "2023-12-30 18:30:46 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.000 | 0.120\n",
      "2023-12-30 18:30:47 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.120\n",
      "2023-12-30 18:30:47 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.120\n",
      "2023-12-30 18:30:47 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:47 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:48 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:48 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:48 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:49 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:49 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:49 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.119\n",
      "2023-12-30 18:30:50 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.118\n",
      "2023-12-30 18:30:50 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.118\n",
      "2023-12-30 18:30:50 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:51 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:52 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:52 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:52 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:53 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.117\n",
      "2023-12-30 18:30:54 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.118\n",
      "2023-12-30 18:30:54 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.118\n",
      "2023-12-30 18:30:54 - INFO     | Early stopping: no decrease (0.090 vs 0.118); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:23<00:00, 10.15s/it]\n",
      "2023-12-30 18:30:54 - INFO     | Best validation loss: 0.090\n",
      "2023-12-30 18:30:54 - INFO     | Best early stopping index/epoch: 5\n",
      "2023-12-30 18:30:54 - INFO     | Average Loss on test set: 0.133\n",
      "2023-12-30 18:30:57 - INFO     | Weighted Precision: 0.985, Recall: 0.985, F1: 0.985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>█████▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>██████████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▃▄▃▄▃▄▄▄▃▃▂▃▂▃▂▃▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▆▄▃▄▄▄▅▅▄▃▂▃▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▃▂▃▃▃▃▃▃▃▃</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_validation_loss</td><td>0.08975</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00016</td></tr><tr><td>step_learning_rate</td><td>0.00016</td></tr><tr><td>step_training_loss</td><td>2e-05</td></tr><tr><td>step_validation_loss</td><td>0.11778</td></tr><tr><td>test_loss</td><td>0.13274</td></tr><tr><td>weighted_f1</td><td>0.98457</td></tr><tr><td>weighted_precision</td><td>0.98461</td></tr><tr><td>weighted_recall</td><td>0.98457</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-sweep-21</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2vkn4mfv' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/2vkn4mfv</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_182731-2vkn4mfv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lrohpc9d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_183106-lrohpc9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/lrohpc9d' target=\"_blank\">glorious-sweep-22</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/lrohpc9d' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/lrohpc9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 64], 'learning_rate': 0.005, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:31:07 - INFO     | Epoch: 0 | Learning Rate: 0.005\n",
      "2023-12-30 18:31:07 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 00064 examples: 10.978 | 647.994\n",
      "2023-12-30 18:31:08 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 01920 examples: 22.446 | 2.294\n",
      "2023-12-30 18:31:08 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 03776 examples: 2.275 | 2.261\n",
      "2023-12-30 18:31:08 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 05632 examples: 2.252 | 2.214\n",
      "2023-12-30 18:31:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 07488 examples: 2.200 | 2.141\n",
      "2023-12-30 18:31:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 09344 examples: 2.080 | 2.016\n",
      "2023-12-30 18:31:09 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 11200 examples: 1.943 | 1.807\n",
      "2023-12-30 18:31:10 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 13056 examples: 1.654 | 1.417\n",
      "2023-12-30 18:31:10 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 14912 examples: 1.118 | 0.833\n",
      "2023-12-30 18:31:10 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 16768 examples: 0.709 | 0.561\n",
      "2023-12-30 18:31:11 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 18624 examples: 0.567 | 0.443\n",
      "2023-12-30 18:31:11 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 20480 examples: 0.451 | 0.395\n",
      "2023-12-30 18:31:11 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 22336 examples: 0.387 | 0.348\n",
      "2023-12-30 18:31:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 24192 examples: 0.300 | 0.299\n",
      "2023-12-30 18:31:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 26048 examples: 0.338 | 0.279\n",
      "2023-12-30 18:31:12 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 27904 examples: 0.271 | 0.234\n",
      "2023-12-30 18:31:13 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 29760 examples: 0.252 | 0.310\n",
      "2023-12-30 18:31:13 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 31616 examples: 0.246 | 0.233\n",
      "2023-12-30 18:31:13 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 33472 examples: 0.223 | 0.257\n",
      "2023-12-30 18:31:14 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 35328 examples: 0.232 | 0.194\n",
      "2023-12-30 18:31:14 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 37184 examples: 0.198 | 0.193\n",
      "2023-12-30 18:31:14 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 39040 examples: 0.207 | 0.182\n",
      "2023-12-30 18:31:14 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 40896 examples: 0.175 | 0.174\n",
      "2023-12-30 18:31:15 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 42752 examples: 0.205 | 0.189\n",
      "2023-12-30 18:31:15 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 44608 examples: 0.155 | 0.178\n",
      "2023-12-30 18:31:15 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 46464 examples: 0.159 | 0.171\n",
      "2023-12-30 18:31:16 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 48320 examples: 0.129 | 0.154\n",
      "2023-12-30 18:31:16 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 50176 examples: 0.160 | 0.166\n",
      "2023-12-30 18:31:16 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 52032 examples: 0.153 | 0.148\n",
      "2023-12-30 18:31:17 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 53888 examples: 0.147 | 0.138\n",
      "2023-12-30 18:31:17 - INFO     | Epoch: 0 | Learning Rate: 0.005: Training/Validation Loss after 55744 examples: 0.161 | 0.156\n",
      "2023-12-30 18:31:17 - INFO     | Early stopping: loss decreased (inf -> 0.142; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:15, 10.27s/it]2023-12-30 18:31:17 - INFO     | Epoch: 1 | Learning Rate: 0.005\n",
      "2023-12-30 18:31:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 56064 examples: 0.143 | 0.137\n",
      "2023-12-30 18:31:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 57920 examples: 0.127 | 0.136\n",
      "2023-12-30 18:31:18 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 59776 examples: 0.128 | 0.141\n",
      "2023-12-30 18:31:19 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 61632 examples: 0.116 | 0.130\n",
      "2023-12-30 18:31:19 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 63488 examples: 0.150 | 0.139\n",
      "2023-12-30 18:31:19 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 65344 examples: 0.130 | 0.162\n",
      "2023-12-30 18:31:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 67200 examples: 0.116 | 0.123\n",
      "2023-12-30 18:31:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 69056 examples: 0.117 | 0.117\n",
      "2023-12-30 18:31:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 70912 examples: 0.105 | 0.118\n",
      "2023-12-30 18:31:20 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 72768 examples: 0.121 | 0.125\n",
      "2023-12-30 18:31:21 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 74624 examples: 0.125 | 0.133\n",
      "2023-12-30 18:31:21 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 76480 examples: 0.107 | 0.133\n",
      "2023-12-30 18:31:21 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 78336 examples: 0.102 | 0.113\n",
      "2023-12-30 18:31:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 80192 examples: 0.120 | 0.105\n",
      "2023-12-30 18:31:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 82048 examples: 0.125 | 0.112\n",
      "2023-12-30 18:31:22 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 83904 examples: 0.111 | 0.120\n",
      "2023-12-30 18:31:23 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 85760 examples: 0.123 | 0.108\n",
      "2023-12-30 18:31:23 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 87616 examples: 0.103 | 0.112\n",
      "2023-12-30 18:31:23 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 89472 examples: 0.113 | 0.099\n",
      "2023-12-30 18:31:24 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 91328 examples: 0.099 | 0.101\n",
      "2023-12-30 18:31:24 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 93184 examples: 0.090 | 0.140\n",
      "2023-12-30 18:31:24 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 95040 examples: 0.080 | 0.109\n",
      "2023-12-30 18:31:24 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 96896 examples: 0.078 | 0.103\n",
      "2023-12-30 18:31:25 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 98752 examples: 0.095 | 0.111\n",
      "2023-12-30 18:31:25 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 100608 examples: 0.086 | 0.104\n",
      "2023-12-30 18:31:25 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 102464 examples: 0.099 | 0.098\n",
      "2023-12-30 18:31:26 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 104320 examples: 0.072 | 0.090\n",
      "2023-12-30 18:31:26 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 106176 examples: 0.085 | 0.117\n",
      "2023-12-30 18:31:26 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 108032 examples: 0.101 | 0.085\n",
      "2023-12-30 18:31:27 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 109888 examples: 0.091 | 0.088\n",
      "2023-12-30 18:31:27 - INFO     | Epoch: 1 | Learning Rate: 0.005: Training/Validation Loss after 111744 examples: 0.092 | 0.086\n",
      "2023-12-30 18:31:27 - INFO     | Early stopping: loss decreased (0.142 -> 0.087; -38.9%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:02, 10.14s/it]2023-12-30 18:31:27 - INFO     | Epoch: 2 | Learning Rate: 0.005\n",
      "2023-12-30 18:31:28 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 112064 examples: 0.025 | 0.096\n",
      "2023-12-30 18:31:28 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 113920 examples: 0.076 | 0.089\n",
      "2023-12-30 18:31:28 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 115776 examples: 0.076 | 0.105\n",
      "2023-12-30 18:31:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 117632 examples: 0.069 | 0.095\n",
      "2023-12-30 18:31:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 119488 examples: 0.090 | 0.096\n",
      "2023-12-30 18:31:29 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 121344 examples: 0.053 | 0.086\n",
      "2023-12-30 18:31:30 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 123200 examples: 0.094 | 0.118\n",
      "2023-12-30 18:31:30 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 125056 examples: 0.058 | 0.089\n",
      "2023-12-30 18:31:30 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 126912 examples: 0.061 | 0.081\n",
      "2023-12-30 18:31:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 128768 examples: 0.078 | 0.083\n",
      "2023-12-30 18:31:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 130624 examples: 0.071 | 0.085\n",
      "2023-12-30 18:31:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 132480 examples: 0.074 | 0.085\n",
      "2023-12-30 18:31:31 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 134336 examples: 0.076 | 0.080\n",
      "2023-12-30 18:31:32 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 136192 examples: 0.072 | 0.094\n",
      "2023-12-30 18:31:32 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 138048 examples: 0.066 | 0.077\n",
      "2023-12-30 18:31:32 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 139904 examples: 0.074 | 0.087\n",
      "2023-12-30 18:31:33 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 141760 examples: 0.069 | 0.083\n",
      "2023-12-30 18:31:33 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 143616 examples: 0.076 | 0.085\n",
      "2023-12-30 18:31:33 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 145472 examples: 0.062 | 0.075\n",
      "2023-12-30 18:31:34 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 147328 examples: 0.073 | 0.099\n",
      "2023-12-30 18:31:34 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 149184 examples: 0.081 | 0.079\n",
      "2023-12-30 18:31:34 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 151040 examples: 0.063 | 0.083\n",
      "2023-12-30 18:31:35 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 152896 examples: 0.068 | 0.103\n",
      "2023-12-30 18:31:35 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 154752 examples: 0.067 | 0.078\n",
      "2023-12-30 18:31:35 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 156608 examples: 0.060 | 0.080\n",
      "2023-12-30 18:31:36 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 158464 examples: 0.080 | 0.076\n",
      "2023-12-30 18:31:36 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 160320 examples: 0.050 | 0.079\n",
      "2023-12-30 18:31:36 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 162176 examples: 0.073 | 0.075\n",
      "2023-12-30 18:31:37 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 164032 examples: 0.056 | 0.073\n",
      "2023-12-30 18:31:37 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 165888 examples: 0.061 | 0.078\n",
      "2023-12-30 18:31:37 - INFO     | Epoch: 2 | Learning Rate: 0.005: Training/Validation Loss after 167744 examples: 0.063 | 0.085\n",
      "2023-12-30 18:31:37 - INFO     | Early stopping: loss decreased (0.087 -> 0.078; -10.4%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:52, 10.12s/it]2023-12-30 18:31:37 - INFO     | Epoch: 3 | Learning Rate: 0.005\n",
      "2023-12-30 18:31:38 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 168064 examples: 0.045 | 0.073\n",
      "2023-12-30 18:31:38 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 169920 examples: 0.062 | 0.071\n",
      "2023-12-30 18:31:38 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 171776 examples: 0.057 | 0.087\n",
      "2023-12-30 18:31:39 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 173632 examples: 0.038 | 0.073\n",
      "2023-12-30 18:31:39 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 175488 examples: 0.043 | 0.077\n",
      "2023-12-30 18:31:39 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 177344 examples: 0.040 | 0.078\n",
      "2023-12-30 18:31:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 179200 examples: 0.038 | 0.075\n",
      "2023-12-30 18:31:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 181056 examples: 0.063 | 0.075\n",
      "2023-12-30 18:31:40 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 182912 examples: 0.062 | 0.074\n",
      "2023-12-30 18:31:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 184768 examples: 0.054 | 0.074\n",
      "2023-12-30 18:31:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 186624 examples: 0.046 | 0.078\n",
      "2023-12-30 18:31:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 188480 examples: 0.058 | 0.071\n",
      "2023-12-30 18:31:41 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 190336 examples: 0.054 | 0.080\n",
      "2023-12-30 18:31:42 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 192192 examples: 0.050 | 0.071\n",
      "2023-12-30 18:31:42 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 194048 examples: 0.053 | 0.075\n",
      "2023-12-30 18:31:42 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 195904 examples: 0.060 | 0.079\n",
      "2023-12-30 18:31:43 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 197760 examples: 0.048 | 0.080\n",
      "2023-12-30 18:31:43 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 199616 examples: 0.046 | 0.073\n",
      "2023-12-30 18:31:43 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 201472 examples: 0.051 | 0.069\n",
      "2023-12-30 18:31:44 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 203328 examples: 0.068 | 0.075\n",
      "2023-12-30 18:31:44 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 205184 examples: 0.041 | 0.068\n",
      "2023-12-30 18:31:44 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 207040 examples: 0.046 | 0.071\n",
      "2023-12-30 18:31:45 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 208896 examples: 0.041 | 0.074\n",
      "2023-12-30 18:31:45 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 210752 examples: 0.044 | 0.067\n",
      "2023-12-30 18:31:45 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 212608 examples: 0.039 | 0.073\n",
      "2023-12-30 18:31:46 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 214464 examples: 0.061 | 0.075\n",
      "2023-12-30 18:31:46 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 216320 examples: 0.062 | 0.076\n",
      "2023-12-30 18:31:46 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 218176 examples: 0.051 | 0.079\n",
      "2023-12-30 18:31:46 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 220032 examples: 0.046 | 0.071\n",
      "2023-12-30 18:31:47 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 221888 examples: 0.060 | 0.072\n",
      "2023-12-30 18:31:47 - INFO     | Epoch: 3 | Learning Rate: 0.005: Training/Validation Loss after 223744 examples: 0.064 | 0.082\n",
      "2023-12-30 18:31:47 - INFO     | Early stopping: no decrease (0.078 vs 0.078); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:41, 10.08s/it]2023-12-30 18:31:47 - INFO     | Epoch: 4 | Learning Rate: 0.005\n",
      "2023-12-30 18:31:48 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 224064 examples: 0.045 | 0.075\n",
      "2023-12-30 18:31:48 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 225920 examples: 0.033 | 0.065\n",
      "2023-12-30 18:31:48 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 227776 examples: 0.037 | 0.072\n",
      "2023-12-30 18:31:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 229632 examples: 0.052 | 0.077\n",
      "2023-12-30 18:31:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 231488 examples: 0.041 | 0.069\n",
      "2023-12-30 18:31:49 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 233344 examples: 0.045 | 0.066\n",
      "2023-12-30 18:31:50 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 235200 examples: 0.034 | 0.065\n",
      "2023-12-30 18:31:50 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 237056 examples: 0.040 | 0.066\n",
      "2023-12-30 18:31:50 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 238912 examples: 0.036 | 0.065\n",
      "2023-12-30 18:31:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 240768 examples: 0.047 | 0.071\n",
      "2023-12-30 18:31:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 242624 examples: 0.025 | 0.067\n",
      "2023-12-30 18:31:51 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 244480 examples: 0.037 | 0.065\n",
      "2023-12-30 18:31:52 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 246336 examples: 0.035 | 0.077\n",
      "2023-12-30 18:31:52 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 248192 examples: 0.031 | 0.068\n",
      "2023-12-30 18:31:52 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 250048 examples: 0.055 | 0.081\n",
      "2023-12-30 18:31:53 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 251904 examples: 0.046 | 0.070\n",
      "2023-12-30 18:31:53 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 253760 examples: 0.032 | 0.070\n",
      "2023-12-30 18:31:53 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 255616 examples: 0.049 | 0.071\n",
      "2023-12-30 18:31:53 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 257472 examples: 0.046 | 0.072\n",
      "2023-12-30 18:31:54 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 259328 examples: 0.045 | 0.072\n",
      "2023-12-30 18:31:54 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 261184 examples: 0.046 | 0.076\n",
      "2023-12-30 18:31:54 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 263040 examples: 0.039 | 0.068\n",
      "2023-12-30 18:31:55 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 264896 examples: 0.040 | 0.064\n",
      "2023-12-30 18:31:55 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 266752 examples: 0.031 | 0.068\n",
      "2023-12-30 18:31:55 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 268608 examples: 0.034 | 0.071\n",
      "2023-12-30 18:31:56 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 270464 examples: 0.046 | 0.070\n",
      "2023-12-30 18:31:56 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 272320 examples: 0.032 | 0.071\n",
      "2023-12-30 18:31:56 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 274176 examples: 0.045 | 0.074\n",
      "2023-12-30 18:31:57 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 276032 examples: 0.043 | 0.065\n",
      "2023-12-30 18:31:57 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 277888 examples: 0.057 | 0.074\n",
      "2023-12-30 18:31:57 - INFO     | Epoch: 4 | Learning Rate: 0.005: Training/Validation Loss after 279744 examples: 0.050 | 0.083\n",
      "2023-12-30 18:31:58 - INFO     | Early stopping: loss decreased (0.078 -> 0.067; -13.8%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:50<02:31, 10.10s/it]2023-12-30 18:31:58 - INFO     | Epoch: 5 | Learning Rate: 0.005\n",
      "2023-12-30 18:31:58 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 280064 examples: 0.030 | 0.068\n",
      "2023-12-30 18:31:58 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 281920 examples: 0.022 | 0.076\n",
      "2023-12-30 18:31:59 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 283776 examples: 0.031 | 0.064\n",
      "2023-12-30 18:31:59 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 285632 examples: 0.020 | 0.067\n",
      "2023-12-30 18:31:59 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 287488 examples: 0.034 | 0.063\n",
      "2023-12-30 18:32:00 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 289344 examples: 0.027 | 0.067\n",
      "2023-12-30 18:32:00 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 291200 examples: 0.041 | 0.075\n",
      "2023-12-30 18:32:00 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 293056 examples: 0.032 | 0.071\n",
      "2023-12-30 18:32:00 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 294912 examples: 0.038 | 0.062\n",
      "2023-12-30 18:32:01 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 296768 examples: 0.034 | 0.061\n",
      "2023-12-30 18:32:01 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 298624 examples: 0.043 | 0.065\n",
      "2023-12-30 18:32:02 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 300480 examples: 0.028 | 0.064\n",
      "2023-12-30 18:32:02 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 302336 examples: 0.044 | 0.061\n",
      "2023-12-30 18:32:02 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 304192 examples: 0.021 | 0.062\n",
      "2023-12-30 18:32:02 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 306048 examples: 0.034 | 0.070\n",
      "2023-12-30 18:32:03 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 307904 examples: 0.030 | 0.063\n",
      "2023-12-30 18:32:03 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 309760 examples: 0.042 | 0.068\n",
      "2023-12-30 18:32:03 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 311616 examples: 0.023 | 0.062\n",
      "2023-12-30 18:32:04 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 313472 examples: 0.029 | 0.067\n",
      "2023-12-30 18:32:04 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 315328 examples: 0.045 | 0.066\n",
      "2023-12-30 18:32:04 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 317184 examples: 0.042 | 0.065\n",
      "2023-12-30 18:32:05 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 319040 examples: 0.027 | 0.069\n",
      "2023-12-30 18:32:05 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 320896 examples: 0.032 | 0.068\n",
      "2023-12-30 18:32:05 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 322752 examples: 0.038 | 0.068\n",
      "2023-12-30 18:32:06 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 324608 examples: 0.039 | 0.070\n",
      "2023-12-30 18:32:06 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 326464 examples: 0.035 | 0.069\n",
      "2023-12-30 18:32:06 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 328320 examples: 0.018 | 0.069\n",
      "2023-12-30 18:32:07 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 330176 examples: 0.034 | 0.070\n",
      "2023-12-30 18:32:07 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 332032 examples: 0.022 | 0.067\n",
      "2023-12-30 18:32:07 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 333888 examples: 0.023 | 0.061\n",
      "2023-12-30 18:32:08 - INFO     | Epoch: 5 | Learning Rate: 0.005: Training/Validation Loss after 335744 examples: 0.051 | 0.073\n",
      "2023-12-30 18:32:08 - INFO     | Early stopping: loss decreased (0.067 -> 0.062; -7.5%). Caching model state.\n",
      " 30%|███       | 6/20 [01:00<02:22, 10.15s/it]2023-12-30 18:32:08 - INFO     | Epoch: 6 | Learning Rate: 0.005\n",
      "2023-12-30 18:32:08 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 336064 examples: 0.015 | 0.062\n",
      "2023-12-30 18:32:08 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 337920 examples: 0.020 | 0.062\n",
      "2023-12-30 18:32:09 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 339776 examples: 0.019 | 0.059\n",
      "2023-12-30 18:32:09 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 341632 examples: 0.042 | 0.062\n",
      "2023-12-30 18:32:09 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 343488 examples: 0.021 | 0.070\n",
      "2023-12-30 18:32:10 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 345344 examples: 0.018 | 0.065\n",
      "2023-12-30 18:32:10 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 347200 examples: 0.021 | 0.064\n",
      "2023-12-30 18:32:10 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 349056 examples: 0.018 | 0.063\n",
      "2023-12-30 18:32:11 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 350912 examples: 0.015 | 0.062\n",
      "2023-12-30 18:32:11 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 352768 examples: 0.025 | 0.068\n",
      "2023-12-30 18:32:11 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 354624 examples: 0.022 | 0.067\n",
      "2023-12-30 18:32:12 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 356480 examples: 0.013 | 0.067\n",
      "2023-12-30 18:32:12 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 358336 examples: 0.032 | 0.064\n",
      "2023-12-30 18:32:12 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 360192 examples: 0.024 | 0.070\n",
      "2023-12-30 18:32:13 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 362048 examples: 0.028 | 0.062\n",
      "2023-12-30 18:32:13 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 363904 examples: 0.027 | 0.066\n",
      "2023-12-30 18:32:13 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 365760 examples: 0.032 | 0.068\n",
      "2023-12-30 18:32:14 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 367616 examples: 0.046 | 0.064\n",
      "2023-12-30 18:32:14 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 369472 examples: 0.026 | 0.068\n",
      "2023-12-30 18:32:14 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 371328 examples: 0.029 | 0.070\n",
      "2023-12-30 18:32:15 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 373184 examples: 0.023 | 0.065\n",
      "2023-12-30 18:32:15 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 375040 examples: 0.032 | 0.071\n",
      "2023-12-30 18:32:15 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 376896 examples: 0.030 | 0.061\n",
      "2023-12-30 18:32:15 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 378752 examples: 0.025 | 0.071\n",
      "2023-12-30 18:32:16 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 380608 examples: 0.045 | 0.070\n",
      "2023-12-30 18:32:16 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 382464 examples: 0.020 | 0.073\n",
      "2023-12-30 18:32:16 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 384320 examples: 0.030 | 0.062\n",
      "2023-12-30 18:32:17 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 386176 examples: 0.028 | 0.060\n",
      "2023-12-30 18:32:17 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 388032 examples: 0.026 | 0.065\n",
      "2023-12-30 18:32:17 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 389888 examples: 0.019 | 0.066\n",
      "2023-12-30 18:32:18 - INFO     | Epoch: 6 | Learning Rate: 0.005: Training/Validation Loss after 391744 examples: 0.025 | 0.061\n",
      "2023-12-30 18:32:18 - INFO     | Early stopping: no decrease (0.062 vs 0.072); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:10<02:11, 10.15s/it]2023-12-30 18:32:18 - INFO     | Epoch: 7 | Learning Rate: 0.005\n",
      "2023-12-30 18:32:18 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 392064 examples: 0.003 | 0.073\n",
      "2023-12-30 18:32:19 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 393920 examples: 0.014 | 0.065\n",
      "2023-12-30 18:32:19 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 395776 examples: 0.022 | 0.065\n",
      "2023-12-30 18:32:19 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 397632 examples: 0.017 | 0.058\n",
      "2023-12-30 18:32:20 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 399488 examples: 0.009 | 0.062\n",
      "2023-12-30 18:32:20 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 401344 examples: 0.021 | 0.065\n",
      "2023-12-30 18:32:20 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 403200 examples: 0.017 | 0.064\n",
      "2023-12-30 18:32:20 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 405056 examples: 0.020 | 0.060\n",
      "2023-12-30 18:32:21 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 406912 examples: 0.025 | 0.064\n",
      "2023-12-30 18:32:21 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 408768 examples: 0.025 | 0.061\n",
      "2023-12-30 18:32:21 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 410624 examples: 0.025 | 0.071\n",
      "2023-12-30 18:32:22 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 412480 examples: 0.026 | 0.064\n",
      "2023-12-30 18:32:22 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 414336 examples: 0.014 | 0.064\n",
      "2023-12-30 18:32:22 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 416192 examples: 0.021 | 0.068\n",
      "2023-12-30 18:32:23 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 418048 examples: 0.029 | 0.067\n",
      "2023-12-30 18:32:23 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 419904 examples: 0.026 | 0.066\n",
      "2023-12-30 18:32:23 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 421760 examples: 0.021 | 0.064\n",
      "2023-12-30 18:32:24 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 423616 examples: 0.017 | 0.068\n",
      "2023-12-30 18:32:24 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 425472 examples: 0.023 | 0.067\n",
      "2023-12-30 18:32:24 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 427328 examples: 0.029 | 0.063\n",
      "2023-12-30 18:32:25 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 429184 examples: 0.019 | 0.066\n",
      "2023-12-30 18:32:25 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 431040 examples: 0.022 | 0.078\n",
      "2023-12-30 18:32:25 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 432896 examples: 0.027 | 0.066\n",
      "2023-12-30 18:32:26 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 434752 examples: 0.014 | 0.066\n",
      "2023-12-30 18:32:26 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 436608 examples: 0.027 | 0.063\n",
      "2023-12-30 18:32:26 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 438464 examples: 0.020 | 0.064\n",
      "2023-12-30 18:32:26 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 440320 examples: 0.023 | 0.061\n",
      "2023-12-30 18:32:27 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 442176 examples: 0.021 | 0.060\n",
      "2023-12-30 18:32:27 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 444032 examples: 0.020 | 0.068\n",
      "2023-12-30 18:32:27 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 445888 examples: 0.027 | 0.075\n",
      "2023-12-30 18:32:28 - INFO     | Epoch: 7 | Learning Rate: 0.005: Training/Validation Loss after 447744 examples: 0.031 | 0.099\n",
      "2023-12-30 18:32:28 - INFO     | Early stopping: no decrease (0.062 vs 0.073); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:20<02:01, 10.11s/it]2023-12-30 18:32:28 - INFO     | Epoch: 8 | Learning Rate: 0.005\n",
      "2023-12-30 18:32:28 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 448064 examples: 0.012 | 0.070\n",
      "2023-12-30 18:32:29 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 449920 examples: 0.029 | 0.065\n",
      "2023-12-30 18:32:29 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 451776 examples: 0.017 | 0.065\n",
      "2023-12-30 18:32:29 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 453632 examples: 0.021 | 0.059\n",
      "2023-12-30 18:32:30 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 455488 examples: 0.014 | 0.064\n",
      "2023-12-30 18:32:30 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 457344 examples: 0.015 | 0.071\n",
      "2023-12-30 18:32:30 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 459200 examples: 0.017 | 0.063\n",
      "2023-12-30 18:32:30 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 461056 examples: 0.015 | 0.064\n",
      "2023-12-30 18:32:31 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 462912 examples: 0.032 | 0.066\n",
      "2023-12-30 18:32:31 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 464768 examples: 0.014 | 0.064\n",
      "2023-12-30 18:32:31 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 466624 examples: 0.025 | 0.069\n",
      "2023-12-30 18:32:32 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 468480 examples: 0.024 | 0.063\n",
      "2023-12-30 18:32:32 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 470336 examples: 0.015 | 0.072\n",
      "2023-12-30 18:32:32 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 472192 examples: 0.011 | 0.062\n",
      "2023-12-30 18:32:33 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 474048 examples: 0.016 | 0.067\n",
      "2023-12-30 18:32:33 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 475904 examples: 0.032 | 0.063\n",
      "2023-12-30 18:32:33 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 477760 examples: 0.014 | 0.061\n",
      "2023-12-30 18:32:34 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 479616 examples: 0.016 | 0.063\n",
      "2023-12-30 18:32:34 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 481472 examples: 0.022 | 0.061\n",
      "2023-12-30 18:32:34 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 483328 examples: 0.020 | 0.068\n",
      "2023-12-30 18:32:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 485184 examples: 0.024 | 0.059\n",
      "2023-12-30 18:32:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 487040 examples: 0.016 | 0.070\n",
      "2023-12-30 18:32:35 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 488896 examples: 0.010 | 0.066\n",
      "2023-12-30 18:32:36 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 490752 examples: 0.019 | 0.063\n",
      "2023-12-30 18:32:36 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 492608 examples: 0.010 | 0.063\n",
      "2023-12-30 18:32:36 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 494464 examples: 0.011 | 0.065\n",
      "2023-12-30 18:32:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 496320 examples: 0.020 | 0.064\n",
      "2023-12-30 18:32:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 498176 examples: 0.013 | 0.059\n",
      "2023-12-30 18:32:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 500032 examples: 0.019 | 0.063\n",
      "2023-12-30 18:32:37 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 501888 examples: 0.021 | 0.061\n",
      "2023-12-30 18:32:38 - INFO     | Epoch: 8 | Learning Rate: 0.005: Training/Validation Loss after 503744 examples: 0.012 | 0.065\n",
      "2023-12-30 18:32:38 - INFO     | Early stopping: no decrease (0.062 vs 0.066); counter: 3 out of 3\n",
      "2023-12-30 18:32:38 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:32:38 - INFO     | Reducing learning rate: 0.005 -> 0.0025\n",
      " 45%|████▌     | 9/20 [01:31<01:51, 10.11s/it]2023-12-30 18:32:38 - INFO     | Epoch: 9 | Learning Rate: 0.003\n",
      "2023-12-30 18:32:38 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 504064 examples: 0.003 | 0.066\n",
      "2023-12-30 18:32:39 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 505920 examples: 0.010 | 0.061\n",
      "2023-12-30 18:32:39 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 507776 examples: 0.010 | 0.061\n",
      "2023-12-30 18:32:39 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 509632 examples: 0.010 | 0.062\n",
      "2023-12-30 18:32:40 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 511488 examples: 0.010 | 0.059\n",
      "2023-12-30 18:32:40 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 513344 examples: 0.007 | 0.058\n",
      "2023-12-30 18:32:40 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 515200 examples: 0.004 | 0.059\n",
      "2023-12-30 18:32:41 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 517056 examples: 0.009 | 0.064\n",
      "2023-12-30 18:32:41 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 518912 examples: 0.011 | 0.062\n",
      "2023-12-30 18:32:41 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 520768 examples: 0.011 | 0.061\n",
      "2023-12-30 18:32:42 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 522624 examples: 0.016 | 0.061\n",
      "2023-12-30 18:32:42 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 524480 examples: 0.008 | 0.061\n",
      "2023-12-30 18:32:42 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 526336 examples: 0.013 | 0.060\n",
      "2023-12-30 18:32:42 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 528192 examples: 0.009 | 0.059\n",
      "2023-12-30 18:32:43 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 530048 examples: 0.012 | 0.060\n",
      "2023-12-30 18:32:43 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 531904 examples: 0.007 | 0.061\n",
      "2023-12-30 18:32:43 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 533760 examples: 0.015 | 0.060\n",
      "2023-12-30 18:32:44 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 535616 examples: 0.018 | 0.065\n",
      "2023-12-30 18:32:44 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 537472 examples: 0.012 | 0.063\n",
      "2023-12-30 18:32:44 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 539328 examples: 0.007 | 0.062\n",
      "2023-12-30 18:32:45 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 541184 examples: 0.010 | 0.061\n",
      "2023-12-30 18:32:45 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 543040 examples: 0.010 | 0.063\n",
      "2023-12-30 18:32:45 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 544896 examples: 0.013 | 0.063\n",
      "2023-12-30 18:32:46 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 546752 examples: 0.010 | 0.066\n",
      "2023-12-30 18:32:46 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 548608 examples: 0.010 | 0.063\n",
      "2023-12-30 18:32:46 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 550464 examples: 0.008 | 0.062\n",
      "2023-12-30 18:32:47 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 552320 examples: 0.010 | 0.061\n",
      "2023-12-30 18:32:47 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 554176 examples: 0.007 | 0.061\n",
      "2023-12-30 18:32:47 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 556032 examples: 0.019 | 0.062\n",
      "2023-12-30 18:32:48 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 557888 examples: 0.009 | 0.062\n",
      "2023-12-30 18:32:48 - INFO     | Epoch: 9 | Learning Rate: 0.003: Training/Validation Loss after 559744 examples: 0.008 | 0.061\n",
      "2023-12-30 18:32:48 - INFO     | Early stopping: no decrease (0.062 vs 0.062); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:41<01:40, 10.09s/it]2023-12-30 18:32:48 - INFO     | Epoch: 10 | Learning Rate: 0.003\n",
      "2023-12-30 18:32:48 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 560064 examples: 0.011 | 0.062\n",
      "2023-12-30 18:32:49 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 561920 examples: 0.006 | 0.061\n",
      "2023-12-30 18:32:49 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 563776 examples: 0.007 | 0.061\n",
      "2023-12-30 18:32:49 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 565632 examples: 0.006 | 0.064\n",
      "2023-12-30 18:32:50 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 567488 examples: 0.011 | 0.061\n",
      "2023-12-30 18:32:50 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 569344 examples: 0.008 | 0.063\n",
      "2023-12-30 18:32:50 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 571200 examples: 0.008 | 0.064\n",
      "2023-12-30 18:32:51 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 573056 examples: 0.007 | 0.062\n",
      "2023-12-30 18:32:51 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 574912 examples: 0.008 | 0.065\n",
      "2023-12-30 18:32:51 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 576768 examples: 0.005 | 0.061\n",
      "2023-12-30 18:32:52 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 578624 examples: 0.006 | 0.062\n",
      "2023-12-30 18:32:52 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 580480 examples: 0.008 | 0.065\n",
      "2023-12-30 18:32:52 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 582336 examples: 0.006 | 0.063\n",
      "2023-12-30 18:32:53 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 584192 examples: 0.007 | 0.064\n",
      "2023-12-30 18:32:53 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 586048 examples: 0.008 | 0.063\n",
      "2023-12-30 18:32:53 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 587904 examples: 0.009 | 0.063\n",
      "2023-12-30 18:32:54 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 589760 examples: 0.008 | 0.064\n",
      "2023-12-30 18:32:54 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 591616 examples: 0.008 | 0.063\n",
      "2023-12-30 18:32:54 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 593472 examples: 0.007 | 0.062\n",
      "2023-12-30 18:32:55 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 595328 examples: 0.004 | 0.062\n",
      "2023-12-30 18:32:55 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 597184 examples: 0.009 | 0.063\n",
      "2023-12-30 18:32:55 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 599040 examples: 0.010 | 0.062\n",
      "2023-12-30 18:32:55 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 600896 examples: 0.006 | 0.062\n",
      "2023-12-30 18:32:56 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 602752 examples: 0.013 | 0.062\n",
      "2023-12-30 18:32:56 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 604608 examples: 0.015 | 0.067\n",
      "2023-12-30 18:32:56 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 606464 examples: 0.009 | 0.066\n",
      "2023-12-30 18:32:57 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 608320 examples: 0.009 | 0.062\n",
      "2023-12-30 18:32:57 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 610176 examples: 0.007 | 0.061\n",
      "2023-12-30 18:32:57 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 612032 examples: 0.014 | 0.062\n",
      "2023-12-30 18:32:58 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 613888 examples: 0.014 | 0.063\n",
      "2023-12-30 18:32:58 - INFO     | Epoch: 10 | Learning Rate: 0.003: Training/Validation Loss after 615744 examples: 0.007 | 0.059\n",
      "2023-12-30 18:32:58 - INFO     | Early stopping: no decrease (0.062 vs 0.059); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:51<01:30, 10.07s/it]2023-12-30 18:32:58 - INFO     | Epoch: 11 | Learning Rate: 0.003\n",
      "2023-12-30 18:32:58 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 616064 examples: 0.003 | 0.059\n",
      "2023-12-30 18:32:59 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 617920 examples: 0.006 | 0.065\n",
      "2023-12-30 18:32:59 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 619776 examples: 0.005 | 0.061\n",
      "2023-12-30 18:32:59 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 621632 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:00 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 623488 examples: 0.005 | 0.061\n",
      "2023-12-30 18:33:00 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 625344 examples: 0.011 | 0.065\n",
      "2023-12-30 18:33:00 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 627200 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:01 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 629056 examples: 0.007 | 0.062\n",
      "2023-12-30 18:33:01 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 630912 examples: 0.006 | 0.065\n",
      "2023-12-30 18:33:01 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 632768 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:02 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 634624 examples: 0.017 | 0.062\n",
      "2023-12-30 18:33:02 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 636480 examples: 0.008 | 0.065\n",
      "2023-12-30 18:33:02 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 638336 examples: 0.004 | 0.065\n",
      "2023-12-30 18:33:03 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 640192 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:03 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 642048 examples: 0.008 | 0.063\n",
      "2023-12-30 18:33:03 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 643904 examples: 0.008 | 0.062\n",
      "2023-12-30 18:33:04 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 645760 examples: 0.007 | 0.061\n",
      "2023-12-30 18:33:04 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 647616 examples: 0.009 | 0.063\n",
      "2023-12-30 18:33:04 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 649472 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:04 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 651328 examples: 0.007 | 0.063\n",
      "2023-12-30 18:33:05 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 653184 examples: 0.008 | 0.064\n",
      "2023-12-30 18:33:05 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 655040 examples: 0.010 | 0.066\n",
      "2023-12-30 18:33:05 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 656896 examples: 0.006 | 0.064\n",
      "2023-12-30 18:33:06 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 658752 examples: 0.007 | 0.062\n",
      "2023-12-30 18:33:06 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 660608 examples: 0.011 | 0.061\n",
      "2023-12-30 18:33:06 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 662464 examples: 0.010 | 0.063\n",
      "2023-12-30 18:33:07 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 664320 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:07 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 666176 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:07 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 668032 examples: 0.006 | 0.064\n",
      "2023-12-30 18:33:08 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 669888 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:08 - INFO     | Epoch: 11 | Learning Rate: 0.003: Training/Validation Loss after 671744 examples: 0.004 | 0.062\n",
      "2023-12-30 18:33:08 - INFO     | Early stopping: no decrease (0.062 vs 0.062); counter: 3 out of 3\n",
      "2023-12-30 18:33:08 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:33:08 - INFO     | Reducing learning rate: 0.0025 -> 0.00125\n",
      " 60%|██████    | 12/20 [02:01<01:20, 10.10s/it]2023-12-30 18:33:08 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 18:33:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.010 | 0.061\n",
      "2023-12-30 18:33:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.004 | 0.062\n",
      "2023-12-30 18:33:09 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.007 | 0.063\n",
      "2023-12-30 18:33:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.008 | 0.062\n",
      "2023-12-30 18:33:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:10 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:11 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.010 | 0.063\n",
      "2023-12-30 18:33:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:12 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:13 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.003 | 0.062\n",
      "2023-12-30 18:33:13 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:13 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.009 | 0.064\n",
      "2023-12-30 18:33:14 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.006 | 0.064\n",
      "2023-12-30 18:33:14 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:14 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.007 | 0.063\n",
      "2023-12-30 18:33:15 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.012 | 0.062\n",
      "2023-12-30 18:33:15 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:15 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.006 | 0.064\n",
      "2023-12-30 18:33:15 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:16 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:16 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:16 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:17 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:17 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:17 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:18 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:18 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.004 | 0.062\n",
      "2023-12-30 18:33:18 - INFO     | Early stopping: no decrease (0.062 vs 0.063); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:11<01:10, 10.03s/it]2023-12-30 18:33:18 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 18:33:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.001 | 0.063\n",
      "2023-12-30 18:33:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.004 | 0.062\n",
      "2023-12-30 18:33:19 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.009 | 0.062\n",
      "2023-12-30 18:33:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.004 | 0.062\n",
      "2023-12-30 18:33:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:20 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.005 | 0.062\n",
      "2023-12-30 18:33:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.004 | 0.062\n",
      "2023-12-30 18:33:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.008 | 0.063\n",
      "2023-12-30 18:33:21 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.002 | 0.062\n",
      "2023-12-30 18:33:22 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:22 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:22 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:23 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:23 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:23 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.002 | 0.063\n",
      "2023-12-30 18:33:24 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:24 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:24 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:25 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:25 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:25 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.009 | 0.064\n",
      "2023-12-30 18:33:25 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:26 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.005 | 0.065\n",
      "2023-12-30 18:33:26 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:27 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:27 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:27 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.008 | 0.065\n",
      "2023-12-30 18:33:27 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:28 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.004 | 0.065\n",
      "2023-12-30 18:33:28 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:28 - INFO     | Early stopping: no decrease (0.062 vs 0.066); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:21<01:00, 10.05s/it]2023-12-30 18:33:28 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:33:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.005 | 0.066\n",
      "2023-12-30 18:33:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:29 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:30 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:31 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.003 | 0.065\n",
      "2023-12-30 18:33:31 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.013 | 0.063\n",
      "2023-12-30 18:33:31 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:32 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:32 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:32 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.004 | 0.065\n",
      "2023-12-30 18:33:32 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.003 | 0.065\n",
      "2023-12-30 18:33:33 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.009 | 0.065\n",
      "2023-12-30 18:33:33 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:33 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:34 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:34 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.004 | 0.065\n",
      "2023-12-30 18:33:34 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:35 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:35 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:35 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.002 | 0.063\n",
      "2023-12-30 18:33:36 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:36 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.006 | 0.063\n",
      "2023-12-30 18:33:36 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.007 | 0.063\n",
      "2023-12-30 18:33:37 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:37 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.005 | 0.063\n",
      "2023-12-30 18:33:37 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:38 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.005 | 0.065\n",
      "2023-12-30 18:33:38 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.005 | 0.065\n",
      "2023-12-30 18:33:38 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:39 - INFO     | Early stopping: no decrease (0.062 vs 0.064); counter: 3 out of 3\n",
      "2023-12-30 18:33:39 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:33:39 - INFO     | Reducing learning rate: 0.00125 -> 0.000625\n",
      " 75%|███████▌  | 15/20 [02:31<00:50, 10.09s/it]2023-12-30 18:33:39 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:33:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:39 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:40 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:40 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.003 | 0.063\n",
      "2023-12-30 18:33:40 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:41 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.004 | 0.063\n",
      "2023-12-30 18:33:41 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:41 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:42 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:42 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:42 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.007 | 0.064\n",
      "2023-12-30 18:33:43 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:43 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:43 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:43 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:44 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.007 | 0.065\n",
      "2023-12-30 18:33:44 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.002 | 0.065\n",
      "2023-12-30 18:33:44 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.002 | 0.064\n",
      "2023-12-30 18:33:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:45 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:46 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.005 | 0.065\n",
      "2023-12-30 18:33:46 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:46 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.007 | 0.064\n",
      "2023-12-30 18:33:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.003 | 0.065\n",
      "2023-12-30 18:33:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.003 | 0.065\n",
      "2023-12-30 18:33:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:47 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:48 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:48 - INFO     | Early stopping: no decrease (0.062 vs 0.064); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:41<00:40, 10.02s/it]2023-12-30 18:33:48 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:33:49 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.002 | 0.064\n",
      "2023-12-30 18:33:49 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:49 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:50 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:50 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.006 | 0.065\n",
      "2023-12-30 18:33:50 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.004 | 0.065\n",
      "2023-12-30 18:33:51 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:51 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:51 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:52 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.002 | 0.064\n",
      "2023-12-30 18:33:52 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:52 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:52 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.007 | 0.064\n",
      "2023-12-30 18:33:53 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:53 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:53 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.005 | 0.064\n",
      "2023-12-30 18:33:54 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.006 | 0.065\n",
      "2023-12-30 18:33:54 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.003 | 0.065\n",
      "2023-12-30 18:33:54 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.003 | 0.065\n",
      "2023-12-30 18:33:55 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:55 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:55 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.004 | 0.065\n",
      "2023-12-30 18:33:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:56 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.005 | 0.065\n",
      "2023-12-30 18:33:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:57 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.002 | 0.064\n",
      "2023-12-30 18:33:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:58 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:58 - INFO     | Early stopping: no decrease (0.062 vs 0.064); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:51<00:30, 10.00s/it]2023-12-30 18:33:58 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:33:59 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.003 | 0.064\n",
      "2023-12-30 18:33:59 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.004 | 0.064\n",
      "2023-12-30 18:33:59 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:00 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:00 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:00 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.003 | 0.064\n",
      "2023-12-30 18:34:00 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.007 | 0.064\n",
      "2023-12-30 18:34:01 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.003 | 0.064\n",
      "2023-12-30 18:34:01 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:01 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:02 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:02 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.008 | 0.065\n",
      "2023-12-30 18:34:02 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:03 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:03 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.003 | 0.064\n",
      "2023-12-30 18:34:03 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.005 | 0.064\n",
      "2023-12-30 18:34:04 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.006 | 0.064\n",
      "2023-12-30 18:34:04 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:04 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.004 | 0.064\n",
      "2023-12-30 18:34:05 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.003 | 0.064\n",
      "2023-12-30 18:34:05 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.003 | 0.064\n",
      "2023-12-30 18:34:05 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.004 | 0.064\n",
      "2023-12-30 18:34:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:06 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.007 | 0.065\n",
      "2023-12-30 18:34:07 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:08 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:08 - INFO     | Early stopping: no decrease (0.062 vs 0.065); counter: 3 out of 3\n",
      "2023-12-30 18:34:08 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:34:08 - INFO     | Reducing learning rate: 0.000625 -> 0.0003125\n",
      " 90%|█████████ | 18/20 [03:01<00:19, 10.00s/it]2023-12-30 18:34:08 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 18:34:09 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:09 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:09 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:10 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:10 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:10 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:11 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:11 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:11 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:11 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:12 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:12 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:12 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:13 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:13 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:13 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:14 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:15 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:15 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:15 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.006 | 0.065\n",
      "2023-12-30 18:34:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:16 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.006 | 0.065\n",
      "2023-12-30 18:34:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:17 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.007 | 0.065\n",
      "2023-12-30 18:34:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:18 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:18 - INFO     | Early stopping: no decrease (0.062 vs 0.065); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [03:11<00:10, 10.01s/it]2023-12-30 18:34:18 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 18:34:19 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.001 | 0.065\n",
      "2023-12-30 18:34:19 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:19 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:20 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:20 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:20 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:21 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:21 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:21 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:21 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:22 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:22 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:22 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:23 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.005 | 0.065\n",
      "2023-12-30 18:34:23 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:23 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:24 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.006 | 0.065\n",
      "2023-12-30 18:34:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.007 | 0.065\n",
      "2023-12-30 18:34:25 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:26 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.004 | 0.065\n",
      "2023-12-30 18:34:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:27 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.008 | 0.065\n",
      "2023-12-30 18:34:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.002 | 0.065\n",
      "2023-12-30 18:34:28 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.003 | 0.065\n",
      "2023-12-30 18:34:28 - INFO     | Early stopping: no decrease (0.062 vs 0.066); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:21<00:00, 10.07s/it]\n",
      "2023-12-30 18:34:28 - INFO     | Best validation loss: 0.062\n",
      "2023-12-30 18:34:28 - INFO     | Best early stopping index/epoch: 5\n",
      "2023-12-30 18:34:29 - INFO     | Average Loss on test set: 0.059\n",
      "2023-12-30 18:34:31 - INFO     | Weighted Precision: 0.983, Recall: 0.983, F1: 0.983\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>█████████▄▄▄▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>██████████████████▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_validation_loss</td><td>0.06217</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>0.00031</td></tr><tr><td>step_learning_rate</td><td>0.00031</td></tr><tr><td>step_training_loss</td><td>0.00306</td></tr><tr><td>step_validation_loss</td><td>0.06547</td></tr><tr><td>test_loss</td><td>0.05919</td></tr><tr><td>weighted_f1</td><td>0.98342</td></tr><tr><td>weighted_precision</td><td>0.98343</td></tr><tr><td>weighted_recall</td><td>0.98343</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-sweep-22</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/lrohpc9d' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/lrohpc9d</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_183106-lrohpc9d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f2qxqwjv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_183441-f2qxqwjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/f2qxqwjv' target=\"_blank\">denim-sweep-23</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/f2qxqwjv' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/f2qxqwjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 64], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'adam', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:34:42 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 18:34:42 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 17.436 | 34.318\n",
      "2023-12-30 18:34:43 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 10.056 | 0.833\n",
      "2023-12-30 18:34:43 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.541 | 0.354\n",
      "2023-12-30 18:34:43 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.313 | 0.256\n",
      "2023-12-30 18:34:44 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.243 | 0.202\n",
      "2023-12-30 18:34:44 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.177 | 0.181\n",
      "2023-12-30 18:34:44 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.167 | 0.150\n",
      "2023-12-30 18:34:45 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.156 | 0.151\n",
      "2023-12-30 18:34:45 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.178 | 0.145\n",
      "2023-12-30 18:34:45 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.155 | 0.125\n",
      "2023-12-30 18:34:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.137 | 0.118\n",
      "2023-12-30 18:34:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.133 | 0.124\n",
      "2023-12-30 18:34:46 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.115 | 0.101\n",
      "2023-12-30 18:34:47 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.122 | 0.129\n",
      "2023-12-30 18:34:47 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.107 | 0.089\n",
      "2023-12-30 18:34:47 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.115 | 0.126\n",
      "2023-12-30 18:34:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.104 | 0.107\n",
      "2023-12-30 18:34:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.124 | 0.094\n",
      "2023-12-30 18:34:48 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.089 | 0.082\n",
      "2023-12-30 18:34:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.076 | 0.090\n",
      "2023-12-30 18:34:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.121 | 0.096\n",
      "2023-12-30 18:34:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.103 | 0.080\n",
      "2023-12-30 18:34:49 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.091 | 0.099\n",
      "2023-12-30 18:34:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.077 | 0.098\n",
      "2023-12-30 18:34:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.091 | 0.092\n",
      "2023-12-30 18:34:50 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.111 | 0.088\n",
      "2023-12-30 18:34:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.112 | 0.087\n",
      "2023-12-30 18:34:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.117 | 0.097\n",
      "2023-12-30 18:34:51 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.113 | 0.073\n",
      "2023-12-30 18:34:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.082 | 0.099\n",
      "2023-12-30 18:34:52 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.100 | 0.083\n",
      "2023-12-30 18:34:52 - INFO     | Early stopping: loss decreased (inf -> 0.091; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:15, 10.30s/it]2023-12-30 18:34:52 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 18:34:53 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.008 | 0.098\n",
      "2023-12-30 18:34:53 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.067 | 0.095\n",
      "2023-12-30 18:34:53 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.053 | 0.098\n",
      "2023-12-30 18:34:54 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.082 | 0.094\n",
      "2023-12-30 18:34:54 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.066 | 0.078\n",
      "2023-12-30 18:34:54 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.082 | 0.084\n",
      "2023-12-30 18:34:54 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.077 | 0.090\n",
      "2023-12-30 18:34:55 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.082 | 0.080\n",
      "2023-12-30 18:34:55 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.049 | 0.078\n",
      "2023-12-30 18:34:55 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.095 | 0.068\n",
      "2023-12-30 18:34:56 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.060 | 0.080\n",
      "2023-12-30 18:34:56 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.089 | 0.070\n",
      "2023-12-30 18:34:56 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.075 | 0.074\n",
      "2023-12-30 18:34:57 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.068 | 0.078\n",
      "2023-12-30 18:34:57 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.076 | 0.070\n",
      "2023-12-30 18:34:57 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.068 | 0.086\n",
      "2023-12-30 18:34:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.060 | 0.077\n",
      "2023-12-30 18:34:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.056 | 0.074\n",
      "2023-12-30 18:34:58 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.048 | 0.075\n",
      "2023-12-30 18:34:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.057 | 0.071\n",
      "2023-12-30 18:34:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.051 | 0.072\n",
      "2023-12-30 18:34:59 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.064 | 0.084\n",
      "2023-12-30 18:35:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.061 | 0.079\n",
      "2023-12-30 18:35:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.091 | 0.083\n",
      "2023-12-30 18:35:00 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.058 | 0.069\n",
      "2023-12-30 18:35:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.066 | 0.068\n",
      "2023-12-30 18:35:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.081 | 0.073\n",
      "2023-12-30 18:35:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.077 | 0.067\n",
      "2023-12-30 18:35:01 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.060 | 0.082\n",
      "2023-12-30 18:35:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.068 | 0.077\n",
      "2023-12-30 18:35:02 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.062 | 0.067\n",
      "2023-12-30 18:35:02 - INFO     | Early stopping: loss decreased (0.091 -> 0.072; -20.1%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:03, 10.18s/it]2023-12-30 18:35:02 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 18:35:03 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.008 | 0.076\n",
      "2023-12-30 18:35:03 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.050 | 0.066\n",
      "2023-12-30 18:35:03 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.042 | 0.074\n",
      "2023-12-30 18:35:04 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.046 | 0.081\n",
      "2023-12-30 18:35:04 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.081 | 0.080\n",
      "2023-12-30 18:35:04 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.065 | 0.084\n",
      "2023-12-30 18:35:05 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.040 | 0.068\n",
      "2023-12-30 18:35:05 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.042 | 0.061\n",
      "2023-12-30 18:35:05 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.038 | 0.070\n",
      "2023-12-30 18:35:06 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.059 | 0.090\n",
      "2023-12-30 18:35:06 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.057 | 0.092\n",
      "2023-12-30 18:35:06 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.055 | 0.068\n",
      "2023-12-30 18:35:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.072 | 0.083\n",
      "2023-12-30 18:35:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.054 | 0.077\n",
      "2023-12-30 18:35:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.045 | 0.073\n",
      "2023-12-30 18:35:07 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.038 | 0.069\n",
      "2023-12-30 18:35:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.067 | 0.077\n",
      "2023-12-30 18:35:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.051 | 0.090\n",
      "2023-12-30 18:35:08 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.072 | 0.081\n",
      "2023-12-30 18:35:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.052 | 0.086\n",
      "2023-12-30 18:35:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.056 | 0.100\n",
      "2023-12-30 18:35:09 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.078 | 0.085\n",
      "2023-12-30 18:35:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.050 | 0.077\n",
      "2023-12-30 18:35:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.058 | 0.081\n",
      "2023-12-30 18:35:10 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.055 | 0.063\n",
      "2023-12-30 18:35:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.075 | 0.079\n",
      "2023-12-30 18:35:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.049 | 0.075\n",
      "2023-12-30 18:35:11 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.081 | 0.085\n",
      "2023-12-30 18:35:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.090 | 0.083\n",
      "2023-12-30 18:35:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.056 | 0.080\n",
      "2023-12-30 18:35:12 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.036 | 0.063\n",
      "2023-12-30 18:35:13 - INFO     | Early stopping: loss decreased (0.072 -> 0.061; -15.2%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:53, 10.21s/it]2023-12-30 18:35:13 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 18:35:13 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.017 | 0.062\n",
      "2023-12-30 18:35:13 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.030 | 0.061\n",
      "2023-12-30 18:35:14 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.029 | 0.070\n",
      "2023-12-30 18:35:14 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.063 | 0.080\n",
      "2023-12-30 18:35:14 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.036 | 0.065\n",
      "2023-12-30 18:35:15 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.028 | 0.066\n",
      "2023-12-30 18:35:15 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.041 | 0.080\n",
      "2023-12-30 18:35:15 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.037 | 0.087\n",
      "2023-12-30 18:35:16 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.036 | 0.070\n",
      "2023-12-30 18:35:16 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.044 | 0.080\n",
      "2023-12-30 18:35:16 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.042 | 0.070\n",
      "2023-12-30 18:35:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.053 | 0.083\n",
      "2023-12-30 18:35:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.048 | 0.071\n",
      "2023-12-30 18:35:17 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.062 | 0.078\n",
      "2023-12-30 18:35:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.046 | 0.087\n",
      "2023-12-30 18:35:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.061 | 0.082\n",
      "2023-12-30 18:35:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.063 | 0.077\n",
      "2023-12-30 18:35:18 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.056 | 0.065\n",
      "2023-12-30 18:35:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.037 | 0.063\n",
      "2023-12-30 18:35:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.050 | 0.072\n",
      "2023-12-30 18:35:19 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.039 | 0.071\n",
      "2023-12-30 18:35:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.069 | 0.080\n",
      "2023-12-30 18:35:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.061 | 0.070\n",
      "2023-12-30 18:35:20 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.038 | 0.084\n",
      "2023-12-30 18:35:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.071 | 0.077\n",
      "2023-12-30 18:35:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.067 | 0.061\n",
      "2023-12-30 18:35:21 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.038 | 0.061\n",
      "2023-12-30 18:35:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.049 | 0.073\n",
      "2023-12-30 18:35:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.053 | 0.077\n",
      "2023-12-30 18:35:22 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.048 | 0.087\n",
      "2023-12-30 18:35:23 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.045 | 0.069\n",
      "2023-12-30 18:35:23 - INFO     | Early stopping: no decrease (0.061 vs 0.064); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:43, 10.24s/it]2023-12-30 18:35:23 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 18:35:23 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.022 | 0.064\n",
      "2023-12-30 18:35:24 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.028 | 0.069\n",
      "2023-12-30 18:35:24 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.035 | 0.067\n",
      "2023-12-30 18:35:24 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.027 | 0.070\n",
      "2023-12-30 18:35:24 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.030 | 0.068\n",
      "2023-12-30 18:35:25 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.029 | 0.067\n",
      "2023-12-30 18:35:25 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.042 | 0.079\n",
      "2023-12-30 18:35:25 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.034 | 0.062\n",
      "2023-12-30 18:35:26 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.034 | 0.073\n",
      "2023-12-30 18:35:26 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.043 | 0.086\n",
      "2023-12-30 18:35:26 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.045 | 0.078\n",
      "2023-12-30 18:35:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.036 | 0.067\n",
      "2023-12-30 18:35:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.048 | 0.089\n",
      "2023-12-30 18:35:27 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.040 | 0.075\n",
      "2023-12-30 18:35:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.030 | 0.078\n",
      "2023-12-30 18:35:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.036 | 0.075\n",
      "2023-12-30 18:35:28 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.054 | 0.065\n",
      "2023-12-30 18:35:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.044 | 0.081\n",
      "2023-12-30 18:35:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.043 | 0.075\n",
      "2023-12-30 18:35:29 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.061 | 0.072\n",
      "2023-12-30 18:35:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.047 | 0.083\n",
      "2023-12-30 18:35:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.058 | 0.080\n",
      "2023-12-30 18:35:30 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.039 | 0.086\n",
      "2023-12-30 18:35:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.071 | 0.084\n",
      "2023-12-30 18:35:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.062 | 0.075\n",
      "2023-12-30 18:35:31 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.058 | 0.080\n",
      "2023-12-30 18:35:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.064 | 0.100\n",
      "2023-12-30 18:35:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.064 | 0.090\n",
      "2023-12-30 18:35:32 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.040 | 0.078\n",
      "2023-12-30 18:35:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.055 | 0.076\n",
      "2023-12-30 18:35:33 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.050 | 0.070\n",
      "2023-12-30 18:35:33 - INFO     | Early stopping: no decrease (0.061 vs 0.071); counter: 2 out of 3\n",
      " 25%|██▌       | 5/20 [00:51<02:33, 10.26s/it]2023-12-30 18:35:33 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 18:35:34 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.002 | 0.072\n",
      "2023-12-30 18:35:34 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.031 | 0.082\n",
      "2023-12-30 18:35:34 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.052 | 0.082\n",
      "2023-12-30 18:35:35 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.017 | 0.079\n",
      "2023-12-30 18:35:35 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.028 | 0.081\n",
      "2023-12-30 18:35:35 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.047 | 0.088\n",
      "2023-12-30 18:35:35 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.028 | 0.075\n",
      "2023-12-30 18:35:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.038 | 0.078\n",
      "2023-12-30 18:35:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.031 | 0.068\n",
      "2023-12-30 18:35:36 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.040 | 0.075\n",
      "2023-12-30 18:35:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.044 | 0.074\n",
      "2023-12-30 18:35:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.041 | 0.068\n",
      "2023-12-30 18:35:37 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.026 | 0.078\n",
      "2023-12-30 18:35:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.053 | 0.068\n",
      "2023-12-30 18:35:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.035 | 0.071\n",
      "2023-12-30 18:35:38 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.048 | 0.079\n",
      "2023-12-30 18:35:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.041 | 0.072\n",
      "2023-12-30 18:35:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.047 | 0.081\n",
      "2023-12-30 18:35:39 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.043 | 0.075\n",
      "2023-12-30 18:35:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.040 | 0.075\n",
      "2023-12-30 18:35:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.032 | 0.071\n",
      "2023-12-30 18:35:40 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.028 | 0.079\n",
      "2023-12-30 18:35:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.039 | 0.072\n",
      "2023-12-30 18:35:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.047 | 0.072\n",
      "2023-12-30 18:35:41 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.033 | 0.091\n",
      "2023-12-30 18:35:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.046 | 0.074\n",
      "2023-12-30 18:35:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.054 | 0.085\n",
      "2023-12-30 18:35:42 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.047 | 0.091\n",
      "2023-12-30 18:35:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.054 | 0.090\n",
      "2023-12-30 18:35:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.065 | 0.075\n",
      "2023-12-30 18:35:43 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.064 | 0.073\n",
      "2023-12-30 18:35:44 - INFO     | Early stopping: no decrease (0.061 vs 0.065); counter: 3 out of 3\n",
      "2023-12-30 18:35:44 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:35:44 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 30%|███       | 6/20 [01:01<02:24, 10.30s/it]2023-12-30 18:35:44 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 18:35:44 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.003 | 0.061\n",
      "2023-12-30 18:35:44 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.015 | 0.058\n",
      "2023-12-30 18:35:45 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.022 | 0.066\n",
      "2023-12-30 18:35:45 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.032 | 0.065\n",
      "2023-12-30 18:35:45 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.020 | 0.066\n",
      "2023-12-30 18:35:45 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.011 | 0.067\n",
      "2023-12-30 18:35:46 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.020 | 0.085\n",
      "2023-12-30 18:35:46 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.023 | 0.087\n",
      "2023-12-30 18:35:46 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.026 | 0.062\n",
      "2023-12-30 18:35:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.012 | 0.063\n",
      "2023-12-30 18:35:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.032 | 0.080\n",
      "2023-12-30 18:35:47 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.022 | 0.073\n",
      "2023-12-30 18:35:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.039 | 0.069\n",
      "2023-12-30 18:35:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.012 | 0.069\n",
      "2023-12-30 18:35:48 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.030 | 0.081\n",
      "2023-12-30 18:35:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.023 | 0.073\n",
      "2023-12-30 18:35:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.041 | 0.071\n",
      "2023-12-30 18:35:49 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.036 | 0.066\n",
      "2023-12-30 18:35:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.024 | 0.077\n",
      "2023-12-30 18:35:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.019 | 0.068\n",
      "2023-12-30 18:35:50 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.025 | 0.076\n",
      "2023-12-30 18:35:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.024 | 0.080\n",
      "2023-12-30 18:35:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.016 | 0.068\n",
      "2023-12-30 18:35:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.022 | 0.066\n",
      "2023-12-30 18:35:51 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.014 | 0.072\n",
      "2023-12-30 18:35:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.029 | 0.079\n",
      "2023-12-30 18:35:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.024 | 0.084\n",
      "2023-12-30 18:35:52 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.027 | 0.078\n",
      "2023-12-30 18:35:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.041 | 0.063\n",
      "2023-12-30 18:35:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.034 | 0.076\n",
      "2023-12-30 18:35:53 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.039 | 0.060\n",
      "2023-12-30 18:35:54 - INFO     | Early stopping: no decrease (0.061 vs 0.064); counter: 1 out of 3\n",
      " 35%|███▌      | 7/20 [01:11<02:12, 10.21s/it]2023-12-30 18:35:54 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 18:35:54 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.001 | 0.065\n",
      "2023-12-30 18:35:54 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.017 | 0.064\n",
      "2023-12-30 18:35:55 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.007 | 0.061\n",
      "2023-12-30 18:35:55 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.015 | 0.060\n",
      "2023-12-30 18:35:55 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.008 | 0.063\n",
      "2023-12-30 18:35:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.011 | 0.066\n",
      "2023-12-30 18:35:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.015 | 0.068\n",
      "2023-12-30 18:35:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.010 | 0.063\n",
      "2023-12-30 18:35:56 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.008 | 0.065\n",
      "2023-12-30 18:35:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.006 | 0.062\n",
      "2023-12-30 18:35:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.018 | 0.072\n",
      "2023-12-30 18:35:57 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.010 | 0.067\n",
      "2023-12-30 18:35:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.011 | 0.069\n",
      "2023-12-30 18:35:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.011 | 0.073\n",
      "2023-12-30 18:35:58 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.005 | 0.070\n",
      "2023-12-30 18:35:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.007 | 0.073\n",
      "2023-12-30 18:35:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.012 | 0.071\n",
      "2023-12-30 18:35:59 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.011 | 0.070\n",
      "2023-12-30 18:36:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.012 | 0.070\n",
      "2023-12-30 18:36:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.024 | 0.079\n",
      "2023-12-30 18:36:00 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.008 | 0.070\n",
      "2023-12-30 18:36:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.017 | 0.073\n",
      "2023-12-30 18:36:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.019 | 0.074\n",
      "2023-12-30 18:36:01 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.014 | 0.076\n",
      "2023-12-30 18:36:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.006 | 0.067\n",
      "2023-12-30 18:36:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.015 | 0.081\n",
      "2023-12-30 18:36:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.017 | 0.092\n",
      "2023-12-30 18:36:02 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.033 | 0.079\n",
      "2023-12-30 18:36:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.017 | 0.074\n",
      "2023-12-30 18:36:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.024 | 0.081\n",
      "2023-12-30 18:36:03 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.018 | 0.076\n",
      "2023-12-30 18:36:04 - INFO     | Early stopping: no decrease (0.061 vs 0.076); counter: 2 out of 3\n",
      " 40%|████      | 8/20 [01:21<02:02, 10.18s/it]2023-12-30 18:36:04 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 18:36:04 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.000 | 0.076\n",
      "2023-12-30 18:36:04 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.015 | 0.074\n",
      "2023-12-30 18:36:05 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.005 | 0.071\n",
      "2023-12-30 18:36:05 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.013 | 0.071\n",
      "2023-12-30 18:36:05 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.006 | 0.068\n",
      "2023-12-30 18:36:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.010 | 0.083\n",
      "2023-12-30 18:36:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.010 | 0.066\n",
      "2023-12-30 18:36:06 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.004 | 0.068\n",
      "2023-12-30 18:36:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.013 | 0.073\n",
      "2023-12-30 18:36:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.009 | 0.072\n",
      "2023-12-30 18:36:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.015 | 0.077\n",
      "2023-12-30 18:36:07 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.017 | 0.076\n",
      "2023-12-30 18:36:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.009 | 0.078\n",
      "2023-12-30 18:36:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.017 | 0.070\n",
      "2023-12-30 18:36:08 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.026 | 0.076\n",
      "2023-12-30 18:36:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.038 | 0.074\n",
      "2023-12-30 18:36:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.020 | 0.066\n",
      "2023-12-30 18:36:09 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.011 | 0.067\n",
      "2023-12-30 18:36:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.013 | 0.069\n",
      "2023-12-30 18:36:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.016 | 0.071\n",
      "2023-12-30 18:36:10 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.012 | 0.075\n",
      "2023-12-30 18:36:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.009 | 0.079\n",
      "2023-12-30 18:36:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.003 | 0.078\n",
      "2023-12-30 18:36:11 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.012 | 0.074\n",
      "2023-12-30 18:36:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.008 | 0.085\n",
      "2023-12-30 18:36:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.014 | 0.078\n",
      "2023-12-30 18:36:12 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.010 | 0.073\n",
      "2023-12-30 18:36:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.020 | 0.080\n",
      "2023-12-30 18:36:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.009 | 0.084\n",
      "2023-12-30 18:36:13 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.017 | 0.083\n",
      "2023-12-30 18:36:14 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.007 | 0.089\n",
      "2023-12-30 18:36:14 - INFO     | Early stopping: no decrease (0.061 vs 0.093); counter: 3 out of 3\n",
      "2023-12-30 18:36:14 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:36:14 - INFO     | Reducing learning rate: 0.0005 -> 0.00025\n",
      " 45%|████▌     | 9/20 [01:31<01:51, 10.16s/it]2023-12-30 18:36:14 - INFO     | Epoch: 9 | Learning Rate: 0.000\n",
      "2023-12-30 18:36:14 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 504064 examples: 0.000 | 0.091\n",
      "2023-12-30 18:36:15 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 505920 examples: 0.004 | 0.076\n",
      "2023-12-30 18:36:15 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 507776 examples: 0.009 | 0.075\n",
      "2023-12-30 18:36:15 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 509632 examples: 0.005 | 0.079\n",
      "2023-12-30 18:36:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 511488 examples: 0.004 | 0.074\n",
      "2023-12-30 18:36:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 513344 examples: 0.004 | 0.073\n",
      "2023-12-30 18:36:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 515200 examples: 0.005 | 0.073\n",
      "2023-12-30 18:36:16 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 517056 examples: 0.013 | 0.070\n",
      "2023-12-30 18:36:17 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 518912 examples: 0.008 | 0.073\n",
      "2023-12-30 18:36:17 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 520768 examples: 0.006 | 0.068\n",
      "2023-12-30 18:36:17 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 522624 examples: 0.007 | 0.066\n",
      "2023-12-30 18:36:18 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 524480 examples: 0.004 | 0.068\n",
      "2023-12-30 18:36:18 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 526336 examples: 0.004 | 0.069\n",
      "2023-12-30 18:36:18 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 528192 examples: 0.002 | 0.070\n",
      "2023-12-30 18:36:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 530048 examples: 0.001 | 0.069\n",
      "2023-12-30 18:36:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 531904 examples: 0.003 | 0.070\n",
      "2023-12-30 18:36:19 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 533760 examples: 0.004 | 0.068\n",
      "2023-12-30 18:36:20 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 535616 examples: 0.004 | 0.070\n",
      "2023-12-30 18:36:20 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 537472 examples: 0.002 | 0.069\n",
      "2023-12-30 18:36:20 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 539328 examples: 0.002 | 0.068\n",
      "2023-12-30 18:36:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 541184 examples: 0.003 | 0.067\n",
      "2023-12-30 18:36:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 543040 examples: 0.004 | 0.069\n",
      "2023-12-30 18:36:21 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 544896 examples: 0.006 | 0.067\n",
      "2023-12-30 18:36:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 546752 examples: 0.005 | 0.072\n",
      "2023-12-30 18:36:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 548608 examples: 0.003 | 0.067\n",
      "2023-12-30 18:36:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 550464 examples: 0.006 | 0.073\n",
      "2023-12-30 18:36:22 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 552320 examples: 0.002 | 0.072\n",
      "2023-12-30 18:36:23 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 554176 examples: 0.005 | 0.070\n",
      "2023-12-30 18:36:23 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 556032 examples: 0.012 | 0.070\n",
      "2023-12-30 18:36:23 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 557888 examples: 0.008 | 0.072\n",
      "2023-12-30 18:36:24 - INFO     | Epoch: 9 | Learning Rate: 0.000: Training/Validation Loss after 559744 examples: 0.005 | 0.064\n",
      "2023-12-30 18:36:24 - INFO     | Early stopping: no decrease (0.061 vs 0.065); counter: 1 out of 3\n",
      " 50%|█████     | 10/20 [01:42<01:41, 10.16s/it]2023-12-30 18:36:24 - INFO     | Epoch: 10 | Learning Rate: 0.000\n",
      "2023-12-30 18:36:24 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 560064 examples: 0.000 | 0.065\n",
      "2023-12-30 18:36:25 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 561920 examples: 0.002 | 0.065\n",
      "2023-12-30 18:36:25 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 563776 examples: 0.000 | 0.066\n",
      "2023-12-30 18:36:25 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 565632 examples: 0.002 | 0.064\n",
      "2023-12-30 18:36:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 567488 examples: 0.000 | 0.067\n",
      "2023-12-30 18:36:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 569344 examples: 0.001 | 0.066\n",
      "2023-12-30 18:36:26 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 571200 examples: 0.001 | 0.066\n",
      "2023-12-30 18:36:27 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 573056 examples: 0.002 | 0.068\n",
      "2023-12-30 18:36:27 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 574912 examples: 0.003 | 0.063\n",
      "2023-12-30 18:36:27 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 576768 examples: 0.001 | 0.062\n",
      "2023-12-30 18:36:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 578624 examples: 0.001 | 0.064\n",
      "2023-12-30 18:36:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 580480 examples: 0.001 | 0.065\n",
      "2023-12-30 18:36:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 582336 examples: 0.000 | 0.064\n",
      "2023-12-30 18:36:28 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 584192 examples: 0.001 | 0.065\n",
      "2023-12-30 18:36:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 586048 examples: 0.000 | 0.064\n",
      "2023-12-30 18:36:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 587904 examples: 0.001 | 0.063\n",
      "2023-12-30 18:36:29 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 589760 examples: 0.001 | 0.064\n",
      "2023-12-30 18:36:30 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 591616 examples: 0.001 | 0.066\n",
      "2023-12-30 18:36:30 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 593472 examples: 0.000 | 0.067\n",
      "2023-12-30 18:36:30 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 595328 examples: 0.001 | 0.068\n",
      "2023-12-30 18:36:31 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 597184 examples: 0.001 | 0.069\n",
      "2023-12-30 18:36:31 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 599040 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:31 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 600896 examples: 0.001 | 0.069\n",
      "2023-12-30 18:36:32 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 602752 examples: 0.000 | 0.070\n",
      "2023-12-30 18:36:32 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 604608 examples: 0.002 | 0.078\n",
      "2023-12-30 18:36:32 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 606464 examples: 0.004 | 0.070\n",
      "2023-12-30 18:36:33 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 608320 examples: 0.001 | 0.071\n",
      "2023-12-30 18:36:33 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 610176 examples: 0.003 | 0.070\n",
      "2023-12-30 18:36:33 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 612032 examples: 0.002 | 0.071\n",
      "2023-12-30 18:36:34 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 613888 examples: 0.003 | 0.069\n",
      "2023-12-30 18:36:34 - INFO     | Epoch: 10 | Learning Rate: 0.000: Training/Validation Loss after 615744 examples: 0.003 | 0.068\n",
      "2023-12-30 18:36:34 - INFO     | Early stopping: no decrease (0.061 vs 0.068); counter: 2 out of 3\n",
      " 55%|█████▌    | 11/20 [01:52<01:31, 10.20s/it]2023-12-30 18:36:34 - INFO     | Epoch: 11 | Learning Rate: 0.000\n",
      "2023-12-30 18:36:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 616064 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 617920 examples: 0.001 | 0.068\n",
      "2023-12-30 18:36:35 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 619776 examples: 0.000 | 0.068\n",
      "2023-12-30 18:36:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 621632 examples: 0.000 | 0.068\n",
      "2023-12-30 18:36:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 623488 examples: 0.000 | 0.068\n",
      "2023-12-30 18:36:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 625344 examples: 0.001 | 0.068\n",
      "2023-12-30 18:36:36 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 627200 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 629056 examples: 0.001 | 0.070\n",
      "2023-12-30 18:36:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 630912 examples: 0.001 | 0.067\n",
      "2023-12-30 18:36:37 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 632768 examples: 0.001 | 0.070\n",
      "2023-12-30 18:36:38 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 634624 examples: 0.001 | 0.068\n",
      "2023-12-30 18:36:38 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 636480 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:38 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 638336 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 640192 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 642048 examples: 0.000 | 0.069\n",
      "2023-12-30 18:36:39 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 643904 examples: 0.001 | 0.070\n",
      "2023-12-30 18:36:40 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 645760 examples: 0.001 | 0.070\n",
      "2023-12-30 18:36:40 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 647616 examples: 0.001 | 0.070\n",
      "2023-12-30 18:36:40 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 649472 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:41 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 651328 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:41 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 653184 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:41 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 655040 examples: 0.001 | 0.068\n",
      "2023-12-30 18:36:42 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 656896 examples: 0.001 | 0.068\n",
      "2023-12-30 18:36:42 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 658752 examples: 0.001 | 0.075\n",
      "2023-12-30 18:36:42 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 660608 examples: 0.001 | 0.067\n",
      "2023-12-30 18:36:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 662464 examples: 0.001 | 0.074\n",
      "2023-12-30 18:36:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 664320 examples: 0.001 | 0.067\n",
      "2023-12-30 18:36:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 666176 examples: 0.002 | 0.075\n",
      "2023-12-30 18:36:43 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 668032 examples: 0.004 | 0.078\n",
      "2023-12-30 18:36:44 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 669888 examples: 0.001 | 0.074\n",
      "2023-12-30 18:36:44 - INFO     | Epoch: 11 | Learning Rate: 0.000: Training/Validation Loss after 671744 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:44 - INFO     | Early stopping: no decrease (0.061 vs 0.074); counter: 3 out of 3\n",
      "2023-12-30 18:36:44 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:36:44 - INFO     | Reducing learning rate: 0.00025 -> 0.000125\n",
      " 60%|██████    | 12/20 [02:02<01:21, 10.17s/it]2023-12-30 18:36:44 - INFO     | Epoch: 12 | Learning Rate: 0.000\n",
      "2023-12-30 18:36:45 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 672064 examples: 0.000 | 0.075\n",
      "2023-12-30 18:36:45 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 673920 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:45 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 675776 examples: 0.000 | 0.073\n",
      "2023-12-30 18:36:46 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 677632 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:46 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 679488 examples: 0.000 | 0.071\n",
      "2023-12-30 18:36:46 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 681344 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 683200 examples: 0.000 | 0.071\n",
      "2023-12-30 18:36:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 685056 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:47 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 686912 examples: 0.001 | 0.074\n",
      "2023-12-30 18:36:48 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 688768 examples: 0.000 | 0.073\n",
      "2023-12-30 18:36:48 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 690624 examples: 0.000 | 0.070\n",
      "2023-12-30 18:36:48 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 692480 examples: 0.001 | 0.074\n",
      "2023-12-30 18:36:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 694336 examples: 0.001 | 0.072\n",
      "2023-12-30 18:36:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 696192 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:49 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 698048 examples: 0.001 | 0.072\n",
      "2023-12-30 18:36:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 699904 examples: 0.001 | 0.074\n",
      "2023-12-30 18:36:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 701760 examples: 0.001 | 0.073\n",
      "2023-12-30 18:36:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 703616 examples: 0.000 | 0.071\n",
      "2023-12-30 18:36:50 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 705472 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:51 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 707328 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:51 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 709184 examples: 0.000 | 0.072\n",
      "2023-12-30 18:36:51 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 711040 examples: 0.002 | 0.074\n",
      "2023-12-30 18:36:52 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 712896 examples: 0.000 | 0.076\n",
      "2023-12-30 18:36:52 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 714752 examples: 0.004 | 0.078\n",
      "2023-12-30 18:36:52 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 716608 examples: 0.000 | 0.073\n",
      "2023-12-30 18:36:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 718464 examples: 0.000 | 0.076\n",
      "2023-12-30 18:36:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 720320 examples: 0.000 | 0.076\n",
      "2023-12-30 18:36:53 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 722176 examples: 0.000 | 0.077\n",
      "2023-12-30 18:36:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 724032 examples: 0.000 | 0.077\n",
      "2023-12-30 18:36:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 725888 examples: 0.000 | 0.077\n",
      "2023-12-30 18:36:54 - INFO     | Epoch: 12 | Learning Rate: 0.000: Training/Validation Loss after 727744 examples: 0.000 | 0.077\n",
      "2023-12-30 18:36:55 - INFO     | Early stopping: no decrease (0.061 vs 0.078); counter: 1 out of 3\n",
      " 65%|██████▌   | 13/20 [02:12<01:11, 10.19s/it]2023-12-30 18:36:55 - INFO     | Epoch: 13 | Learning Rate: 0.000\n",
      "2023-12-30 18:36:55 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 728064 examples: 0.000 | 0.078\n",
      "2023-12-30 18:36:55 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 729920 examples: 0.000 | 0.077\n",
      "2023-12-30 18:36:56 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 731776 examples: 0.000 | 0.076\n",
      "2023-12-30 18:36:56 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 733632 examples: 0.000 | 0.075\n",
      "2023-12-30 18:36:56 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 735488 examples: 0.000 | 0.075\n",
      "2023-12-30 18:36:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 737344 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 739200 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 741056 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:57 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 742912 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:58 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 744768 examples: 0.000 | 0.073\n",
      "2023-12-30 18:36:58 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 746624 examples: 0.000 | 0.073\n",
      "2023-12-30 18:36:58 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 748480 examples: 0.000 | 0.073\n",
      "2023-12-30 18:36:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 750336 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 752192 examples: 0.000 | 0.074\n",
      "2023-12-30 18:36:59 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 754048 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:00 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 755904 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:00 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 757760 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:00 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 759616 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:01 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 761472 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:01 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 763328 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:01 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 765184 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:02 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 767040 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:02 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 768896 examples: 0.000 | 0.075\n",
      "2023-12-30 18:37:02 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 770752 examples: 0.000 | 0.075\n",
      "2023-12-30 18:37:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 772608 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 774464 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:03 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 776320 examples: 0.000 | 0.075\n",
      "2023-12-30 18:37:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 778176 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 780032 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 781888 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:04 - INFO     | Epoch: 13 | Learning Rate: 0.000: Training/Validation Loss after 783744 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:05 - INFO     | Early stopping: no decrease (0.061 vs 0.073); counter: 2 out of 3\n",
      " 70%|███████   | 14/20 [02:22<01:01, 10.17s/it]2023-12-30 18:37:05 - INFO     | Epoch: 14 | Learning Rate: 0.000\n",
      "2023-12-30 18:37:05 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 784064 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:05 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 785920 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 787776 examples: 0.000 | 0.072\n",
      "2023-12-30 18:37:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 789632 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:06 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 791488 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:07 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 793344 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:07 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 795200 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:07 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 797056 examples: 0.000 | 0.073\n",
      "2023-12-30 18:37:08 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 798912 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:08 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 800768 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:08 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 802624 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 804480 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 806336 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:09 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 808192 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 810048 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 811904 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:10 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 813760 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:11 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 815616 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:11 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 817472 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:11 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 819328 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 821184 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 823040 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 824896 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:12 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 826752 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:13 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 828608 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:13 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 830464 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:13 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 832320 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 834176 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 836032 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:14 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 837888 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:15 - INFO     | Epoch: 14 | Learning Rate: 0.000: Training/Validation Loss after 839744 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:15 - INFO     | Early stopping: no decrease (0.061 vs 0.074); counter: 3 out of 3\n",
      "2023-12-30 18:37:15 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:37:15 - INFO     | Reducing learning rate: 0.000125 -> 6.25e-05\n",
      " 75%|███████▌  | 15/20 [02:33<00:51, 10.20s/it]2023-12-30 18:37:15 - INFO     | Epoch: 15 | Learning Rate: 0.000\n",
      "2023-12-30 18:37:15 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 840064 examples: 0.000 | 0.074\n",
      "2023-12-30 18:37:16 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 841920 examples: 0.000 | 0.075\n",
      "2023-12-30 18:37:16 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 843776 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:16 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 845632 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 847488 examples: 0.000 | 0.078\n",
      "2023-12-30 18:37:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 849344 examples: 0.000 | 0.078\n",
      "2023-12-30 18:37:17 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 851200 examples: 0.000 | 0.077\n",
      "2023-12-30 18:37:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 853056 examples: 0.000 | 0.075\n",
      "2023-12-30 18:37:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 854912 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 856768 examples: 0.000 | 0.076\n",
      "2023-12-30 18:37:18 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 858624 examples: 0.000 | 0.077\n",
      "2023-12-30 18:37:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 860480 examples: 0.000 | 0.078\n",
      "2023-12-30 18:37:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 862336 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:19 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 864192 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:20 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 866048 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:20 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 867904 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:20 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 869760 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 871616 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 873472 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:21 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 875328 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:22 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 877184 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:22 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 879040 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:22 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 880896 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 882752 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 884608 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:23 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 886464 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:24 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 888320 examples: 0.000 | 0.078\n",
      "2023-12-30 18:37:24 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 890176 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:24 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 892032 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:25 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 893888 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:25 - INFO     | Epoch: 15 | Learning Rate: 0.000: Training/Validation Loss after 895744 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:25 - INFO     | Early stopping: no decrease (0.061 vs 0.080); counter: 1 out of 3\n",
      " 80%|████████  | 16/20 [02:43<00:40, 10.17s/it]2023-12-30 18:37:25 - INFO     | Epoch: 16 | Learning Rate: 0.000\n",
      "2023-12-30 18:37:25 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 896064 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:26 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 897920 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:26 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 899776 examples: 0.000 | 0.079\n",
      "2023-12-30 18:37:26 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 901632 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:27 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 903488 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:27 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 905344 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:27 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 907200 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 909056 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 910912 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:28 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 912768 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 914624 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 916480 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 918336 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:29 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 920192 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 922048 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 923904 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:30 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 925760 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 927616 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 929472 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:31 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 931328 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 933184 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 935040 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:32 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 936896 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 938752 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 940608 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:33 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 942464 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 944320 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 946176 examples: 0.000 | 0.080\n",
      "2023-12-30 18:37:34 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 948032 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 949888 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:35 - INFO     | Epoch: 16 | Learning Rate: 0.000: Training/Validation Loss after 951744 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:35 - INFO     | Early stopping: no decrease (0.061 vs 0.082); counter: 2 out of 3\n",
      " 85%|████████▌ | 17/20 [02:53<00:30, 10.11s/it]2023-12-30 18:37:35 - INFO     | Epoch: 17 | Learning Rate: 0.000\n",
      "2023-12-30 18:37:35 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 952064 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:36 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 953920 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:36 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 955776 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:36 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 957632 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 959488 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 961344 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:37 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 963200 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 965056 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 966912 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:38 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 968768 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 970624 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 972480 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:39 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 974336 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 976192 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 978048 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:40 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 979904 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 981760 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 983616 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 985472 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:41 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 987328 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 989184 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 991040 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:42 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 992896 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 994752 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 996608 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:43 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 998464 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1000320 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1002176 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:44 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1004032 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1005888 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:45 - INFO     | Epoch: 17 | Learning Rate: 0.000: Training/Validation Loss after 1007744 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:45 - INFO     | Early stopping: no decrease (0.061 vs 0.081); counter: 3 out of 3\n",
      "2023-12-30 18:37:45 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:37:45 - INFO     | Reducing learning rate: 6.25e-05 -> 3.125e-05\n",
      " 90%|█████████ | 18/20 [03:03<00:20, 10.11s/it]2023-12-30 18:37:45 - INFO     | Epoch: 18 | Learning Rate: 0.000\n",
      "2023-12-30 18:37:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1008064 examples: 0.000 | 0.081\n",
      "2023-12-30 18:37:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1009920 examples: 0.000 | 0.082\n",
      "2023-12-30 18:37:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1011776 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:46 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1013632 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1015488 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1017344 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:47 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1019200 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1021056 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1022912 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:48 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1024768 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:49 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1026624 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:49 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1028480 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:49 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1030336 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1032192 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1034048 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:50 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1035904 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1037760 examples: 0.000 | 0.083\n",
      "2023-12-30 18:37:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1039616 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:51 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1041472 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1043328 examples: 0.000 | 0.084\n",
      "2023-12-30 18:37:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1045184 examples: 0.000 | 0.085\n",
      "2023-12-30 18:37:52 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1047040 examples: 0.000 | 0.085\n",
      "2023-12-30 18:37:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1048896 examples: 0.000 | 0.085\n",
      "2023-12-30 18:37:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1050752 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:53 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1052608 examples: 0.000 | 0.085\n",
      "2023-12-30 18:37:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1054464 examples: 0.000 | 0.085\n",
      "2023-12-30 18:37:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1056320 examples: 0.000 | 0.085\n",
      "2023-12-30 18:37:54 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1058176 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1060032 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1061888 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:55 - INFO     | Epoch: 18 | Learning Rate: 0.000: Training/Validation Loss after 1063744 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:55 - INFO     | Early stopping: no decrease (0.061 vs 0.086); counter: 1 out of 3\n",
      " 95%|█████████▌| 19/20 [03:13<00:10, 10.14s/it]2023-12-30 18:37:55 - INFO     | Epoch: 19 | Learning Rate: 0.000\n",
      "2023-12-30 18:37:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1064064 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1065920 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:56 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1067776 examples: 0.000 | 0.087\n",
      "2023-12-30 18:37:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1069632 examples: 0.000 | 0.087\n",
      "2023-12-30 18:37:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1071488 examples: 0.000 | 0.087\n",
      "2023-12-30 18:37:57 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1073344 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:58 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1075200 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:58 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1077056 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:58 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1078912 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1080768 examples: 0.000 | 0.086\n",
      "2023-12-30 18:37:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1082624 examples: 0.000 | 0.087\n",
      "2023-12-30 18:37:59 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1084480 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1086336 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1088192 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:00 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1090048 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1091904 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1093760 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1095616 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:01 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1097472 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1099328 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1101184 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:02 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1103040 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1104896 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1106752 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:03 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1108608 examples: 0.000 | 0.086\n",
      "2023-12-30 18:38:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1110464 examples: 0.000 | 0.086\n",
      "2023-12-30 18:38:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1112320 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:04 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1114176 examples: 0.000 | 0.087\n",
      "2023-12-30 18:38:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1116032 examples: 0.000 | 0.086\n",
      "2023-12-30 18:38:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1117888 examples: 0.000 | 0.086\n",
      "2023-12-30 18:38:05 - INFO     | Epoch: 19 | Learning Rate: 0.000: Training/Validation Loss after 1119744 examples: 0.000 | 0.086\n",
      "2023-12-30 18:38:06 - INFO     | Early stopping: no decrease (0.061 vs 0.086); counter: 2 out of 3\n",
      "100%|██████████| 20/20 [03:23<00:00, 10.19s/it]\n",
      "2023-12-30 18:38:06 - INFO     | Best validation loss: 0.061\n",
      "2023-12-30 18:38:06 - INFO     | Best early stopping index/epoch: 2\n",
      "2023-12-30 18:38:06 - INFO     | Average Loss on test set: 0.078\n",
      "2023-12-30 18:38:08 - INFO     | Weighted Precision: 0.990, Recall: 0.990, F1: 0.990\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>step_learning_rate</td><td>████████████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_training_loss</td><td>█▆▁▄▃▃▂▃▂▄▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_validation_loss</td><td>█▆▄▂▁▃▂▁▂▂▂▂▁▂▁▂▁▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>weighted_f1</td><td>▁</td></tr><tr><td>weighted_precision</td><td>▁</td></tr><tr><td>weighted_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_validation_loss</td><td>0.06144</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>step_learning_rate</td><td>3e-05</td></tr><tr><td>step_training_loss</td><td>0.0</td></tr><tr><td>step_validation_loss</td><td>0.08604</td></tr><tr><td>test_loss</td><td>0.07759</td></tr><tr><td>weighted_f1</td><td>0.99014</td></tr><tr><td>weighted_precision</td><td>0.99016</td></tr><tr><td>weighted_recall</td><td>0.99014</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-sweep-23</strong> at: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/f2qxqwjv' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/f2qxqwjv</a><br/>Synced 5 W&B file(s), 4 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_183441-f2qxqwjv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9kl3o7rf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: CNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopped_count: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnotes: Notes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: pytorch-demo-v2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttags: ['pytorch', 'demo']\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/source/notebooks/wandb/run-20231230_183816-9kl3o7rf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/9kl3o7rf' target=\"_blank\">genial-sweep-24</a></strong> to <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/sweeps/7uanjfsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/9kl3o7rf' target=\"_blank\">https://wandb.ai/shane-kercheval/pytorch-demo-v2/runs/9kl3o7rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'CNN', 'batch_size': 64, 'early_stopped_count': 5, 'epochs': 20, 'kernels': [16, 64], 'learning_rate': 0.001, 'notes': 'Notes', 'optimizer': 'sgd', 'project': 'pytorch-demo-v2', 'tags': ['pytorch', 'demo']}\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]2023-12-30 18:38:17 - INFO     | Epoch: 0 | Learning Rate: 0.001\n",
      "2023-12-30 18:38:17 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 00064 examples: 16.615 | 158.633\n",
      "2023-12-30 18:38:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 01920 examples: 8.918 | 0.622\n",
      "2023-12-30 18:38:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 03776 examples: 0.495 | 0.398\n",
      "2023-12-30 18:38:18 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 05632 examples: 0.325 | 0.312\n",
      "2023-12-30 18:38:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 07488 examples: 0.277 | 0.261\n",
      "2023-12-30 18:38:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 09344 examples: 0.299 | 0.237\n",
      "2023-12-30 18:38:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 11200 examples: 0.218 | 0.210\n",
      "2023-12-30 18:38:19 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 13056 examples: 0.202 | 0.204\n",
      "2023-12-30 18:38:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 14912 examples: 0.189 | 0.181\n",
      "2023-12-30 18:38:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 16768 examples: 0.192 | 0.174\n",
      "2023-12-30 18:38:20 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 18624 examples: 0.197 | 0.169\n",
      "2023-12-30 18:38:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 20480 examples: 0.162 | 0.156\n",
      "2023-12-30 18:38:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 22336 examples: 0.151 | 0.142\n",
      "2023-12-30 18:38:21 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 24192 examples: 0.139 | 0.139\n",
      "2023-12-30 18:38:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 26048 examples: 0.149 | 0.144\n",
      "2023-12-30 18:38:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 27904 examples: 0.126 | 0.126\n",
      "2023-12-30 18:38:22 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 29760 examples: 0.136 | 0.123\n",
      "2023-12-30 18:38:23 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 31616 examples: 0.139 | 0.129\n",
      "2023-12-30 18:38:23 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 33472 examples: 0.119 | 0.126\n",
      "2023-12-30 18:38:23 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 35328 examples: 0.122 | 0.120\n",
      "2023-12-30 18:38:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 37184 examples: 0.120 | 0.120\n",
      "2023-12-30 18:38:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 39040 examples: 0.120 | 0.120\n",
      "2023-12-30 18:38:24 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 40896 examples: 0.109 | 0.110\n",
      "2023-12-30 18:38:25 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 42752 examples: 0.110 | 0.110\n",
      "2023-12-30 18:38:25 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 44608 examples: 0.103 | 0.119\n",
      "2023-12-30 18:38:25 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 46464 examples: 0.112 | 0.111\n",
      "2023-12-30 18:38:26 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 48320 examples: 0.111 | 0.102\n",
      "2023-12-30 18:38:26 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 50176 examples: 0.102 | 0.105\n",
      "2023-12-30 18:38:26 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 52032 examples: 0.094 | 0.104\n",
      "2023-12-30 18:38:27 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 53888 examples: 0.106 | 0.113\n",
      "2023-12-30 18:38:27 - INFO     | Epoch: 0 | Learning Rate: 0.001: Training/Validation Loss after 55744 examples: 0.123 | 0.106\n",
      "2023-12-30 18:38:27 - INFO     | Early stopping: loss decreased (inf -> 0.096; nan%). Caching model state.\n",
      "  5%|▌         | 1/20 [00:10<03:15, 10.30s/it]2023-12-30 18:38:27 - INFO     | Epoch: 1 | Learning Rate: 0.001\n",
      "2023-12-30 18:38:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 56064 examples: 0.187 | 0.094\n",
      "2023-12-30 18:38:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 57920 examples: 0.087 | 0.096\n",
      "2023-12-30 18:38:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 59776 examples: 0.071 | 0.090\n",
      "2023-12-30 18:38:28 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 61632 examples: 0.072 | 0.095\n",
      "2023-12-30 18:38:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 63488 examples: 0.078 | 0.094\n",
      "2023-12-30 18:38:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 65344 examples: 0.099 | 0.089\n",
      "2023-12-30 18:38:29 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 67200 examples: 0.071 | 0.107\n",
      "2023-12-30 18:38:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 69056 examples: 0.093 | 0.088\n",
      "2023-12-30 18:38:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 70912 examples: 0.089 | 0.088\n",
      "2023-12-30 18:38:30 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 72768 examples: 0.066 | 0.085\n",
      "2023-12-30 18:38:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 74624 examples: 0.058 | 0.088\n",
      "2023-12-30 18:38:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 76480 examples: 0.081 | 0.093\n",
      "2023-12-30 18:38:31 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 78336 examples: 0.079 | 0.088\n",
      "2023-12-30 18:38:32 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 80192 examples: 0.076 | 0.087\n",
      "2023-12-30 18:38:32 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 82048 examples: 0.076 | 0.087\n",
      "2023-12-30 18:38:32 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 83904 examples: 0.062 | 0.082\n",
      "2023-12-30 18:38:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 85760 examples: 0.070 | 0.085\n",
      "2023-12-30 18:38:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 87616 examples: 0.074 | 0.085\n",
      "2023-12-30 18:38:33 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 89472 examples: 0.061 | 0.080\n",
      "2023-12-30 18:38:34 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 91328 examples: 0.067 | 0.083\n",
      "2023-12-30 18:38:34 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 93184 examples: 0.066 | 0.080\n",
      "2023-12-30 18:38:34 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 95040 examples: 0.058 | 0.085\n",
      "2023-12-30 18:38:35 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 96896 examples: 0.060 | 0.082\n",
      "2023-12-30 18:38:35 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 98752 examples: 0.081 | 0.081\n",
      "2023-12-30 18:38:35 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 100608 examples: 0.069 | 0.079\n",
      "2023-12-30 18:38:36 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 102464 examples: 0.067 | 0.078\n",
      "2023-12-30 18:38:36 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 104320 examples: 0.078 | 0.077\n",
      "2023-12-30 18:38:36 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 106176 examples: 0.067 | 0.079\n",
      "2023-12-30 18:38:37 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 108032 examples: 0.078 | 0.081\n",
      "2023-12-30 18:38:37 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 109888 examples: 0.061 | 0.075\n",
      "2023-12-30 18:38:37 - INFO     | Epoch: 1 | Learning Rate: 0.001: Training/Validation Loss after 111744 examples: 0.052 | 0.073\n",
      "2023-12-30 18:38:37 - INFO     | Early stopping: loss decreased (0.096 -> 0.072; -25.8%). Caching model state.\n",
      " 10%|█         | 2/20 [00:20<03:05, 10.29s/it]2023-12-30 18:38:37 - INFO     | Epoch: 2 | Learning Rate: 0.001\n",
      "2023-12-30 18:38:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 112064 examples: 0.025 | 0.071\n",
      "2023-12-30 18:38:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 113920 examples: 0.050 | 0.074\n",
      "2023-12-30 18:38:38 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 115776 examples: 0.052 | 0.074\n",
      "2023-12-30 18:38:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 117632 examples: 0.064 | 0.076\n",
      "2023-12-30 18:38:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 119488 examples: 0.049 | 0.070\n",
      "2023-12-30 18:38:39 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 121344 examples: 0.062 | 0.070\n",
      "2023-12-30 18:38:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 123200 examples: 0.053 | 0.077\n",
      "2023-12-30 18:38:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 125056 examples: 0.053 | 0.075\n",
      "2023-12-30 18:38:40 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 126912 examples: 0.050 | 0.088\n",
      "2023-12-30 18:38:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 128768 examples: 0.049 | 0.078\n",
      "2023-12-30 18:38:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 130624 examples: 0.050 | 0.075\n",
      "2023-12-30 18:38:41 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 132480 examples: 0.065 | 0.072\n",
      "2023-12-30 18:38:42 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 134336 examples: 0.029 | 0.072\n",
      "2023-12-30 18:38:42 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 136192 examples: 0.050 | 0.075\n",
      "2023-12-30 18:38:42 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 138048 examples: 0.047 | 0.071\n",
      "2023-12-30 18:38:43 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 139904 examples: 0.048 | 0.071\n",
      "2023-12-30 18:38:43 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 141760 examples: 0.050 | 0.074\n",
      "2023-12-30 18:38:43 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 143616 examples: 0.054 | 0.072\n",
      "2023-12-30 18:38:44 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 145472 examples: 0.064 | 0.069\n",
      "2023-12-30 18:38:44 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 147328 examples: 0.059 | 0.072\n",
      "2023-12-30 18:38:44 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 149184 examples: 0.063 | 0.072\n",
      "2023-12-30 18:38:44 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 151040 examples: 0.045 | 0.073\n",
      "2023-12-30 18:38:45 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 152896 examples: 0.046 | 0.067\n",
      "2023-12-30 18:38:45 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 154752 examples: 0.053 | 0.068\n",
      "2023-12-30 18:38:45 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 156608 examples: 0.043 | 0.067\n",
      "2023-12-30 18:38:46 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 158464 examples: 0.051 | 0.063\n",
      "2023-12-30 18:38:46 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 160320 examples: 0.052 | 0.068\n",
      "2023-12-30 18:38:46 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 162176 examples: 0.039 | 0.071\n",
      "2023-12-30 18:38:47 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 164032 examples: 0.049 | 0.067\n",
      "2023-12-30 18:38:47 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 165888 examples: 0.052 | 0.067\n",
      "2023-12-30 18:38:47 - INFO     | Epoch: 2 | Learning Rate: 0.001: Training/Validation Loss after 167744 examples: 0.049 | 0.065\n",
      "2023-12-30 18:38:48 - INFO     | Early stopping: loss decreased (0.072 -> 0.066; -8.0%). Caching model state.\n",
      " 15%|█▌        | 3/20 [00:30<02:53, 10.21s/it]2023-12-30 18:38:48 - INFO     | Epoch: 3 | Learning Rate: 0.001\n",
      "2023-12-30 18:38:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 168064 examples: 0.014 | 0.065\n",
      "2023-12-30 18:38:48 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 169920 examples: 0.032 | 0.068\n",
      "2023-12-30 18:38:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 171776 examples: 0.043 | 0.064\n",
      "2023-12-30 18:38:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 173632 examples: 0.045 | 0.065\n",
      "2023-12-30 18:38:49 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 175488 examples: 0.027 | 0.066\n",
      "2023-12-30 18:38:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 177344 examples: 0.035 | 0.066\n",
      "2023-12-30 18:38:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 179200 examples: 0.043 | 0.066\n",
      "2023-12-30 18:38:50 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 181056 examples: 0.039 | 0.062\n",
      "2023-12-30 18:38:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 182912 examples: 0.034 | 0.069\n",
      "2023-12-30 18:38:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 184768 examples: 0.043 | 0.062\n",
      "2023-12-30 18:38:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 186624 examples: 0.049 | 0.064\n",
      "2023-12-30 18:38:51 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 188480 examples: 0.045 | 0.069\n",
      "2023-12-30 18:38:52 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 190336 examples: 0.034 | 0.068\n",
      "2023-12-30 18:38:52 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 192192 examples: 0.039 | 0.062\n",
      "2023-12-30 18:38:52 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 194048 examples: 0.034 | 0.062\n",
      "2023-12-30 18:38:53 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 195904 examples: 0.049 | 0.061\n",
      "2023-12-30 18:38:53 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 197760 examples: 0.039 | 0.061\n",
      "2023-12-30 18:38:53 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 199616 examples: 0.058 | 0.061\n",
      "2023-12-30 18:38:54 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 201472 examples: 0.051 | 0.061\n",
      "2023-12-30 18:38:54 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 203328 examples: 0.049 | 0.069\n",
      "2023-12-30 18:38:54 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 205184 examples: 0.043 | 0.065\n",
      "2023-12-30 18:38:55 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 207040 examples: 0.044 | 0.065\n",
      "2023-12-30 18:38:55 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 208896 examples: 0.031 | 0.059\n",
      "2023-12-30 18:38:55 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 210752 examples: 0.042 | 0.058\n",
      "2023-12-30 18:38:56 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 212608 examples: 0.036 | 0.060\n",
      "2023-12-30 18:38:56 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 214464 examples: 0.044 | 0.061\n",
      "2023-12-30 18:38:56 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 216320 examples: 0.040 | 0.063\n",
      "2023-12-30 18:38:56 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 218176 examples: 0.038 | 0.058\n",
      "2023-12-30 18:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 220032 examples: 0.041 | 0.067\n",
      "2023-12-30 18:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 221888 examples: 0.035 | 0.069\n",
      "2023-12-30 18:38:57 - INFO     | Epoch: 3 | Learning Rate: 0.001: Training/Validation Loss after 223744 examples: 0.053 | 0.063\n",
      "2023-12-30 18:38:58 - INFO     | Early stopping: no decrease (0.066 vs 0.063); counter: 1 out of 3\n",
      " 20%|██        | 4/20 [00:40<02:42, 10.18s/it]2023-12-30 18:38:58 - INFO     | Epoch: 4 | Learning Rate: 0.001\n",
      "2023-12-30 18:38:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 224064 examples: 0.011 | 0.062\n",
      "2023-12-30 18:38:58 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 225920 examples: 0.033 | 0.059\n",
      "2023-12-30 18:38:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 227776 examples: 0.026 | 0.057\n",
      "2023-12-30 18:38:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 229632 examples: 0.033 | 0.057\n",
      "2023-12-30 18:38:59 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 231488 examples: 0.037 | 0.058\n",
      "2023-12-30 18:39:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 233344 examples: 0.027 | 0.057\n",
      "2023-12-30 18:39:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 235200 examples: 0.037 | 0.061\n",
      "2023-12-30 18:39:00 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 237056 examples: 0.027 | 0.058\n",
      "2023-12-30 18:39:01 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 238912 examples: 0.035 | 0.060\n",
      "2023-12-30 18:39:01 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 240768 examples: 0.031 | 0.060\n",
      "2023-12-30 18:39:01 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 242624 examples: 0.037 | 0.057\n",
      "2023-12-30 18:39:02 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 244480 examples: 0.035 | 0.059\n",
      "2023-12-30 18:39:02 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 246336 examples: 0.042 | 0.060\n",
      "2023-12-30 18:39:02 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 248192 examples: 0.038 | 0.059\n",
      "2023-12-30 18:39:03 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 250048 examples: 0.032 | 0.062\n",
      "2023-12-30 18:39:03 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 251904 examples: 0.027 | 0.056\n",
      "2023-12-30 18:39:03 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 253760 examples: 0.022 | 0.057\n",
      "2023-12-30 18:39:04 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 255616 examples: 0.039 | 0.057\n",
      "2023-12-30 18:39:04 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 257472 examples: 0.039 | 0.056\n",
      "2023-12-30 18:39:04 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 259328 examples: 0.031 | 0.056\n",
      "2023-12-30 18:39:05 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 261184 examples: 0.040 | 0.062\n",
      "2023-12-30 18:39:05 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 263040 examples: 0.032 | 0.054\n",
      "2023-12-30 18:39:05 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 264896 examples: 0.029 | 0.056\n",
      "2023-12-30 18:39:05 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 266752 examples: 0.038 | 0.052\n",
      "2023-12-30 18:39:06 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 268608 examples: 0.028 | 0.057\n",
      "2023-12-30 18:39:06 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 270464 examples: 0.036 | 0.059\n",
      "2023-12-30 18:39:06 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 272320 examples: 0.031 | 0.062\n",
      "2023-12-30 18:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 274176 examples: 0.040 | 0.056\n",
      "2023-12-30 18:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 276032 examples: 0.033 | 0.057\n",
      "2023-12-30 18:39:07 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 277888 examples: 0.029 | 0.056\n",
      "2023-12-30 18:39:08 - INFO     | Epoch: 4 | Learning Rate: 0.001: Training/Validation Loss after 279744 examples: 0.046 | 0.057\n",
      "2023-12-30 18:39:08 - INFO     | Early stopping: loss decreased (0.066 -> 0.055; -16.1%). Caching model state.\n",
      " 25%|██▌       | 5/20 [00:51<02:33, 10.23s/it]2023-12-30 18:39:08 - INFO     | Epoch: 5 | Learning Rate: 0.001\n",
      "2023-12-30 18:39:08 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 280064 examples: 0.010 | 0.055\n",
      "2023-12-30 18:39:09 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 281920 examples: 0.023 | 0.059\n",
      "2023-12-30 18:39:09 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 283776 examples: 0.021 | 0.055\n",
      "2023-12-30 18:39:09 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 285632 examples: 0.022 | 0.054\n",
      "2023-12-30 18:39:10 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 287488 examples: 0.025 | 0.054\n",
      "2023-12-30 18:39:10 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 289344 examples: 0.023 | 0.054\n",
      "2023-12-30 18:39:10 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 291200 examples: 0.027 | 0.057\n",
      "2023-12-30 18:39:11 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 293056 examples: 0.036 | 0.056\n",
      "2023-12-30 18:39:11 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 294912 examples: 0.039 | 0.059\n",
      "2023-12-30 18:39:11 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 296768 examples: 0.027 | 0.065\n",
      "2023-12-30 18:39:11 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 298624 examples: 0.036 | 0.059\n",
      "2023-12-30 18:39:12 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 300480 examples: 0.023 | 0.056\n",
      "2023-12-30 18:39:12 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 302336 examples: 0.034 | 0.053\n",
      "2023-12-30 18:39:12 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 304192 examples: 0.021 | 0.056\n",
      "2023-12-30 18:39:13 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 306048 examples: 0.036 | 0.054\n",
      "2023-12-30 18:39:13 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 307904 examples: 0.033 | 0.055\n",
      "2023-12-30 18:39:13 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 309760 examples: 0.038 | 0.055\n",
      "2023-12-30 18:39:14 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 311616 examples: 0.037 | 0.052\n",
      "2023-12-30 18:39:14 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 313472 examples: 0.024 | 0.053\n",
      "2023-12-30 18:39:14 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 315328 examples: 0.026 | 0.055\n",
      "2023-12-30 18:39:15 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 317184 examples: 0.029 | 0.054\n",
      "2023-12-30 18:39:15 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 319040 examples: 0.028 | 0.053\n",
      "2023-12-30 18:39:15 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 320896 examples: 0.024 | 0.054\n",
      "2023-12-30 18:39:16 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 322752 examples: 0.021 | 0.052\n",
      "2023-12-30 18:39:16 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 324608 examples: 0.028 | 0.053\n",
      "2023-12-30 18:39:16 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 326464 examples: 0.032 | 0.051\n",
      "2023-12-30 18:39:17 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 328320 examples: 0.023 | 0.054\n",
      "2023-12-30 18:39:17 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 330176 examples: 0.029 | 0.052\n",
      "2023-12-30 18:39:17 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 332032 examples: 0.030 | 0.054\n",
      "2023-12-30 18:39:18 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 333888 examples: 0.028 | 0.052\n",
      "2023-12-30 18:39:18 - INFO     | Epoch: 5 | Learning Rate: 0.001: Training/Validation Loss after 335744 examples: 0.028 | 0.055\n",
      "2023-12-30 18:39:18 - INFO     | Early stopping: no decrease (0.055 vs 0.057); counter: 1 out of 3\n",
      " 30%|███       | 6/20 [01:01<02:22, 10.19s/it]2023-12-30 18:39:18 - INFO     | Epoch: 6 | Learning Rate: 0.001\n",
      "2023-12-30 18:39:18 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 336064 examples: 0.003 | 0.057\n",
      "2023-12-30 18:39:19 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 337920 examples: 0.017 | 0.051\n",
      "2023-12-30 18:39:19 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 339776 examples: 0.022 | 0.054\n",
      "2023-12-30 18:39:19 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 341632 examples: 0.018 | 0.052\n",
      "2023-12-30 18:39:20 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 343488 examples: 0.028 | 0.056\n",
      "2023-12-30 18:39:20 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 345344 examples: 0.026 | 0.052\n",
      "2023-12-30 18:39:20 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 347200 examples: 0.021 | 0.052\n",
      "2023-12-30 18:39:21 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 349056 examples: 0.029 | 0.055\n",
      "2023-12-30 18:39:21 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 350912 examples: 0.025 | 0.056\n",
      "2023-12-30 18:39:21 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 352768 examples: 0.024 | 0.052\n",
      "2023-12-30 18:39:22 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 354624 examples: 0.032 | 0.054\n",
      "2023-12-30 18:39:22 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 356480 examples: 0.021 | 0.052\n",
      "2023-12-30 18:39:22 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 358336 examples: 0.021 | 0.052\n",
      "2023-12-30 18:39:23 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 360192 examples: 0.020 | 0.055\n",
      "2023-12-30 18:39:23 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 362048 examples: 0.036 | 0.052\n",
      "2023-12-30 18:39:23 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 363904 examples: 0.022 | 0.054\n",
      "2023-12-30 18:39:23 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 365760 examples: 0.013 | 0.053\n",
      "2023-12-30 18:39:24 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 367616 examples: 0.021 | 0.053\n",
      "2023-12-30 18:39:24 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 369472 examples: 0.023 | 0.053\n",
      "2023-12-30 18:39:24 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 371328 examples: 0.029 | 0.059\n",
      "2023-12-30 18:39:25 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 373184 examples: 0.029 | 0.058\n",
      "2023-12-30 18:39:25 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 375040 examples: 0.021 | 0.053\n",
      "2023-12-30 18:39:25 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 376896 examples: 0.021 | 0.053\n",
      "2023-12-30 18:39:26 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 378752 examples: 0.024 | 0.053\n",
      "2023-12-30 18:39:26 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 380608 examples: 0.027 | 0.057\n",
      "2023-12-30 18:39:26 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 382464 examples: 0.021 | 0.054\n",
      "2023-12-30 18:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 384320 examples: 0.016 | 0.053\n",
      "2023-12-30 18:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 386176 examples: 0.029 | 0.050\n",
      "2023-12-30 18:39:27 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 388032 examples: 0.030 | 0.054\n",
      "2023-12-30 18:39:28 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 389888 examples: 0.023 | 0.056\n",
      "2023-12-30 18:39:28 - INFO     | Epoch: 6 | Learning Rate: 0.001: Training/Validation Loss after 391744 examples: 0.023 | 0.054\n",
      "2023-12-30 18:39:28 - INFO     | Early stopping: no decrease (0.055 vs 0.054); counter: 2 out of 3\n",
      " 35%|███▌      | 7/20 [01:11<02:11, 10.13s/it]2023-12-30 18:39:28 - INFO     | Epoch: 7 | Learning Rate: 0.001\n",
      "2023-12-30 18:39:28 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 392064 examples: 0.076 | 0.054\n",
      "2023-12-30 18:39:29 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 393920 examples: 0.015 | 0.050\n",
      "2023-12-30 18:39:29 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 395776 examples: 0.015 | 0.048\n",
      "2023-12-30 18:39:29 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 397632 examples: 0.009 | 0.049\n",
      "2023-12-30 18:39:30 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 399488 examples: 0.024 | 0.049\n",
      "2023-12-30 18:39:30 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 401344 examples: 0.027 | 0.051\n",
      "2023-12-30 18:39:30 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 403200 examples: 0.021 | 0.050\n",
      "2023-12-30 18:39:31 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 405056 examples: 0.017 | 0.051\n",
      "2023-12-30 18:39:31 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 406912 examples: 0.025 | 0.058\n",
      "2023-12-30 18:39:31 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 408768 examples: 0.022 | 0.054\n",
      "2023-12-30 18:39:32 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 410624 examples: 0.017 | 0.050\n",
      "2023-12-30 18:39:32 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 412480 examples: 0.025 | 0.051\n",
      "2023-12-30 18:39:32 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 414336 examples: 0.026 | 0.052\n",
      "2023-12-30 18:39:33 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 416192 examples: 0.013 | 0.051\n",
      "2023-12-30 18:39:33 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 418048 examples: 0.014 | 0.052\n",
      "2023-12-30 18:39:33 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 419904 examples: 0.019 | 0.051\n",
      "2023-12-30 18:39:33 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 421760 examples: 0.018 | 0.051\n",
      "2023-12-30 18:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 423616 examples: 0.022 | 0.052\n",
      "2023-12-30 18:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 425472 examples: 0.029 | 0.055\n",
      "2023-12-30 18:39:34 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 427328 examples: 0.015 | 0.051\n",
      "2023-12-30 18:39:35 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 429184 examples: 0.016 | 0.051\n",
      "2023-12-30 18:39:35 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 431040 examples: 0.021 | 0.051\n",
      "2023-12-30 18:39:35 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 432896 examples: 0.021 | 0.053\n",
      "2023-12-30 18:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 434752 examples: 0.019 | 0.050\n",
      "2023-12-30 18:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 436608 examples: 0.023 | 0.051\n",
      "2023-12-30 18:39:36 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 438464 examples: 0.027 | 0.056\n",
      "2023-12-30 18:39:37 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 440320 examples: 0.020 | 0.051\n",
      "2023-12-30 18:39:37 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 442176 examples: 0.016 | 0.048\n",
      "2023-12-30 18:39:37 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 444032 examples: 0.023 | 0.052\n",
      "2023-12-30 18:39:38 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 445888 examples: 0.017 | 0.052\n",
      "2023-12-30 18:39:38 - INFO     | Epoch: 7 | Learning Rate: 0.001: Training/Validation Loss after 447744 examples: 0.027 | 0.055\n",
      "2023-12-30 18:39:38 - INFO     | Early stopping: loss decreased (0.055 -> 0.052; -6.4%). Caching model state.\n",
      " 40%|████      | 8/20 [01:21<02:01, 10.12s/it]2023-12-30 18:39:38 - INFO     | Epoch: 8 | Learning Rate: 0.001\n",
      "2023-12-30 18:39:39 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 448064 examples: 0.016 | 0.052\n",
      "2023-12-30 18:39:39 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 449920 examples: 0.014 | 0.051\n",
      "2023-12-30 18:39:39 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 451776 examples: 0.016 | 0.049\n",
      "2023-12-30 18:39:40 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 453632 examples: 0.016 | 0.049\n",
      "2023-12-30 18:39:40 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 455488 examples: 0.017 | 0.049\n",
      "2023-12-30 18:39:40 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 457344 examples: 0.019 | 0.050\n",
      "2023-12-30 18:39:40 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 459200 examples: 0.016 | 0.054\n",
      "2023-12-30 18:39:41 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 461056 examples: 0.011 | 0.050\n",
      "2023-12-30 18:39:41 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 462912 examples: 0.023 | 0.051\n",
      "2023-12-30 18:39:41 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 464768 examples: 0.022 | 0.050\n",
      "2023-12-30 18:39:42 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 466624 examples: 0.026 | 0.049\n",
      "2023-12-30 18:39:42 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 468480 examples: 0.014 | 0.050\n",
      "2023-12-30 18:39:42 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 470336 examples: 0.021 | 0.052\n",
      "2023-12-30 18:39:43 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 472192 examples: 0.011 | 0.049\n",
      "2023-12-30 18:39:43 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 474048 examples: 0.013 | 0.053\n",
      "2023-12-30 18:39:43 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 475904 examples: 0.022 | 0.052\n",
      "2023-12-30 18:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 477760 examples: 0.015 | 0.051\n",
      "2023-12-30 18:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 479616 examples: 0.015 | 0.049\n",
      "2023-12-30 18:39:44 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 481472 examples: 0.022 | 0.049\n",
      "2023-12-30 18:39:45 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 483328 examples: 0.012 | 0.048\n",
      "2023-12-30 18:39:45 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 485184 examples: 0.017 | 0.051\n",
      "2023-12-30 18:39:45 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 487040 examples: 0.019 | 0.053\n",
      "2023-12-30 18:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 488896 examples: 0.030 | 0.052\n",
      "2023-12-30 18:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 490752 examples: 0.023 | 0.050\n",
      "2023-12-30 18:39:46 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 492608 examples: 0.016 | 0.051\n",
      "2023-12-30 18:39:47 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 494464 examples: 0.017 | 0.049\n",
      "2023-12-30 18:39:47 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 496320 examples: 0.017 | 0.049\n",
      "2023-12-30 18:39:47 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 498176 examples: 0.024 | 0.055\n",
      "2023-12-30 18:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 500032 examples: 0.014 | 0.050\n",
      "2023-12-30 18:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 501888 examples: 0.023 | 0.049\n",
      "2023-12-30 18:39:48 - INFO     | Epoch: 8 | Learning Rate: 0.001: Training/Validation Loss after 503744 examples: 0.016 | 0.049\n",
      "2023-12-30 18:39:48 - INFO     | Early stopping: no decrease (0.052 vs 0.052); counter: 1 out of 3\n",
      " 45%|████▌     | 9/20 [01:31<01:51, 10.14s/it]2023-12-30 18:39:48 - INFO     | Epoch: 9 | Learning Rate: 0.001\n",
      "2023-12-30 18:39:49 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 504064 examples: 0.001 | 0.052\n",
      "2023-12-30 18:39:49 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 505920 examples: 0.018 | 0.050\n",
      "2023-12-30 18:39:49 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 507776 examples: 0.015 | 0.053\n",
      "2023-12-30 18:39:50 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 509632 examples: 0.013 | 0.053\n",
      "2023-12-30 18:39:50 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 511488 examples: 0.010 | 0.051\n",
      "2023-12-30 18:39:50 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 513344 examples: 0.016 | 0.049\n",
      "2023-12-30 18:39:51 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 515200 examples: 0.022 | 0.050\n",
      "2023-12-30 18:39:51 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 517056 examples: 0.013 | 0.052\n",
      "2023-12-30 18:39:51 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 518912 examples: 0.014 | 0.049\n",
      "2023-12-30 18:39:52 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 520768 examples: 0.012 | 0.049\n",
      "2023-12-30 18:39:52 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 522624 examples: 0.017 | 0.050\n",
      "2023-12-30 18:39:52 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 524480 examples: 0.010 | 0.050\n",
      "2023-12-30 18:39:53 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 526336 examples: 0.010 | 0.049\n",
      "2023-12-30 18:39:53 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 528192 examples: 0.016 | 0.050\n",
      "2023-12-30 18:39:53 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 530048 examples: 0.015 | 0.050\n",
      "2023-12-30 18:39:54 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 531904 examples: 0.023 | 0.052\n",
      "2023-12-30 18:39:54 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 533760 examples: 0.009 | 0.051\n",
      "2023-12-30 18:39:54 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 535616 examples: 0.020 | 0.052\n",
      "2023-12-30 18:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 537472 examples: 0.012 | 0.049\n",
      "2023-12-30 18:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 539328 examples: 0.013 | 0.050\n",
      "2023-12-30 18:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 541184 examples: 0.018 | 0.052\n",
      "2023-12-30 18:39:55 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 543040 examples: 0.021 | 0.051\n",
      "2023-12-30 18:39:56 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 544896 examples: 0.014 | 0.047\n",
      "2023-12-30 18:39:56 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 546752 examples: 0.018 | 0.051\n",
      "2023-12-30 18:39:56 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 548608 examples: 0.020 | 0.050\n",
      "2023-12-30 18:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 550464 examples: 0.013 | 0.049\n",
      "2023-12-30 18:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 552320 examples: 0.025 | 0.049\n",
      "2023-12-30 18:39:57 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 554176 examples: 0.015 | 0.049\n",
      "2023-12-30 18:39:58 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 556032 examples: 0.012 | 0.049\n",
      "2023-12-30 18:39:58 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 557888 examples: 0.008 | 0.049\n",
      "2023-12-30 18:39:58 - INFO     | Epoch: 9 | Learning Rate: 0.001: Training/Validation Loss after 559744 examples: 0.029 | 0.050\n",
      "2023-12-30 18:39:59 - INFO     | Early stopping: no decrease (0.052 vs 0.051); counter: 2 out of 3\n",
      " 50%|█████     | 10/20 [01:41<01:41, 10.14s/it]2023-12-30 18:39:59 - INFO     | Epoch: 10 | Learning Rate: 0.001\n",
      "2023-12-30 18:39:59 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 560064 examples: 0.004 | 0.051\n",
      "2023-12-30 18:39:59 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 561920 examples: 0.012 | 0.050\n",
      "2023-12-30 18:40:00 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 563776 examples: 0.013 | 0.050\n",
      "2023-12-30 18:40:00 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 565632 examples: 0.015 | 0.049\n",
      "2023-12-30 18:40:00 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 567488 examples: 0.011 | 0.048\n",
      "2023-12-30 18:40:00 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 569344 examples: 0.011 | 0.049\n",
      "2023-12-30 18:40:01 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 571200 examples: 0.013 | 0.048\n",
      "2023-12-30 18:40:01 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 573056 examples: 0.011 | 0.050\n",
      "2023-12-30 18:40:01 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 574912 examples: 0.014 | 0.053\n",
      "2023-12-30 18:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 576768 examples: 0.011 | 0.054\n",
      "2023-12-30 18:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 578624 examples: 0.012 | 0.053\n",
      "2023-12-30 18:40:02 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 580480 examples: 0.012 | 0.052\n",
      "2023-12-30 18:40:03 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 582336 examples: 0.015 | 0.049\n",
      "2023-12-30 18:40:03 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 584192 examples: 0.014 | 0.050\n",
      "2023-12-30 18:40:03 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 586048 examples: 0.015 | 0.051\n",
      "2023-12-30 18:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 587904 examples: 0.009 | 0.047\n",
      "2023-12-30 18:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 589760 examples: 0.011 | 0.048\n",
      "2023-12-30 18:40:04 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 591616 examples: 0.016 | 0.050\n",
      "2023-12-30 18:40:05 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 593472 examples: 0.015 | 0.050\n",
      "2023-12-30 18:40:05 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 595328 examples: 0.032 | 0.049\n",
      "2023-12-30 18:40:05 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 597184 examples: 0.015 | 0.050\n",
      "2023-12-30 18:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 599040 examples: 0.011 | 0.046\n",
      "2023-12-30 18:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 600896 examples: 0.010 | 0.047\n",
      "2023-12-30 18:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 602752 examples: 0.019 | 0.050\n",
      "2023-12-30 18:40:06 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 604608 examples: 0.013 | 0.047\n",
      "2023-12-30 18:40:07 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 606464 examples: 0.010 | 0.049\n",
      "2023-12-30 18:40:07 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 608320 examples: 0.015 | 0.050\n",
      "2023-12-30 18:40:07 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 610176 examples: 0.015 | 0.048\n",
      "2023-12-30 18:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 612032 examples: 0.017 | 0.049\n",
      "2023-12-30 18:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 613888 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:08 - INFO     | Epoch: 10 | Learning Rate: 0.001: Training/Validation Loss after 615744 examples: 0.016 | 0.046\n",
      "2023-12-30 18:40:09 - INFO     | Early stopping: loss decreased (0.052 -> 0.049; -5.7%). Caching model state.\n",
      " 55%|█████▌    | 11/20 [01:51<01:30, 10.09s/it]2023-12-30 18:40:09 - INFO     | Epoch: 11 | Learning Rate: 0.001\n",
      "2023-12-30 18:40:09 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 616064 examples: 0.006 | 0.048\n",
      "2023-12-30 18:40:09 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 617920 examples: 0.010 | 0.049\n",
      "2023-12-30 18:40:10 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 619776 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:10 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 621632 examples: 0.012 | 0.048\n",
      "2023-12-30 18:40:10 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 623488 examples: 0.012 | 0.050\n",
      "2023-12-30 18:40:10 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 625344 examples: 0.011 | 0.047\n",
      "2023-12-30 18:40:11 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 627200 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:11 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 629056 examples: 0.014 | 0.050\n",
      "2023-12-30 18:40:11 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 630912 examples: 0.015 | 0.048\n",
      "2023-12-30 18:40:12 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 632768 examples: 0.012 | 0.050\n",
      "2023-12-30 18:40:12 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 634624 examples: 0.009 | 0.048\n",
      "2023-12-30 18:40:12 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 636480 examples: 0.008 | 0.049\n",
      "2023-12-30 18:40:13 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 638336 examples: 0.010 | 0.049\n",
      "2023-12-30 18:40:13 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 640192 examples: 0.010 | 0.047\n",
      "2023-12-30 18:40:13 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 642048 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:14 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 643904 examples: 0.012 | 0.048\n",
      "2023-12-30 18:40:14 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 645760 examples: 0.012 | 0.051\n",
      "2023-12-30 18:40:14 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 647616 examples: 0.015 | 0.050\n",
      "2023-12-30 18:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 649472 examples: 0.013 | 0.048\n",
      "2023-12-30 18:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 651328 examples: 0.018 | 0.047\n",
      "2023-12-30 18:40:15 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 653184 examples: 0.012 | 0.049\n",
      "2023-12-30 18:40:16 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 655040 examples: 0.015 | 0.048\n",
      "2023-12-30 18:40:16 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 656896 examples: 0.014 | 0.048\n",
      "2023-12-30 18:40:16 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 658752 examples: 0.019 | 0.047\n",
      "2023-12-30 18:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 660608 examples: 0.007 | 0.048\n",
      "2023-12-30 18:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 662464 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 664320 examples: 0.014 | 0.049\n",
      "2023-12-30 18:40:17 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 666176 examples: 0.016 | 0.050\n",
      "2023-12-30 18:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 668032 examples: 0.011 | 0.050\n",
      "2023-12-30 18:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 669888 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:18 - INFO     | Epoch: 11 | Learning Rate: 0.001: Training/Validation Loss after 671744 examples: 0.009 | 0.051\n",
      "2023-12-30 18:40:19 - INFO     | Early stopping: no decrease (0.049 vs 0.049); counter: 1 out of 3\n",
      " 60%|██████    | 12/20 [02:01<01:20, 10.10s/it]2023-12-30 18:40:19 - INFO     | Epoch: 12 | Learning Rate: 0.001\n",
      "2023-12-30 18:40:19 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 672064 examples: 0.006 | 0.049\n",
      "2023-12-30 18:40:19 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 673920 examples: 0.012 | 0.050\n",
      "2023-12-30 18:40:20 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 675776 examples: 0.008 | 0.048\n",
      "2023-12-30 18:40:20 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 677632 examples: 0.023 | 0.049\n",
      "2023-12-30 18:40:20 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 679488 examples: 0.008 | 0.048\n",
      "2023-12-30 18:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 681344 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 683200 examples: 0.007 | 0.048\n",
      "2023-12-30 18:40:21 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 685056 examples: 0.007 | 0.048\n",
      "2023-12-30 18:40:22 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 686912 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:22 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 688768 examples: 0.011 | 0.051\n",
      "2023-12-30 18:40:22 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 690624 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 692480 examples: 0.009 | 0.047\n",
      "2023-12-30 18:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 694336 examples: 0.012 | 0.049\n",
      "2023-12-30 18:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 696192 examples: 0.010 | 0.047\n",
      "2023-12-30 18:40:23 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 698048 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:24 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 699904 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:24 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 701760 examples: 0.010 | 0.049\n",
      "2023-12-30 18:40:24 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 703616 examples: 0.008 | 0.049\n",
      "2023-12-30 18:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 705472 examples: 0.014 | 0.050\n",
      "2023-12-30 18:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 707328 examples: 0.015 | 0.047\n",
      "2023-12-30 18:40:25 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 709184 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:26 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 711040 examples: 0.011 | 0.050\n",
      "2023-12-30 18:40:26 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 712896 examples: 0.011 | 0.048\n",
      "2023-12-30 18:40:26 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 714752 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 716608 examples: 0.012 | 0.051\n",
      "2023-12-30 18:40:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 718464 examples: 0.013 | 0.049\n",
      "2023-12-30 18:40:27 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 720320 examples: 0.011 | 0.050\n",
      "2023-12-30 18:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 722176 examples: 0.012 | 0.048\n",
      "2023-12-30 18:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 724032 examples: 0.009 | 0.048\n",
      "2023-12-30 18:40:28 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 725888 examples: 0.009 | 0.049\n",
      "2023-12-30 18:40:29 - INFO     | Epoch: 12 | Learning Rate: 0.001: Training/Validation Loss after 727744 examples: 0.014 | 0.048\n",
      "2023-12-30 18:40:29 - INFO     | Early stopping: no decrease (0.049 vs 0.047); counter: 2 out of 3\n",
      " 65%|██████▌   | 13/20 [02:11<01:10, 10.11s/it]2023-12-30 18:40:29 - INFO     | Epoch: 13 | Learning Rate: 0.001\n",
      "2023-12-30 18:40:29 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 728064 examples: 0.014 | 0.047\n",
      "2023-12-30 18:40:29 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 729920 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 731776 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 733632 examples: 0.010 | 0.053\n",
      "2023-12-30 18:40:30 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 735488 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:31 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 737344 examples: 0.005 | 0.047\n",
      "2023-12-30 18:40:31 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 739200 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:31 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 741056 examples: 0.007 | 0.048\n",
      "2023-12-30 18:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 742912 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 744768 examples: 0.011 | 0.050\n",
      "2023-12-30 18:40:32 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 746624 examples: 0.012 | 0.048\n",
      "2023-12-30 18:40:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 748480 examples: 0.011 | 0.048\n",
      "2023-12-30 18:40:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 750336 examples: 0.005 | 0.048\n",
      "2023-12-30 18:40:33 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 752192 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 754048 examples: 0.012 | 0.049\n",
      "2023-12-30 18:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 755904 examples: 0.008 | 0.048\n",
      "2023-12-30 18:40:34 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 757760 examples: 0.009 | 0.048\n",
      "2023-12-30 18:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 759616 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 761472 examples: 0.009 | 0.048\n",
      "2023-12-30 18:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 763328 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:35 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 765184 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 767040 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 768896 examples: 0.020 | 0.051\n",
      "2023-12-30 18:40:36 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 770752 examples: 0.015 | 0.053\n",
      "2023-12-30 18:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 772608 examples: 0.009 | 0.047\n",
      "2023-12-30 18:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 774464 examples: 0.007 | 0.048\n",
      "2023-12-30 18:40:37 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 776320 examples: 0.008 | 0.048\n",
      "2023-12-30 18:40:38 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 778176 examples: 0.012 | 0.049\n",
      "2023-12-30 18:40:38 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 780032 examples: 0.010 | 0.048\n",
      "2023-12-30 18:40:38 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 781888 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:39 - INFO     | Epoch: 13 | Learning Rate: 0.001: Training/Validation Loss after 783744 examples: 0.009 | 0.048\n",
      "2023-12-30 18:40:39 - INFO     | Early stopping: no decrease (0.049 vs 0.047); counter: 3 out of 3\n",
      "2023-12-30 18:40:39 - INFO     | Early stopping. Loading previous best state.\n",
      "2023-12-30 18:40:39 - INFO     | Reducing learning rate: 0.001 -> 0.0005\n",
      " 70%|███████   | 14/20 [02:22<01:00, 10.11s/it]2023-12-30 18:40:39 - INFO     | Epoch: 14 | Learning Rate: 0.001\n",
      "2023-12-30 18:40:39 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 784064 examples: 0.001 | 0.047\n",
      "2023-12-30 18:40:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 785920 examples: 0.008 | 0.046\n",
      "2023-12-30 18:40:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 787776 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:40 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 789632 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 791488 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 793344 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 795200 examples: 0.004 | 0.046\n",
      "2023-12-30 18:40:41 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 797056 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 798912 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 800768 examples: 0.009 | 0.046\n",
      "2023-12-30 18:40:42 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 802624 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 804480 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 806336 examples: 0.008 | 0.046\n",
      "2023-12-30 18:40:43 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 808192 examples: 0.010 | 0.047\n",
      "2023-12-30 18:40:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 810048 examples: 0.005 | 0.047\n",
      "2023-12-30 18:40:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 811904 examples: 0.009 | 0.047\n",
      "2023-12-30 18:40:44 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 813760 examples: 0.009 | 0.047\n",
      "2023-12-30 18:40:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 815616 examples: 0.016 | 0.048\n",
      "2023-12-30 18:40:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 817472 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:45 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 819328 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 821184 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 823040 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:46 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 824896 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 826752 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 828608 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 830464 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:47 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 832320 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 834176 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 836032 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:48 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 837888 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:49 - INFO     | Epoch: 14 | Learning Rate: 0.001: Training/Validation Loss after 839744 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:49 - INFO     | Early stopping: no decrease (0.049 vs 0.047); counter: 1 out of 3\n",
      " 75%|███████▌  | 15/20 [02:32<00:50, 10.11s/it]2023-12-30 18:40:49 - INFO     | Epoch: 15 | Learning Rate: 0.001\n",
      "2023-12-30 18:40:49 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 840064 examples: 0.001 | 0.047\n",
      "2023-12-30 18:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 841920 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 843776 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:50 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 845632 examples: 0.004 | 0.047\n",
      "2023-12-30 18:40:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 847488 examples: 0.004 | 0.047\n",
      "2023-12-30 18:40:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 849344 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:51 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 851200 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 853056 examples: 0.010 | 0.045\n",
      "2023-12-30 18:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 854912 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:52 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 856768 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 858624 examples: 0.011 | 0.046\n",
      "2023-12-30 18:40:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 860480 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:53 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 862336 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 864192 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 866048 examples: 0.006 | 0.046\n",
      "2023-12-30 18:40:54 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 867904 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 869760 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 871616 examples: 0.004 | 0.046\n",
      "2023-12-30 18:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 873472 examples: 0.006 | 0.047\n",
      "2023-12-30 18:40:55 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 875328 examples: 0.004 | 0.046\n",
      "2023-12-30 18:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 877184 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 879040 examples: 0.008 | 0.046\n",
      "2023-12-30 18:40:56 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 880896 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 882752 examples: 0.004 | 0.047\n",
      "2023-12-30 18:40:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 884608 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:57 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 886464 examples: 0.004 | 0.047\n",
      "2023-12-30 18:40:58 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 888320 examples: 0.007 | 0.046\n",
      "2023-12-30 18:40:58 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 890176 examples: 0.005 | 0.046\n",
      "2023-12-30 18:40:58 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 892032 examples: 0.017 | 0.046\n",
      "2023-12-30 18:40:59 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 893888 examples: 0.007 | 0.047\n",
      "2023-12-30 18:40:59 - INFO     | Epoch: 15 | Learning Rate: 0.001: Training/Validation Loss after 895744 examples: 0.008 | 0.047\n",
      "2023-12-30 18:40:59 - INFO     | Early stopping: no decrease (0.049 vs 0.046); counter: 2 out of 3\n",
      " 80%|████████  | 16/20 [02:42<00:40, 10.14s/it]2023-12-30 18:40:59 - INFO     | Epoch: 16 | Learning Rate: 0.001\n",
      "2023-12-30 18:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 896064 examples: 0.002 | 0.046\n",
      "2023-12-30 18:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 897920 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 899776 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:00 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 901632 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 903488 examples: 0.004 | 0.047\n",
      "2023-12-30 18:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 905344 examples: 0.007 | 0.046\n",
      "2023-12-30 18:41:01 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 907200 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 909056 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 910912 examples: 0.015 | 0.048\n",
      "2023-12-30 18:41:02 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 912768 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 914624 examples: 0.007 | 0.046\n",
      "2023-12-30 18:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 916480 examples: 0.010 | 0.047\n",
      "2023-12-30 18:41:03 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 918336 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 920192 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 922048 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:04 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 923904 examples: 0.008 | 0.047\n",
      "2023-12-30 18:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 925760 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 927616 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:05 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 929472 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 931328 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 933184 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 935040 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:06 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 936896 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 938752 examples: 0.008 | 0.046\n",
      "2023-12-30 18:41:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 940608 examples: 0.007 | 0.047\n",
      "2023-12-30 18:41:07 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 942464 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:08 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 944320 examples: 0.005 | 0.048\n",
      "2023-12-30 18:41:08 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 946176 examples: 0.007 | 0.046\n",
      "2023-12-30 18:41:08 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 948032 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:09 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 949888 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:09 - INFO     | Epoch: 16 | Learning Rate: 0.001: Training/Validation Loss after 951744 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:09 - INFO     | Early stopping: loss decreased (0.049 -> 0.046; -5.4%). Caching model state.\n",
      " 85%|████████▌ | 17/20 [02:52<00:30, 10.10s/it]2023-12-30 18:41:09 - INFO     | Epoch: 17 | Learning Rate: 0.001\n",
      "2023-12-30 18:41:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 952064 examples: 0.010 | 0.046\n",
      "2023-12-30 18:41:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 953920 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:10 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 955776 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 957632 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 959488 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 961344 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:11 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 963200 examples: 0.006 | 0.048\n",
      "2023-12-30 18:41:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 965056 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 966912 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:12 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 968768 examples: 0.007 | 0.046\n",
      "2023-12-30 18:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 970624 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 972480 examples: 0.008 | 0.046\n",
      "2023-12-30 18:41:13 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 974336 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 976192 examples: 0.007 | 0.046\n",
      "2023-12-30 18:41:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 978048 examples: 0.004 | 0.047\n",
      "2023-12-30 18:41:14 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 979904 examples: 0.003 | 0.046\n",
      "2023-12-30 18:41:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 981760 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 983616 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:15 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 985472 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 987328 examples: 0.005 | 0.045\n",
      "2023-12-30 18:41:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 989184 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:16 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 991040 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:17 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 992896 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:17 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 994752 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:17 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 996608 examples: 0.007 | 0.046\n",
      "2023-12-30 18:41:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 998464 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1000320 examples: 0.004 | 0.045\n",
      "2023-12-30 18:41:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1002176 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:18 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1004032 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:19 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1005888 examples: 0.016 | 0.046\n",
      "2023-12-30 18:41:19 - INFO     | Epoch: 17 | Learning Rate: 0.001: Training/Validation Loss after 1007744 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:19 - INFO     | Early stopping: no decrease (0.046 vs 0.046); counter: 1 out of 3\n",
      " 90%|█████████ | 18/20 [03:02<00:20, 10.11s/it]2023-12-30 18:41:19 - INFO     | Epoch: 18 | Learning Rate: 0.001\n",
      "2023-12-30 18:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1008064 examples: 0.002 | 0.046\n",
      "2023-12-30 18:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1009920 examples: 0.004 | 0.047\n",
      "2023-12-30 18:41:20 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1011776 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1013632 examples: 0.006 | 0.046\n",
      "2023-12-30 18:41:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1015488 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:21 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1017344 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1019200 examples: 0.007 | 0.047\n",
      "2023-12-30 18:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1021056 examples: 0.009 | 0.047\n",
      "2023-12-30 18:41:22 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1022912 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1024768 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1026624 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:23 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1028480 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1030336 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1032192 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:24 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1034048 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1035904 examples: 0.005 | 0.047\n",
      "2023-12-30 18:41:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1037760 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:25 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1039616 examples: 0.004 | 0.046\n",
      "2023-12-30 18:41:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1041472 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1043328 examples: 0.003 | 0.047\n",
      "2023-12-30 18:41:26 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1045184 examples: 0.004 | 0.047\n",
      "2023-12-30 18:41:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1047040 examples: 0.005 | 0.046\n",
      "2023-12-30 18:41:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1048896 examples: 0.006 | 0.047\n",
      "2023-12-30 18:41:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1050752 examples: 0.007 | 0.047\n",
      "2023-12-30 18:41:27 - INFO     | Epoch: 18 | Learning Rate: 0.001: Training/Validation Loss after 1052608 examples: 0.006 | 0.047\n"
     ]
    }
   ],
   "source": [
    "count = None if sweep_config['method'] == 'grid' else 50\n",
    "print(count)\n",
    "wandb.agent(sweep_id, model_pipeline, count=count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
