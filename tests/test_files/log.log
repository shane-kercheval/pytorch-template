2023-12-31 00:17:40 - INFO     | Training set  : X-torch.Size([56000, 784]), y-torch.Size([56000])
2023-12-31 00:17:40 - INFO     | Validation set: X-torch.Size([7000, 784]), y-torch.Size([7000])
2023-12-31 00:17:40 - INFO     | Test set      : X-torch.Size([7000, 784]), y-torch.Size([7000])
2023-12-31 00:17:47 - INFO     | Training set  : X-torch.Size([56000, 1, 28, 28]), y-torch.Size([56000])
2023-12-31 00:17:47 - INFO     | Validation set: X-torch.Size([7000, 1, 28, 28]), y-torch.Size([7000])
2023-12-31 00:17:47 - INFO     | Test set      : X-torch.Size([7000, 1, 28, 28]), y-torch.Size([7000])
2023-12-31 00:17:47 - INFO     | Training on cuda; epochs: 2; learning rate: 0.01
2023-12-31 00:17:47 - INFO     | Epoch: 0 | Learning Rate: 0.010
2023-12-31 00:17:47 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 128 examples: 35.764 | 139.939
2023-12-31 00:17:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 1,920 examples: 416.250 | 2.155
2023-12-31 00:17:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 3,712 examples: 2.288 | 2.308
2023-12-31 00:17:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 5,504 examples: 2.280 | 2.171
2023-12-31 00:17:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 7,296 examples: 2.179 | 2.130
2023-12-31 00:17:48 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 9,088 examples: 2.078 | 2.038
2023-12-31 00:17:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 10,880 examples: 1.946 | 1.850
2023-12-31 00:17:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 12,672 examples: 1.854 | 2.178
2023-12-31 00:17:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 14,464 examples: 1.882 | 1.738
2023-12-31 00:17:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 16,256 examples: 1.749 | 1.743
2023-12-31 00:17:49 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 18,048 examples: 1.815 | 1.805
2023-12-31 00:17:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 19,840 examples: 1.814 | 1.772
2023-12-31 00:17:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 21,632 examples: 1.767 | 1.752
2023-12-31 00:17:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 23,424 examples: 1.716 | 1.665
2023-12-31 00:17:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 25,216 examples: 1.601 | 1.535
2023-12-31 00:17:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 27,008 examples: 1.561 | 1.521
2023-12-31 00:17:50 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 28,800 examples: 1.605 | 1.498
2023-12-31 00:17:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 30,592 examples: 1.595 | 1.670
2023-12-31 00:17:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 32,384 examples: 1.588 | 1.516
2023-12-31 00:17:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 34,176 examples: 1.584 | 1.514
2023-12-31 00:17:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 35,968 examples: 1.515 | 1.483
2023-12-31 00:17:51 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 37,760 examples: 1.451 | 1.359
2023-12-31 00:17:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 39,552 examples: 1.513 | 1.594
2023-12-31 00:17:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 41,344 examples: 1.568 | 1.326
2023-12-31 00:17:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 43,136 examples: 1.285 | 1.356
2023-12-31 00:17:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 44,928 examples: 1.402 | 1.417
2023-12-31 00:17:52 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 46,720 examples: 1.292 | 1.365
2023-12-31 00:17:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 48,512 examples: 1.320 | 1.346
2023-12-31 00:17:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 50,304 examples: 1.382 | 1.341
2023-12-31 00:17:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 52,096 examples: 1.404 | 1.376
2023-12-31 00:17:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 53,888 examples: 1.287 | 1.270
2023-12-31 00:17:53 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 55,680 examples: 1.280 | 1.718
2023-12-31 00:17:54 - INFO     | Early stopping: loss decreased (inf -> 1.350; nan%). Caching model state.
2023-12-31 00:17:54 - INFO     | Epoch: 1 | Learning Rate: 0.010
2023-12-31 00:17:54 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 56,128 examples: 1.396 | 1.326
2023-12-31 00:17:54 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 57,920 examples: 1.320 | 1.657
2023-12-31 00:17:54 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 59,712 examples: 1.370 | 1.617
2023-12-31 00:17:54 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 61,504 examples: 1.363 | 1.269
2023-12-31 00:17:54 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 63,296 examples: 1.294 | 1.367
2023-12-31 00:17:55 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 65,088 examples: 1.296 | 1.276
2023-12-31 00:17:55 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 66,880 examples: 1.269 | 1.260
2023-12-31 00:17:55 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 68,672 examples: 1.219 | 1.246
2023-12-31 00:17:55 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 70,464 examples: 1.273 | 1.260
2023-12-31 00:17:55 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 72,256 examples: 1.364 | 1.308
2023-12-31 00:17:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 74,048 examples: 1.235 | 1.231
2023-12-31 00:17:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 75,840 examples: 1.191 | 1.246
2023-12-31 00:17:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 77,632 examples: 1.294 | 1.230
2023-12-31 00:17:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 79,424 examples: 1.222 | 1.214
2023-12-31 00:17:56 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 81,216 examples: 1.235 | 1.235
2023-12-31 00:17:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 83,008 examples: 1.258 | 1.275
2023-12-31 00:17:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 84,800 examples: 1.269 | 1.432
2023-12-31 00:17:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 86,592 examples: 1.491 | 1.450
2023-12-31 00:17:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 88,384 examples: 1.504 | 1.264
2023-12-31 00:17:57 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 90,176 examples: 1.271 | 1.209
2023-12-31 00:17:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 91,968 examples: 1.293 | 1.238
2023-12-31 00:17:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 93,760 examples: 1.237 | 1.196
2023-12-31 00:17:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 95,552 examples: 1.215 | 1.224
2023-12-31 00:17:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 97,344 examples: 1.257 | 1.205
2023-12-31 00:17:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 99,136 examples: 1.238 | 1.196
2023-12-31 00:17:58 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 100,928 examples: 1.177 | 1.177
2023-12-31 00:17:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 102,720 examples: 1.197 | 1.175
2023-12-31 00:17:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 104,512 examples: 1.272 | 1.188
2023-12-31 00:17:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 106,304 examples: 1.231 | 1.199
2023-12-31 00:17:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 108,096 examples: 1.238 | 1.246
2023-12-31 00:17:59 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 109,888 examples: 1.215 | 1.228
2023-12-31 00:18:00 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 111,680 examples: 1.354 | 1.400
2023-12-31 00:18:00 - INFO     | Early stopping: no decrease (1.350 vs 1.412); counter: 1 out of 3
2023-12-31 00:18:00 - INFO     | Training completed without early stopping.
2023-12-31 00:18:00 - INFO     | Best validation loss: 1.350
2023-12-31 00:18:00 - INFO     | Best early stopping index/epoch: 0
2023-12-31 00:18:01 - INFO     | Final Average Loss on training set: 1.419
2023-12-31 00:18:01 - INFO     | Final Average Loss on validation set: 1.412
2023-12-31 00:18:01 - INFO     | Final Average Loss on test set: 1.411
2023-12-31 00:18:02 - INFO     | Weighted Precision: 0.412, Recall: 0.485, F1: 0.418
2023-12-31 00:18:02 - INFO     | Training on cuda; epochs: 2; learning rate: 0.01
2023-12-31 00:18:02 - INFO     | Epoch: 0 | Learning Rate: 0.010
2023-12-31 00:18:02 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 128 examples: 12.327 | 71.359
2023-12-31 00:18:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 1,920 examples: 9.436 | 2.303
2023-12-31 00:18:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 3,712 examples: 2.302 | 2.303
2023-12-31 00:18:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 5,504 examples: 2.303 | 2.304
2023-12-31 00:18:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 7,296 examples: 2.303 | 2.303
2023-12-31 00:18:03 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 9,088 examples: 2.303 | 2.302
2023-12-31 00:18:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 10,880 examples: 2.303 | 2.301
2023-12-31 00:18:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 12,672 examples: 2.303 | 2.301
2023-12-31 00:18:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 14,464 examples: 2.302 | 2.301
2023-12-31 00:18:04 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 16,256 examples: 2.302 | 2.301
2023-12-31 00:18:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 18,048 examples: 2.297 | 2.270
2023-12-31 00:18:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 19,840 examples: 2.152 | 2.061
2023-12-31 00:18:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 21,632 examples: 2.034 | 1.985
2023-12-31 00:18:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 23,424 examples: 1.984 | 1.905
2023-12-31 00:18:05 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 25,216 examples: 1.834 | 1.726
2023-12-31 00:18:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 27,008 examples: 1.423 | 1.164
2023-12-31 00:18:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 28,800 examples: 1.100 | 0.885
2023-12-31 00:18:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 30,592 examples: 0.818 | 0.691
2023-12-31 00:18:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 32,384 examples: 0.673 | 0.580
2023-12-31 00:18:06 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 34,176 examples: 0.589 | 0.535
2023-12-31 00:18:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 35,968 examples: 0.463 | 0.462
2023-12-31 00:18:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 37,760 examples: 0.431 | 0.454
2023-12-31 00:18:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 39,552 examples: 0.429 | 0.401
2023-12-31 00:18:07 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 41,344 examples: 0.385 | 0.398
2023-12-31 00:18:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 43,136 examples: 0.365 | 0.383
2023-12-31 00:18:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 44,928 examples: 0.380 | 0.371
2023-12-31 00:18:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 46,720 examples: 0.359 | 0.340
2023-12-31 00:18:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 48,512 examples: 0.330 | 0.318
2023-12-31 00:18:08 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 50,304 examples: 0.394 | 0.319
2023-12-31 00:18:09 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 52,096 examples: 0.322 | 0.299
2023-12-31 00:18:09 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 53,888 examples: 0.278 | 0.356
2023-12-31 00:18:09 - INFO     | Epoch: 0 | Learning Rate: 0.010: Avg Training/Validation Loss after 55,680 examples: 0.313 | 0.313
2023-12-31 00:18:09 - INFO     | Early stopping: loss decreased (inf -> 0.306; nan%). Caching model state.
2023-12-31 00:18:09 - INFO     | Epoch: 1 | Learning Rate: 0.010
2023-12-31 00:18:09 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 56,128 examples: 0.377 | 0.291
2023-12-31 00:18:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 57,920 examples: 0.305 | 0.287
2023-12-31 00:18:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 59,712 examples: 0.303 | 0.281
2023-12-31 00:18:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 61,504 examples: 0.282 | 0.262
2023-12-31 00:18:10 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 63,296 examples: 0.269 | 0.282
2023-12-31 00:18:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 65,088 examples: 0.253 | 0.291
2023-12-31 00:18:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 66,880 examples: 0.240 | 0.247
2023-12-31 00:18:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 68,672 examples: 0.252 | 0.255
2023-12-31 00:18:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 70,464 examples: 0.261 | 0.238
2023-12-31 00:18:11 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 72,256 examples: 0.251 | 0.269
2023-12-31 00:18:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 74,048 examples: 0.262 | 0.261
2023-12-31 00:18:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 75,840 examples: 0.282 | 0.220
2023-12-31 00:18:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 77,632 examples: 0.249 | 0.264
2023-12-31 00:18:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 79,424 examples: 0.241 | 0.253
2023-12-31 00:18:12 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 81,216 examples: 0.242 | 0.266
2023-12-31 00:18:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 83,008 examples: 0.236 | 0.249
2023-12-31 00:18:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 84,800 examples: 0.306 | 0.263
2023-12-31 00:18:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 86,592 examples: 0.231 | 0.242
2023-12-31 00:18:13 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 88,384 examples: 0.294 | 0.250
2023-12-31 00:18:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 90,176 examples: 0.244 | 0.257
2023-12-31 00:18:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 91,968 examples: 0.271 | 0.241
2023-12-31 00:18:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 93,760 examples: 0.188 | 0.222
2023-12-31 00:18:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 95,552 examples: 0.246 | 0.226
2023-12-31 00:18:14 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 97,344 examples: 0.244 | 0.219
2023-12-31 00:18:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 99,136 examples: 0.238 | 0.221
2023-12-31 00:18:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 100,928 examples: 0.279 | 0.251
2023-12-31 00:18:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 102,720 examples: 0.256 | 0.241
2023-12-31 00:18:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 104,512 examples: 0.248 | 0.237
2023-12-31 00:18:15 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 106,304 examples: 0.218 | 0.241
2023-12-31 00:18:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 108,096 examples: 0.246 | 0.255
2023-12-31 00:18:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 109,888 examples: 0.223 | 0.230
2023-12-31 00:18:16 - INFO     | Epoch: 1 | Learning Rate: 0.010: Avg Training/Validation Loss after 111,680 examples: 0.238 | 0.242
2023-12-31 00:18:16 - INFO     | Early stopping: loss decreased (0.306 -> 0.218; -28.9%). Caching model state.
2023-12-31 00:18:16 - INFO     | Training completed without early stopping.
2023-12-31 00:18:16 - INFO     | Best validation loss: 0.218
2023-12-31 00:18:16 - INFO     | Best early stopping index/epoch: 1
2023-12-31 00:18:17 - INFO     | Final Average Loss on training set: 0.206
2023-12-31 00:18:17 - INFO     | Final Average Loss on validation set: 0.218
2023-12-31 00:18:17 - INFO     | Final Average Loss on test set: 0.241
2023-12-31 00:18:18 - INFO     | Weighted Precision: 0.934, Recall: 0.934, F1: 0.934
